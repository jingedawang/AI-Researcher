[
  {
    "title": "LaMP: When Large Language Models Meet Personalization",
    "abstract": "  This paper highlights the importance of personalization in large language\nmodels and introduces the LaMP benchmark -- a novel benchmark for training and\nevaluating language models for producing personalized outputs. LaMP offers a\ncomprehensive evaluation framework with diverse language tasks and multiple\nentries for each user profile. It consists of seven personalized tasks,\nspanning three text classification and four text generation tasks. We\nadditionally propose two retrieval augmentation approaches that retrieve\npersonal items from each user profile for personalizing language model outputs.\nTo this aim, we study various retrieval models, including term matching,\nsemantic matching, and time-aware methods. Extensive experiments on LaMP for\nzero-shot and fine-tuned language models demonstrate the efficacy of the\nproposed retrieval augmentation approach and highlight the impact of\npersonalization in various natural language tasks.\n",
    "related_paper_titles": [
      "Personalisation within bounds: A risk taxonomy and policy framework for\n  the alignment of large language models with personalised feedback",
      "News Category Dataset",
      "Reference-less Analysis of Context Specificity in Translation with\n  Personalised Language Models"
    ],
    "related_paper_abstract": [
      "  Large language models (LLMs) are used to generate content for a wide range of\ntasks, and are set to reach a growing audience in coming years due to\nintegration in product interfaces like ChatGPT or search engines like Bing.\nThis intensifies the need to ensure that models are aligned with human\npreferences and do not produce unsafe, inaccurate or toxic outputs. While\nalignment techniques like reinforcement learning with human feedback (RLHF) and\nred-teaming can mitigate some safety concerns and improve model capabilities,\nit is unlikely that an aggregate fine-tuning process can adequately represent\nthe full range of users' preferences and values. Different people may\nlegitimately disagree on their preferences for language and conversational\nnorms, as well as on values or ideologies which guide their communication.\nPersonalising LLMs through micro-level preference learning processes may result\nin models that are better aligned with each user. However, there are several\nnormative challenges in defining the bounds of a societally-acceptable and safe\ndegree of personalisation. In this paper, we ask how, and in what ways, LLMs\nshould be personalised. First, we review literature on current paradigms for\naligning LLMs with human feedback, and identify issues including (i) a lack of\nclarity regarding what alignment means; (ii) a tendency of technology providers\nto prescribe definitions of inherently subjective preferences and values; and\n(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in\nwho we are really aligning to. Second, we present a taxonomy of benefits and\nrisks associated with personalised LLMs, for individuals and society at large.\nFinally, we propose a three-tiered policy framework that allows users to\nexperience the benefits of personalised alignment, while restraining unsafe and\nundesirable LLM-behaviours within (supra-)national and organisational bounds.\n",
      "  People rely on news to know what is happening around the world and inform\ntheir daily lives. In today's world, when the proliferation of fake news is\nrampant, having a large-scale and high-quality source of authentic news\narticles with the published category information is valuable to learning\nauthentic news' Natural Language syntax and semantics. As part of this work, we\npresent a News Category Dataset that contains around 210k news headlines from\nthe year 2012 to 2022 obtained from HuffPost, along with useful metadata to\nenable various NLP tasks. In this paper, we also produce some novel insights\nfrom the dataset and describe various existing and potential applications of\nour dataset.\n",
      "  Sensitising language models (LMs) to external context helps them to more\neffectively capture the speaking patterns of individuals with specific\ncharacteristics or in particular environments. This work investigates to what\nextent rich character and film annotations can be leveraged to personalise LMs\nin a scalable manner. We then explore the use of such models in evaluating\ncontext specificity in machine translation. We build LMs which leverage rich\ncontextual information to reduce perplexity by up to 6.5% compared to a\nnon-contextual model, and generalise well to a scenario with no\nspeaker-specific data, relying on combinations of demographic characteristics\nexpressed via metadata. Our findings are consistent across two corpora, one of\nwhich (Cornell-rich) is also a contribution of this paper. We then use our\npersonalised LMs to measure the co-occurrence of extra-textual context and\ntranslation hypotheses in a machine translation setting. Our results suggest\nthat the degree to which professional translations in our domain are\ncontext-specific can be preserved to a better extent by a contextual machine\ntranslation model than a non-contextual model, which is also reflected in the\ncontextual model's superior reference-based scores.\n"
    ],
    "entities": [
      "Language",
      "LLMs",
      "LLM",
      "Large Language Models",
      "HYDRA",
      "Model Factorization Framework",
      "LaMP",
      "PSW",
      "Personalized Scientific Writing Large",
      "User History",
      "Human Trust",
      "News Headline Generation"
    ],
    "retrieved_papers": [
      {
        "title": "LaMP: When Large Language Models Meet Personalization,",
        "abstract": "This paper highlights the importance of personalization in large language\nmodels and introduces the LaMP benchmark -- a novel benchmark for training and\nevaluating language models for producing personalized outputs. LaMP offers a\ncomprehensive evaluation framework with diverse language tasks and multiple\nentries for each user profile. It consists of seven personalized tasks,\nspanning three text classification and four text generation tasks. We\nadditionally propose two retrieval augmentation approaches that retrieve\npersonal items from each user profile for personalizing language model outputs.\nTo this aim, we study various retrieval models, including term matching,\nsemantic matching, and time-aware methods. Extensive experiments on LaMP for\nzero-shot and fine-tuned language models demonstrate the efficacy of the\nproposed retrieval augmentation approach and highlight the impact of\npersonalization in various natural language tasks.",
        "score": 6.503839492797852
      },
      {
        "title": "Optimization Methods for Personalizing Large Language Models through\n  Retrieval Augmentation,",
        "abstract": "This paper studies retrieval-augmented approaches for personalizing large\nlanguage models (LLMs), which potentially have a substantial impact on various\napplications and domains. We propose the first attempt to optimize the\nretrieval models that deliver a limited number of personal documents to large\nlanguage models for the purpose of personalized generation. We develop two\noptimization algorithms that solicit feedback from the downstream personalized\ngeneration tasks for retrieval optimization -- one based on reinforcement\nlearning whose reward function is defined using any arbitrary metric for\npersonalized generation and another based on knowledge distillation from the\ndownstream LLM to the retrieval model. This paper also introduces a pre- and\npost-generation retriever selection model that decides what retriever to choose\nfor each LLM input. Extensive experiments on diverse tasks from the language\nmodel personalization (LaMP) benchmark reveal statistically significant\nimprovements in six out of seven datasets.",
        "score": 4.291491985321045
      },
      {
        "title": "LongLaMP: A Benchmark for Personalized Long-form Text Generation,",
        "abstract": "Long-text generation is seemingly ubiquitous in real-world applications of\nlarge language models such as generating an email or writing a review. Despite\nthe fundamental importance and prevalence of long-text generation in many\npractical applications, existing work on personalized generation has focused on\nthe generation of very short text. To overcome these limitations, we study the\nproblem of personalized long-text generation, that is, generating long-text\nthat is personalized for a specific user while being practically useful for the\nvast majority of real-world applications that naturally require the generation\nof longer text. In this work, we demonstrate the importance of user-specific\npersonalization for long-text generation tasks and develop the Long-text\nLanguage Model Personalization (LongLaMP) Benchmark. LongLaMP provides a\ncomprehensive and diverse evaluation framework for personalized long-text\ngeneration. Extensive experiments on LongLaMP for zero-shot and fine-tuned\nlanguage tasks demonstrate the effectiveness of the proposed benchmark and its\nutility for developing and evaluating techniques for personalized long-text\ngeneration across a wide variety of long-text generation tasks. The results\nhighlight the importance of personalization across a wide variety of long-text\ngeneration tasks. Finally, we release the benchmark for others to use for this\nimportant problem.",
        "score": 3.4823219776153564
      },
      {
        "title": "Step-Back Profiling: Distilling User History for Personalized Scientific\n  Writing,",
        "abstract": "Large language models (LLM) excel at a variety of natural language processing\ntasks, yet they struggle to generate personalized content for individuals,\nparticularly in real-world scenarios like scientific writing. Addressing this\nchallenge, we introduce STEP-BACK PROFILING to personalize LLMs by distilling\nuser history into concise profiles, including essential traits and preferences\nof users. To conduct the experiments, we construct a Personalized Scientific\nWriting (PSW) dataset to study multi-user personalization. PSW requires the\nmodels to write scientific papers given specialized author groups with diverse\nacademic backgrounds. As for the results, we demonstrate the effectiveness of\ncapturing user characteristics via STEP-BACK PROFILING for collaborative\nwriting. Moreover, our approach outperforms the baselines by up to 3.6 points\non the general personalization benchmark (LaMP), including 7 personalization\nLLM tasks. Our ablation studies validate the contributions of different\ncomponents in our method and provide insights into our task definition. Our\ndataset and code are available at\n\\url{https://github.com/gersteinlab/step-back-profiling}.",
        "score": 2.7284066677093506
      },
      {
        "title": "Automated Evaluation of Personalized Text Generation using Large\n  Language Models,",
        "abstract": "Personalized text generation presents a specialized mechanism for delivering\ncontent that is specific to a user's personal context. While the research\nprogress in this area has been rapid, evaluation still presents a challenge.\nTraditional automated metrics such as BLEU and ROUGE primarily measure lexical\nsimilarity to human-written references, and are not able to distinguish\npersonalization from other subtle semantic aspects, thus falling short of\ncapturing the nuances of personalized generated content quality. On the other\nhand, human judgments are costly to obtain, especially in the realm of\npersonalized evaluation. Inspired by these challenges, we explore the use of\nlarge language models (LLMs) for evaluating personalized text generation, and\nexamine their ability to understand nuanced user context. We present AuPEL, a\nnovel evaluation method that distills three major semantic aspects of the\ngenerated text: personalization, quality and relevance, and automatically\nmeasures these aspects. To validate the effectiveness of AuPEL, we design\ncarefully controlled experiments and compare the accuracy of the evaluation\njudgments made by LLMs versus that of judgements made by human annotators, and\nconduct rigorous analyses of the consistency and sensitivity of the proposed\nmetric. We find that, compared to existing evaluation metrics, AuPEL not only\ndistinguishes and ranks models based on their personalization abilities more\naccurately, but also presents commendable consistency and efficiency for this\ntask. Our work suggests that using LLMs as the evaluators of personalized text\ngeneration is superior to traditional text similarity metrics, even though\ninteresting new challenges still remain.",
        "score": 1.996277928352356
      },
      {
        "title": "PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization,",
        "abstract": "The recent emergence of Large Language Models (LLMs) has heralded a new era\nof human-AI interaction. These sophisticated models, exemplified by Chat-GPT\nand its successors, have exhibited remarkable capabilities in language\nunderstanding. However, as these LLMs have undergone exponential growth, a\ncrucial dimension that remains understudied is the personalization of these\nmodels. Large foundation models such as GPT-3 etc. focus on creating a\nuniversal model that serves a broad range of tasks and users. This approach\nemphasizes the model's generalization capabilities, treating users as a\ncollective rather than as distinct individuals. While practical for many common\napplications, this one-size-fits-all approach often fails to address the rich\ntapestry of human diversity and individual needs. To explore this issue we\nintroduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP\nmodels for user personalization. \\datasetname{} consists of a series of\nuser-centered tasks containing diverse and individualized expressions where the\npreferences of users can potentially differ for the same input. Using PEFT-U,\nwe explore the challenge of efficiently personalizing LLMs to accommodate\nuser-specific preferences in the context of diverse user-centered tasks.",
        "score": 1.9634464979171753
      },
      {
        "title": "Integrating Summarization and Retrieval for Enhanced Personalization via\n  Large Language Models,",
        "abstract": "Personalization, the ability to tailor a system to individual users, is an\nessential factor in user experience with natural language processing (NLP)\nsystems. With the emergence of Large Language Models (LLMs), a key question is\nhow to leverage these models to better personalize user experiences. To\npersonalize a language model's output, a straightforward approach is to\nincorporate past user data into the language model prompt, but this approach\ncan result in lengthy inputs exceeding limitations on input length and\nincurring latency and cost issues. Existing approaches tackle such challenges\nby selectively extracting relevant user data (i.e. selective retrieval) to\nconstruct a prompt for downstream tasks. However, retrieval-based methods are\nlimited by potential information loss, lack of more profound user\nunderstanding, and cold-start challenges. To overcome these limitations, we\npropose a novel summary-augmented approach by extending retrieval-augmented\npersonalization with task-aware user summaries generated by LLMs. The summaries\ncan be generated and stored offline, enabling real-world systems with runtime\nconstraints like voice assistants to leverage the power of LLMs. Experiments\nshow our method with 75% less of retrieved user data is on-par or outperforms\nretrieval augmentation on most tasks in the LaMP personalization benchmark. We\ndemonstrate that offline summarization via LLMs and runtime retrieval enables\nbetter performance for personalization on a range of tasks under practical\nconstraints.",
        "score": 1.5397363901138306
      },
      {
        "title": "Persona-DB: Efficient Large Language Model Personalization for Response\n  Prediction with Collaborative Data Refinement,",
        "abstract": "The increasing demand for personalized interactions with large language\nmodels (LLMs) calls for the development of methodologies capable of accurately\nand efficiently identifying user opinions and preferences. Retrieval\naugmentation emerges as an effective strategy, as it can accommodate a vast\nnumber of users without the costs from fine-tuning. Existing research, however,\nhas largely focused on enhancing the retrieval stage and devoted limited\nexploration toward optimizing the representation of the database, a crucial\naspect for tasks such as personalization. In this work, we examine the problem\nfrom a novel angle, focusing on how data can be better represented for more\nefficient retrieval in the context of LLM customization. To tackle this\nchallenge, we introduce Persona-DB, a simple yet effective framework consisting\nof a hierarchical construction process to improve generalization across task\ncontexts and collaborative refinement to effectively bridge knowledge gaps\namong users. In the task of response forecasting, Persona-DB demonstrates\nsuperior efficiency in maintaining accuracy with a significantly reduced\nretrieval size, a critical advantage in scenarios with extensive histories or\nlimited context windows. Our experiments also indicate a marked improvement of\nover 15% under cold-start scenarios, when users have extremely sparse data.\nFurthermore, our analysis reveals the increasing importance of collaborative\nknowledge as the retrieval capacity expands.",
        "score": 1.5057306289672852
      },
      {
        "title": "PEARL: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers,",
        "abstract": "Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style\nand specialized knowledge. In this paper, we address this challenge by\nproposing PEARL, a retrieval-augmented LLM writing assistant personalized with\na generation-calibrated retriever. Our retriever is trained to select historic\nuser-authored documents for prompt augmentation, such that they are likely to\nbest personalize LLM generations for a user request. We propose two key\nnovelties for training our retriever: 1) A training data selection method that\nidentifies user requests likely to benefit from personalization and documents\nthat provide that benefit; and 2) A scale-calibrating KL-divergence objective\nthat ensures that our retriever closely tracks the benefit of a document for\npersonalized generation. We demonstrate the effectiveness of PEARL in\ngenerating personalized workplace social media posts and Reddit comments.\nFinally, we showcase the potential of a generation-calibrated retriever to\ndouble as a performance predictor and further improve low-quality generations\nvia LLM chaining.",
        "score": 0.9684835076332092
      },
      {
        "title": "Few-shot Personalization of LLMs with Mis-aligned Responses,",
        "abstract": "As the diversity of users increases, the capability of providing personalized\nresponses by large language models (LLMs) has become increasingly important.\nExisting approaches have only limited successes in LLM personalization, due to\nthe absence of personalized learning or the reliance on shared personal data.\nThis paper proposes a new approach for a few-shot personalization of LLMs with\ntheir mis-aligned responses (Fermi). Our key idea is to learn a set of\npersonalized prompts for each user by progressively improving the prompts using\nLLMs, based on user profile (e.g., demographic information) and a few examples\nof previous opinions. During an iterative process of prompt improvement, we\nincorporate the contexts of mis-aligned responses by LLMs, which are especially\ncrucial for the effective personalization of LLMs. In addition, we develop an\neffective inference method to further leverage the context of the test query\nand the personalized prompts. Our experimental results demonstrate that Fermi\nsignificantly improves performance across various benchmarks, compared to the\nbest-performing baselines.",
        "score": 0.8250079154968262
      },
      {
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language\n  Models,",
        "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs.",
        "score": 0.6341516971588135
      },
      {
        "title": "Assessing generalization capability of text ranking models in Polish,",
        "abstract": "Retrieval-augmented generation (RAG) is becoming an increasingly popular\ntechnique for integrating internal knowledge bases with large language models.\nIn a typical RAG pipeline, three models are used, responsible for the\nretrieval, reranking, and generation stages. In this article, we focus on the\nreranking problem for the Polish language, examining the performance of\nrerankers and comparing their results with available retrieval models. We\nconduct a comprehensive evaluation of existing models and those trained by us,\nutilizing a benchmark of 41 diverse information retrieval tasks for the Polish\nlanguage. The results of our experiments show that most models struggle with\nout-of-domain generalization. However, a combination of effective optimization\nmethod and a large training dataset allows for building rerankers that are both\ncompact in size and capable of generalization. The best of our models\nestablishes a new state-of-the-art for reranking in the Polish language,\noutperforming existing models with up to 30 times more parameters.",
        "score": 0.15523429214954376
      },
      {
        "title": "Self-Retrieval: Building an Information Retrieval System with One Large\n  Language Model,",
        "abstract": "The rise of large language models (LLMs) has transformed the role of\ninformation retrieval (IR) systems in the way to humans accessing information.\nDue to the isolated architecture and the limited interaction, existing IR\nsystems are unable to fully accommodate the shift from directly providing\ninformation to humans to indirectly serving large language models. In this\npaper, we propose Self-Retrieval, an end-to-end, LLM-driven information\nretrieval architecture that can fully internalize the required abilities of IR\nsystems into a single LLM and deeply leverage the capabilities of LLMs during\nIR process. Specifically, Self-retrieval internalizes the corpus to retrieve\ninto a LLM via a natural language indexing architecture. Then the entire\nretrieval process is redefined as a procedure of document generation and\nself-assessment, which can be end-to-end executed using a single large language\nmodel. Experimental results demonstrate that Self-Retrieval not only\nsignificantly outperforms previous retrieval approaches by a large margin, but\nalso can significantly boost the performance of LLM-driven downstream\napplications like retrieval augumented generation.",
        "score": 0.07717806100845337
      },
      {
        "title": "Leveraging Translation For Optimal Recall: Tailoring LLM Personalization\n  With User Profiles,",
        "abstract": "This paper explores a novel technique for improving recall in cross-language\ninformation retrieval (CLIR) systems using iterative query refinement grounded\nin the user's lexical-semantic space. The proposed methodology combines\nmulti-level translation, semantic embedding-based expansion, and user\nprofile-centered augmentation to address the challenge of matching variance\nbetween user queries and relevant documents. Through an initial BM25 retrieval,\ntranslation into intermediate languages, embedding lookup of similar terms, and\niterative re-ranking, the technique aims to expand the scope of potentially\nrelevant results personalized to the individual user. Comparative experiments\non news and Twitter datasets demonstrate superior performance over baseline\nBM25 ranking for the proposed approach across ROUGE metrics. The translation\nmethodology also showed maintained semantic accuracy through the multi-step\nprocess. This personalized CLIR framework paves the path for improved\ncontext-aware retrieval attentive to the nuances of user language.",
        "score": 0.07269154489040375
      },
      {
        "title": "ERAGent: Enhancing Retrieval-Augmented Language Models with Improved\n  Accuracy, Efficiency, and Personalization,",
        "abstract": "Retrieval-augmented generation (RAG) for language models significantly\nimproves language understanding systems. The basic retrieval-then-read pipeline\nof response generation has evolved into a more extended process due to the\nintegration of various components, sometimes even forming loop structures.\nDespite its advancements in improving response accuracy, challenges like poor\nretrieval quality for complex questions that require the search of multifaceted\nsemantic information, inefficiencies in knowledge re-retrieval during long-term\nserving, and lack of personalized responses persist. Motivated by transcending\nthese limitations, we introduce ERAGent, a cutting-edge framework that embodies\nan advancement in the RAG area. Our contribution is the introduction of the\nsynergistically operated module: Enhanced Question Rewriter and Knowledge\nFilter, for better retrieval quality. Retrieval Trigger is incorporated to\ncurtail extraneous external knowledge retrieval without sacrificing response\nquality. ERAGent also personalizes responses by incorporating a learned user\nprofile. The efficiency and personalization characteristics of ERAGent are\nsupported by the Experiential Learner module which makes the AI assistant being\ncapable of expanding its knowledge and modeling user profile incrementally.\nRigorous evaluations across six datasets and three question-answering tasks\nprove ERAGent's superior accuracy, efficiency, and personalization, emphasizing\nits potential to advance the RAG field and its applicability in practical\nsystems.",
        "score": -0.17037290334701538
      },
      {
        "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large\n  Language Models,",
        "abstract": "Large Language Models (LLMs) have the unique capability to understand and\ngenerate human-like text from input queries. When fine-tuned, these models show\nenhanced performance on domain-specific queries. OpenAI highlights the process\nof fine-tuning, stating: \"To fine-tune a model, you are required to provide at\nleast 10 examples. We typically see clear improvements from fine-tuning on 50\nto 100 training examples, but the right number varies greatly based on the\nexact use case.\" This study extends this concept to the integration of LLMs\nwithin Retrieval-Augmented Generation (RAG) pipelines, which aim to improve\naccuracy and relevance by leveraging external corpus data for information\nretrieval. However, RAG's promise of delivering optimal responses often falls\nshort in complex query scenarios. This study aims to specifically examine the\neffects of fine-tuning LLMs on their ability to extract and integrate\ncontextual data to enhance the performance of RAG systems across multiple\ndomains. We evaluate the impact of fine-tuning on the LLMs' capacity for data\nextraction and contextual understanding by comparing the accuracy and\ncompleteness of fine-tuned models against baseline performances across datasets\nfrom multiple domains. Our findings indicate that fine-tuning resulted in a\ndecline in performance compared to the baseline models, contrary to the\nimprovements observed in standalone LLM applications as suggested by OpenAI.\nThis study highlights the need for vigorous investigation and validation of\nfine-tuned models for domain-specific tasks.",
        "score": -0.842251718044281
      },
      {
        "title": "QUILL: Query Intent with Large Language Models using Retrieval\n  Augmentation and Multi-stage Distillation,",
        "abstract": "Large Language Models (LLMs) have shown impressive results on a variety of\ntext understanding tasks. Search queries though pose a unique challenge, given\ntheir short-length and lack of nuance or context. Complicated feature\nengineering efforts do not always lead to downstream improvements as their\nperformance benefits may be offset by increased complexity of knowledge\ndistillation. Thus, in this paper we make the following contributions: (1) We\ndemonstrate that Retrieval Augmentation of queries provides LLMs with valuable\nadditional context enabling improved understanding. While Retrieval\nAugmentation typically increases latency of LMs (thus hurting distillation\nefficacy), (2) we provide a practical and effective way of distilling Retrieval\nAugmentation LLMs. Specifically, we use a novel two-stage distillation approach\nthat allows us to carry over the gains of retrieval augmentation, without\nsuffering the increased compute typically associated with it. (3) We\ndemonstrate the benefits of the proposed approach (QUILL) on a billion-scale,\nreal-world query understanding system resulting in huge gains. Via extensive\nexperiments, including on public benchmarks, we believe this work offers a\nrecipe for practical use of retrieval-augmented query understanding.",
        "score": -0.8745914697647095
      },
      {
        "title": "Unsupervised Information Refinement Training of Large Language Models\n  for Retrieval-Augmented Generation,",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.",
        "score": -0.9236140251159668
      },
      {
        "title": "LlamaRec: Two-Stage Recommendation using Large Language Models for\n  Ranking,",
        "abstract": "Recently, large language models (LLMs) have exhibited significant progress in\nlanguage understanding and generation. By leveraging textual features,\ncustomized LLMs are also applied for recommendation and demonstrate\nimprovements across diverse recommendation scenarios. Yet the majority of\nexisting methods perform training-free recommendation that heavily relies on\npretrained knowledge (e.g., movie recommendation). In addition, inference on\nLLMs is slow due to autoregressive generation, rendering existing methods less\neffective for real-time recommendation. As such, we propose a two-stage\nframework using large language models for ranking-based recommendation\n(LlamaRec). In particular, we use small-scale sequential recommenders to\nretrieve candidates based on the user interaction history. Then, both history\nand retrieved items are fed to the LLM in text via a carefully designed prompt\ntemplate. Instead of generating next-item titles, we adopt a verbalizer-based\napproach that transforms output logits into probability distributions over the\ncandidate items. Therefore, the proposed LlamaRec can efficiently rank items\nwithout generating long text. To validate the effectiveness of the proposed\nframework, we compare against state-of-the-art baseline methods on benchmark\ndatasets. Our experimental results demonstrate the performance of LlamaRec,\nwhich consistently achieves superior performance in both recommendation\nperformance and efficiency.",
        "score": -1.0179190635681152
      },
      {
        "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions,",
        "abstract": "Modern Language Models (LMs) are capable of following long and complex\ninstructions that enable a large and diverse set of user requests. While\nInformation Retrieval (IR) models use these LMs as the backbone of their\narchitectures, virtually none of them allow users to provide detailed\ninstructions alongside queries, thus limiting their ability to satisfy complex\ninformation needs. In this work, we study the use of instructions in IR\nsystems. First, we introduce our dataset FollowIR, which contains a rigorous\ninstruction evaluation benchmark as well as a training set for helping IR\nmodels learn to better follow real-world instructions. FollowIR repurposes\ndetailed instructions -- also known as narratives -- developed for professional\nassessors to evaluate retrieval systems. In particular, we build our benchmark\nfrom three collections curated for shared tasks at the Text REtrieval\nConference (TREC). These collections contains hundreds to thousands of labeled\ndocuments per query, making them suitable for our exploration. Through this\nprocess, we can measure how well IR models follow instructions, through a new\npairwise evaluation framework. Our results indicate that existing retrieval\nmodels fail to correctly use instructions, using them for basic keywords and\nstruggling to understand long-form information. However, we show that it is\npossible for IR models to learn to follow complex instructions: our new\nFollowIR-7B model has significant improvements after fine-tuning on our\ntraining set.",
        "score": -1.1338005065917969
      },
      {
        "title": "From Matching to Generation: A Survey on Generative Information\n  Retrieval,",
        "abstract": "Information Retrieval (IR) systems are crucial tools for users to access\ninformation, widely applied in scenarios like search engines, question\nanswering, and recommendation systems. Traditional IR methods, based on\nsimilarity matching to return ranked lists of documents, have been reliable\nmeans of information acquisition, dominating the IR field for years. With the\nadvancement of pre-trained language models, generative information retrieval\n(GenIR) has emerged as a novel paradigm, gaining increasing attention in recent\nyears. Currently, research in GenIR can be categorized into two aspects:\ngenerative document retrieval (GR) and reliable response generation. GR\nleverages the generative model's parameters for memorizing documents, enabling\nretrieval by directly generating relevant document identifiers without explicit\nindexing. Reliable response generation, on the other hand, employs language\nmodels to directly generate the information users seek, breaking the\nlimitations of traditional IR in terms of document granularity and relevance\nmatching, offering more flexibility, efficiency, and creativity, thus better\nmeeting practical needs. This paper aims to systematically review the latest\nresearch progress in GenIR. We will summarize the advancements in GR regarding\nmodel training, document identifier, incremental learning, downstream tasks\nadaptation, multi-modal GR and generative recommendation, as well as progress\nin reliable response generation in aspects of internal knowledge memorization,\nexternal knowledge augmentation, generating response with citations and\npersonal information assistant. We also review the evaluation, challenges and\nfuture prospects in GenIR systems. This review aims to offer a comprehensive\nreference for researchers in the GenIR field, encouraging further development\nin this area.",
        "score": -1.2233253717422485
      },
      {
        "title": "Synergistic Interplay between Search and Large Language Models for\n  Information Retrieval,",
        "abstract": "Information retrieval (IR) plays a crucial role in locating relevant\nresources from vast amounts of data, and its applications have evolved from\ntraditional knowledge bases to modern retrieval models (RMs). The emergence of\nlarge language models (LLMs) has further revolutionized the IR field by\nenabling users to interact with search systems in natural languages. In this\npaper, we explore the advantages and disadvantages of LLMs and RMs,\nhighlighting their respective strengths in understanding user-issued queries\nand retrieving up-to-date information. To leverage the benefits of both\nparadigms while circumventing their limitations, we propose InteR, a novel\nframework that facilitates information refinement through synergy between RMs\nand LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated\nknowledge collections and enables LLMs to enhance prompt formulation using\nretrieved documents. This iterative refinement process augments the inputs of\nRMs and LLMs, leading to more accurate retrieval. Experiments on large-scale\nretrieval benchmarks involving web search and low-resource retrieval tasks\ndemonstrate that InteR achieves overall superior zero-shot retrieval\nperformance compared to state-of-the-art methods, even those using relevance\njudgment. Source code is available at https://github.com/Cyril-JZ/InteR",
        "score": -1.6974912881851196
      },
      {
        "title": "RE-AdaptIR: Improving Information Retrieval through Reverse Engineered\n  Adaptation,",
        "abstract": "Large language models (LLMs) fine-tuned for text-retrieval have demonstrated\nstate-of-the-art results across several information retrieval (IR) benchmarks.\nHowever, supervised training for improving these models requires numerous\nlabeled examples, which are generally unavailable or expensive to acquire. In\nthis work, we explore the effectiveness of extending reverse engineered\nadaptation to the context of information retrieval (RE-AdaptIR). We use\nRE-AdaptIR to improve LLM-based IR models using only unlabeled data. We\ndemonstrate improved performance both in training domains as well as zero-shot\nin domains where the models have seen no queries. We analyze performance\nchanges in various fine-tuning scenarios and offer findings of immediate use to\npractitioners.",
        "score": -1.8496581315994263
      },
      {
        "title": "Parameter Efficient Tuning Allows Scalable Personalization of LLMs for\n  Text Entry: A Case Study on Abbreviation Expansion,",
        "abstract": "Abbreviation expansion is a strategy used to speed up communication by\nlimiting the amount of typing and using a language model to suggest expansions.\nHere we look at personalizing a Large Language Model's (LLM) suggestions based\non prior conversations to enhance the relevance of predictions, particularly\nwhen the user data is small (~1000 samples). Specifically, we compare\nfine-tuning, prompt-tuning, and retrieval augmented generation of expanded text\nsuggestions for abbreviated inputs. Our case study with a deployed 8B parameter\nLLM on a real user living with ALS, and experiments on movie character\npersonalization indicates that (1) customization may be necessary in some\nscenarios and prompt-tuning generalizes well to those, (2) fine-tuning on\nin-domain data (with as few as 600 samples) still shows some gains, however (3)\nretrieval augmented few-shot selection also outperforms fine-tuning. (4)\nParameter efficient tuning allows for efficient and scalable personalization.\nFor prompt-tuning, we also find that initializing the learned \"soft-prompts\" to\nuser relevant concept tokens leads to higher accuracy than random\ninitialization.",
        "score": -1.8989442586898804
      },
      {
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models,",
        "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that\ntreats the language model (LM) as a black box and augments it with a tuneable\nretrieval model. Unlike prior retrieval-augmented LMs that train language\nmodels with special cross attention mechanisms to encode the retrieved text,\nREPLUG simply prepends retrieved documents to the input for the frozen\nblack-box LM. This simple design can be easily applied to any existing\nretrieval and language models. Furthermore, we show that the LM can be used to\nsupervise the retrieval model, which can then find documents that help the LM\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\nretriever significantly improves the performance of GPT-3 (175B) on language\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\n5.1%.",
        "score": -2.018676996231079
      },
      {
        "title": "ReFusion: Improving Natural Language Understanding with\n  Computation-Efficient Retrieval Representation Fusion,",
        "abstract": "Retrieval-based augmentations (RA) incorporating knowledge from an external\ndatabase into language models have greatly succeeded in various\nknowledge-intensive (KI) tasks. However, integrating retrievals in\nnon-knowledge-intensive (NKI) tasks is still challenging. Existing works focus\non concatenating retrievals with inputs to improve model performance.\nUnfortunately, the use of retrieval concatenation-based augmentations causes an\nincrease in the input length, substantially raising the computational demands\nof attention mechanisms. This paper proposes a new paradigm of RA named\n\\textbf{ReFusion}, a computation-efficient Retrieval representation Fusion with\nbi-level optimization. Unlike previous works, ReFusion directly fuses the\nretrieval representations into the hidden states of models. Specifically,\nReFusion leverages an adaptive retrieval integrator to seek the optimal\ncombination of the proposed ranking schemes across different model layers.\nExperimental results demonstrate that the proposed ReFusion can achieve\nsuperior and robust performance in various NKI tasks.",
        "score": -2.380725383758545
      },
      {
        "title": "Corrective Retrieval Augmented Generation,",
        "abstract": "Large language models (LLMs) inevitably exhibit hallucinations since the\naccuracy of generated texts cannot be secured solely by the parametric\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\ndocuments, raising concerns about how the model behaves if retrieval goes\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\nretrieval evaluator is designed to assess the overall quality of retrieved\ndocuments for a query, returning a confidence degree based on which different\nknowledge retrieval actions can be triggered. Since retrieval from static and\nlimited corpora can only return sub-optimal documents, large-scale web searches\nare utilized as an extension for augmenting the retrieval results. Besides, a\ndecompose-then-recompose algorithm is designed for retrieved documents to\nselectively focus on key information and filter out irrelevant information in\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\nRAG-based approaches. Experiments on four datasets covering short- and\nlong-form generation tasks show that CRAG can significantly improve the\nperformance of RAG-based approaches.",
        "score": -2.4729928970336914
      },
      {
        "title": "Augmentation-Adapted Retriever Improves Generalization of Language\n  Models as Generic Plug-In,",
        "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive\ntasks by supplying them with external information. Prior works on retrieval\naugmentation usually jointly fine-tune the retriever and the LM, making them\nclosely coupled. In this paper, we explore the scheme of generic retrieval\nplug-in: the retriever is to assist target LMs that may not be known beforehand\nor are unable to be fine-tuned together. To retrieve useful documents for\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\nlearns LM's preferences obtained from a known source LM. Experiments on the\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\nis able to significantly improve the zero-shot generalization of larger target\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\nthat the preferences of different LMs overlap, enabling AAR trained with a\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",
        "score": -3.1360957622528076
      },
      {
        "title": "Retrieval-Pretrained Transformer: Long-range Language Modeling with\n  Self-retrieval,",
        "abstract": "Retrieval-augmented language models (LMs) have received much attention\nrecently. However, typically the retriever is not trained jointly as a native\ncomponent of the LM, but added post-hoc to an already-pretrained LM, which\nlimits the ability of the LM and the retriever to adapt to one another. In this\nwork, we propose the Retrieval-Pretrained Transformer (RPT), an architecture\nand training procedure for jointly training a retrieval-augmented LM from\nscratch and apply it to the task of modeling long texts. Given a recently\ngenerated text chunk in a long document, the LM computes query representations,\nwhich are then used to retrieve earlier chunks in the document, located\npotentially tens of thousands of tokens before. Information from retrieved\nchunks is fused into the LM representations to predict the next target chunk.\nWe train the retriever component with a semantic objective, where the goal is\nto retrieve chunks that increase the probability of the next chunk, according\nto a reference LM. We evaluate RPT on four long-range language modeling tasks,\nspanning books, code, and mathematical writing, and demonstrate that RPT\nimproves retrieval quality and subsequently perplexity across the board\ncompared to strong baselines.",
        "score": -3.6497156620025635
      },
      {
        "title": "Query-oriented Data Augmentation for Session Search,",
        "abstract": "Modeling contextual information in a search session has drawn more and more\nattention when understanding complex user intents. Recent methods are all\ndata-driven, i.e., they train different models on large-scale search log data\nto identify the relevance between search contexts and candidate documents. The\ncommon training paradigm is to pair the search context with different candidate\ndocuments and train the model to rank the clicked documents higher than the\nunclicked ones. However, this paradigm neglects the symmetric nature of the\nrelevance between the session context and document, i.e., the clicked documents\ncan also be paired with different search contexts when training. In this work,\nwe propose query-oriented data augmentation to enrich search logs and empower\nthe modeling. We generate supplemental training pairs by altering the most\nimportant part of a search context, i.e., the current query, and train our\nmodel to rank the generated sequence along with the original sequence. This\napproach enables models to learn that the relevance of a document may vary as\nthe session context changes, leading to a better understanding of users' search\npatterns. We develop several strategies to alter the current query, resulting\nin new training data with varying degrees of difficulty. Through\nexperimentation on two extensive public search logs, we have successfully\ndemonstrated the effectiveness of our model.",
        "score": -3.893636703491211
      }
    ]
  },
  {
    "title": "Harnessing the Power of Large Language Models for Natural Language to\n  First-Order Logic Translation",
    "abstract": "  Translating natural language sentences to first-order logic (NL-FOL\ntranslation) is a longstanding challenge in the NLP and formal logic\nliterature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for\nNL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of\ndirectly translating natural language into FOL rules, which outperforms\nGPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,\nand can achieve similar performance as GPT-4 with a fraction of the cost. This\ncorrection ability was achieved by a novel supervised fine-tuning (SFT) +\nreinforcement learning with human feedback (RLHF) framework, which initially\ntrains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought\nreasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier\nas the reward model.\n  To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel\ngener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset\nof 34K high-quality and diverse sentence-level NL-FOL pairs collected from\nGPT-4. The dataset was created by implementing a pipeline that prompts GPT-4\nfor pairs, and dynamically adjusts the prompts to ensure the collection of\npairs with rich and diverse contexts at different levels of complexity, and\nverifies the validity of the generated FOL rules. Codes, weights, and data are\navailable at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small\n\\text{https://github.com/gblackout/LogicLLaMA}}}$.\n",
    "related_paper_titles": [
      "Exploring Neural Models for Parsing Natural Language into First-Order\n  Logic",
      "Formal Specifications from Natural Language",
      "LoRA: Low-Rank Adaptation of Large Language Models"
    ],
    "related_paper_abstract": [
      "  Semantic parsing is the task of obtaining machine-interpretable\nrepresentations from natural language text. We consider one such formal\nrepresentation - First-Order Logic (FOL) and explore the capability of neural\nmodels in parsing English sentences to FOL. We model FOL parsing as a sequence\nto sequence mapping task where given a natural language sentence, it is encoded\ninto an intermediate representation using an LSTM followed by a decoder which\nsequentially generates the predicates in the corresponding FOL formula. We\nimprove the standard encoder-decoder model by introducing a variable alignment\nmechanism that enables it to align variables across predicates in the predicted\nFOL. We further show the effectiveness of predicting the category of FOL entity\n- Unary, Binary, Variables and Scoped Entities, at each decoder step as an\nauxiliary task on improving the consistency of generated FOL. We perform\nrigorous evaluations and extensive ablations. We also aim to release our code\nas well as large scale FOL dataset along with models to aid further research in\nlogic-based parsing and inference in NLP.\n",
      "  We study the generalization abilities of language models when translating\nnatural language into formal specifications with complex semantics. In\nparticular, we fine-tune language models on three datasets consisting of\nEnglish sentences and their corresponding formal representation: 1) regular\nexpressions (regex), frequently used in programming and search; 2) First-order\nlogic (FOL), commonly used in software verification and theorem proving; and 3)\nlinear-time temporal logic (LTL), which forms the basis for industrial hardware\nspecification languages. Our experiments show that, in these diverse domains,\nthe language models maintain their generalization capabilities from pre-trained\nknowledge of natural language to generalize, e.g., to new variable names or\noperator descriptions. Additionally, they achieve competitive performance, and\neven outperform the state-of-the-art for translating into regular expressions,\nwith the benefits of being easy to access, efficient to fine-tune, and without\na particular need for domain-specific reasoning.\n",
      "  An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.\n"
    ],
    "entities": [
      "ToM",
      "GenAI",
      "GNN",
      "GAI",
      "AIGC",
      "Mind",
      "Future",
      "LVLMs",
      "ASD",
      "New",
      "SAR",
      "Dynamic",
      "VLN",
      "ANN",
      "Korean",
      "Semantic",
      "AC",
      "MARL",
      "LMM",
      "VQA",
      "Embodied",
      "U.S.",
      "GUI",
      "Earth",
      "CAVs",
      "XGBoost",
      "ECG",
      "UAV",
      "RMSE",
      "NTK"
    ],
    "retrieved_papers": [
      {
        "title": "Harnessing the Power of Large Language Models for Natural Language to\n  First-Order Logic Translation,",
        "abstract": "Translating natural language sentences to first-order logic (NL-FOL\ntranslation) is a longstanding challenge in the NLP and formal logic\nliterature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for\nNL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of\ndirectly translating natural language into FOL rules, which outperforms\nGPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,\nand can achieve similar performance as GPT-4 with a fraction of the cost. This\ncorrection ability was achieved by a novel supervised fine-tuning (SFT) +\nreinforcement learning with human feedback (RLHF) framework, which initially\ntrains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought\nreasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier\nas the reward model.\n  To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel\ngener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset\nof 34K high-quality and diverse sentence-level NL-FOL pairs collected from\nGPT-4. The dataset was created by implementing a pipeline that prompts GPT-4\nfor pairs, and dynamically adjusts the prompts to ensure the collection of\npairs with rich and diverse contexts at different levels of complexity, and\nverifies the validity of the generated FOL rules. Codes, weights, and data are\navailable at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small\n\\text{https://github.com/gblackout/LogicLLaMA}}}$.",
        "score": 5.376354694366455
      },
      {
        "title": "FOLIO: Natural Language Reasoning with First-Order Logic,",
        "abstract": "Large language models (LLMs) have achieved remarkable performance on a\nvariety of natural language understanding tasks. However, existing benchmarks\nare inadequate in measuring the complex logical reasoning capabilities of a\nmodel. We present FOLIO, a human-annotated, logically complex and diverse\ndataset for reasoning in natural language (NL), equipped with first-order logic\n(FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each\npaired with one of 487 sets of premises used to deductively reason for the\nvalidity of each conclusion. The logical correctness of the premises and\nconclusions is ensured by their FOL annotations, which are automatically\nverified by an FOL inference engine. In addition to the main NL reasoning task,\nNL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our\nexperiments on FOLIO systematically evaluate the FOL reasoning ability of\nsupervised fine-tuning on medium-sized language models. For both NL reasoning\nand NL-FOL translation, we benchmark multiple state-of-the-art language models.\nOur results show that a subset of FOLIO presents a challenge for one of the\nmost capable {Large Language Model (LLM)} publicly available, GPT-4.",
        "score": 2.538381576538086
      },
      {
        "title": "Coupling Large Language Models with Logic Programming for Robust and\n  General Reasoning from Text,",
        "abstract": "While large language models (LLMs), such as GPT-3, appear to be robust and\ngeneral, their reasoning ability is not at a level to compete with the best\nmodels trained for specific natural language reasoning problems. In this study,\nwe observe that a large language model can serve as a highly effective few-shot\nsemantic parser. It can convert natural language sentences into a logical form\nthat serves as input for answer set programs, a logic-based declarative\nknowledge representation formalism. The combination results in a robust and\ngeneral system that can handle multiple question-answering tasks without\nrequiring retraining for each new task. It only needs a few examples to guide\nthe LLM's adaptation to a specific task, along with reusable ASP knowledge\nmodules that can be applied to multiple tasks. We demonstrate that this method\nachieves state-of-the-art performance on several NLP benchmarks, including\nbAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot\nplanning tasks that an LLM alone fails to solve.",
        "score": 0.7361197471618652
      },
      {
        "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability\n  of Large Language Models,",
        "abstract": "Recently developed large language models (LLMs) have been shown to perform\nremarkably well on a wide range of language understanding tasks. But, can they\nreally \"reason\" over the natural language? This question has been receiving\nsignificant research attention and many reasoning skills such as commonsense,\nnumerical, and qualitative have been studied. However, the crucial skill\npertaining to 'logical reasoning' has remained underexplored. Existing work\ninvestigating this reasoning ability of LLMs has focused only on a couple of\ninference rules (such as modus ponens and modus tollens) of propositional and\nfirst-order logic. Addressing the above limitation, we comprehensively evaluate\nthe logical reasoning ability of LLMs on 25 different reasoning patterns\nspanning over propositional, first-order, and non-monotonic logics. To enable\nsystematic evaluation, we introduce LogicBench, a natural language\nquestion-answering dataset focusing on the use of a single inference rule. We\nconduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini,\nLlama-2, and Mistral using chain-of-thought prompting. Experimental results\nshow that existing LLMs do not fare well on LogicBench; especially, they\nstruggle with instances involving complex reasoning and negations. Furthermore,\nthey sometimes overlook contextual information necessary for reasoning to\narrive at the correct conclusion. We believe that our work and findings\nfacilitate future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data and code are available at\nhttps://github.com/Mihir3009/LogicBench.",
        "score": -0.5872920155525208
      },
      {
        "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models,",
        "abstract": "Recently, large language models (LLMs), including notable models such as\nGPT-4 and burgeoning community models, have showcased significant general\nlanguage understanding abilities. However, there has been a scarcity of\nattempts to assess the logical reasoning capacities of these LLMs, an essential\nfacet of natural language understanding. To encourage further investigation in\nthis area, we introduce GLoRE, a meticulously assembled General Logical\nReasoning Evaluation benchmark comprised of 12 datasets that span three\ndifferent types of tasks. Our experimental results show that compared to the\nperformance of human and supervised fine-tuning, the logical reasoning\ncapabilities of open LLM models necessitate additional improvement; ChatGPT and\nGPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing\nChatGPT by a large margin. We propose a self-consistency probing method to\nenhance the accuracy of ChatGPT and a fine-tuned method to boost the\nperformance of an open LLM. We release the datasets and evaluation programs to\nfacilitate future research.",
        "score": -0.9063416719436646
      },
      {
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models\n  via Reinforced Evol-Instruct,",
        "abstract": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical reasoning\nabilities of Llama-2, by applying our proposed Reinforcement Learning from\nEvol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive\nexperiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we\nreveal the extraordinary capabilities of our model. WizardMath surpasses all\nother open-source LLMs by a substantial margin. Furthermore, our model even\noutperforms ChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k,\nsimultaneously surpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More\ndetails and model weights are public at https://github.com/nlpxucan/WizardLM\nand https://huggingface.co/WizardLM.",
        "score": -1.0397077798843384
      },
      {
        "title": "Exploring Advanced Large Language Models with LLMsuite,",
        "abstract": "This tutorial explores the advancements and challenges in the development of\nLarge Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent\nlimitations like temporal knowledge cutoffs, mathematical inaccuracies, and the\ngeneration of incorrect information, proposing solutions like Retrieval\nAugmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks\nsuch as ReAct and LangChain. The integration of these techniques enhances LLM\nperformance and reliability, especially in multi-step reasoning and complex\ntask execution. The paper also covers fine-tuning strategies, including\ninstruction fine-tuning, parameter-efficient methods like LoRA, and\nReinforcement Learning from Human Feedback (RLHF) as well as Reinforced\nSelf-Training (ReST). Additionally, it provides a comprehensive survey of\ntransformer architectures and training techniques for LLMs. The toolbox for\nimplementing these techniques is publicly available at\nhttps://github.com/giorgioroffo/large_language_models_open_suite",
        "score": -1.350527286529541
      },
      {
        "title": "A & B == B & A: Triggering Logical Reasoning Failures in Large Language\n  Models,",
        "abstract": "Recent advancements in large language models (LLMs) have propelled Artificial\nIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\nas writing assistance, code generation, and machine translation. A significant\ndistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\n\"reason.\" However, evaluating the reasoning ability of LLMs remains a challenge\nas most existing evaluations focus on their accuracy on the downstream tasks\nrather than directly assessing their reasoning processes. Efforts have been\nmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\nsuffer from data leakage or limited scope. In this paper, we introduce\nLogicAsker, an automatic approach that comprehensively evaluates and improves\nthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\nbased on propositional and predicate logic. The results provide insights into\nLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\nwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\nChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\nfrom LogicAsker can find logical reasoning failures in different LLMs with a\nrate of 25\\% - 94\\%. In addition, the test cases of LogicAsker can be further\nused to design demonstration examples for in-context learning, which\neffectively improves the logical reasoning ability of LLMs, e.g., 10\\% for\nGPT-4. As far as we know, our work is the first to create prompts based on\ntesting results to improve LLMs' formal reasoning ability effectively. All the\ncode, data, and results will be released for reproduction and future research.",
        "score": -1.592613935470581
      },
      {
        "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning\n  for Repairing Model Outputs,",
        "abstract": "Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.",
        "score": -1.6106585264205933
      },
      {
        "title": "Natural Language Reinforcement Learning,",
        "abstract": "Reinforcement Learning (RL) has shown remarkable abilities in learning\npolicies for decision-making tasks. However, RL is often hindered by issues\nsuch as low sample efficiency, lack of interpretability, and sparse supervision\nsignals. To tackle these limitations, we take inspiration from the human\nlearning process and introduce Natural Language Reinforcement Learning (NLRL),\nwhich innovatively combines RL principles with natural language representation.\nSpecifically, NLRL redefines RL concepts like task objectives, policy, value\nfunction, Bellman equation, and policy iteration in natural language space. We\npresent how NLRL can be practically implemented with the latest advancements in\nlarge language models (LLMs) like GPT-4. Initial experiments over tabular MDPs\ndemonstrate the effectiveness, efficiency, and also interpretability of the\nNLRL framework.",
        "score": -1.6600626707077026
      },
      {
        "title": "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine\n  Translation to the Next Level,",
        "abstract": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved\nremarkable advancements in machine translation (MT) by leveraging extensive web\ncontent. On the other hand, translation-specific LLMs are built by pre-training\non domain-specific monolingual corpora and fine-tuning with human-annotated\ntranslation data. Despite the superior performance, these methods either demand\nan unprecedented scale of computing and data or substantial human editing and\nannotation efforts. In this paper, we develop Ladder, a novel model-agnostic\nand cost-effective tool to refine the performance of general LLMs for MT.\nLadder is trained on pseudo-refinement triplets which can be easily obtained\nfrom existing LLMs without additional human cost. During training, we propose a\nhierarchical fine-tuning strategy with an easy-to-hard schema, improving\nLadder's refining performance progressively. The trained Ladder can be\nseamlessly integrated with any general-purpose LLMs to boost their translation\nperformance. By utilizing Gemma-2B/7B as the backbone, Ladder-2B can elevate\nraw translations to the level of top-tier open-source models (e.g., refining\nBigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and Ladder-7B can\nfurther enhance model performance to be on par with the state-of-the-art GPT-4.\nExtensive ablation and analysis corroborate the effectiveness of Ladder in\ndiverse settings. Our code is available at https://github.com/fzp0424/Ladder",
        "score": -1.7527320384979248
      },
      {
        "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement\n  Learning and Large Language Models,",
        "abstract": "In this work, we review research studies that combine Reinforcement Learning\n(RL) and Large Language Models (LLMs), two areas that owe their momentum to the\ndevelopment of deep neural networks. We propose a novel taxonomy of three main\nclasses based on the way that the two model types interact with each other. The\nfirst class, RL4LLM, includes studies where RL is leveraged to improve the\nperformance of LLMs on tasks related to Natural Language Processing. L4LLM is\ndivided into two sub-categories depending on whether RL is used to directly\nfine-tune an existing LLM or to improve the prompt of the LLM. In the second\nclass, LLM4RL, an LLM assists the training of an RL model that performs a task\nthat is not inherently related to natural language. We further break down\nLLM4RL based on the component of the RL training framework that the LLM assists\nor replaces, namely reward shaping, goal generation, and policy function.\nFinally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a\ncommon planning framework without either of them contributing to training or\nfine-tuning of the other. We further branch this class to distinguish between\nstudies with and without natural language feedback. We use this taxonomy to\nexplore the motivations behind the synergy of LLMs and RL and explain the\nreasons for its success, while pinpointing potential shortcomings and areas\nwhere further research is needed, as well as alternative methodologies that\nserve the same goal.",
        "score": -1.853397250175476
      },
      {
        "title": "Natural Language-conditioned Reinforcement Learning with Inside-out Task\n  Language Development and Translation,",
        "abstract": "Natural Language-conditioned reinforcement learning (RL) enables the agents\nto follow human instructions. Previous approaches generally implemented\nlanguage-conditioned RL by providing human instructions in natural language\n(NL) and training a following policy. In this outside-in approach, the policy\nneeds to comprehend the NL and manage the task simultaneously. However, the\nunbounded NL examples often bring much extra complexity for solving concrete RL\ntasks, which can distract policy learning from completing the task. To ease the\nlearning burden of the policy, we investigate an inside-out scheme for natural\nlanguage-conditioned RL by developing a task language (TL) that is task-related\nand unique. The TL is used in RL to achieve highly efficient and effective\npolicy training. Besides, a translator is trained to translate NL into TL. We\nimplement this scheme as TALAR (TAsk Language with predicAte Representation)\nthat learns multiple predicates to model object relationships as the TL.\nExperiments indicate that TALAR not only better comprehends NL instructions but\nalso leads to a better instruction-following policy that improves 13.4% success\nrate and adapts to unseen expressions of NL instruction. The TL can also be an\neffective task abstraction, naturally compatible with hierarchical RL.",
        "score": -2.163228750228882
      },
      {
        "title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural\n  Language Understanding,",
        "abstract": "Recent strides in large language models (LLMs) have yielded remarkable\nperformance, leveraging reinforcement learning from human feedback (RLHF) to\nsignificantly enhance generation and alignment capabilities. However, RLHF\nencounters numerous challenges, including the objective mismatch issue, leading\nto suboptimal performance in Natural Language Understanding (NLU) tasks. To\naddress this limitation, we propose a novel Reinforcement Learning framework\nenhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs\nin NLU tasks. By incorporating label-sensitive pairs into reinforcement\nlearning, our method aims to adeptly capture nuanced label-sensitive semantic\nfeatures during RL, thereby enhancing natural language understanding.\nExperiments conducted on five diverse foundation models across eight tasks\nshowcase promising results. In comparison to Supervised Fine-tuning models\n(SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared\nwith RLHF models, the improvement averages at 0.69%. These results reveal the\neffectiveness of our method for LLMs in NLU tasks. Code and data available at:\nhttps://github.com/MagiaSN/ACL2024_RLLR.",
        "score": -2.205275058746338
      },
      {
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing:\n  Benchmarks, Baselines, and Building Blocks for Natural Language Policy\n  Optimization,",
        "abstract": "We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n  To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference. GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization) that learns to effectively reduce the combinatorial action\nspace in language generation. We show 1) that RL techniques are generally\nbetter than supervised methods at aligning LMs to human preferences; and 2)\nthat NLPO exhibits greater stability and performance than previous policy\ngradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic\nand human evaluations.",
        "score": -2.305603265762329
      },
      {
        "title": "FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning,",
        "abstract": "Learning paradigms for large language models (LLMs) currently tend to fall\nwithin either in-context learning (ICL) or full fine-tuning. Each of these\ncomes with their own trade-offs based on available data, model size, compute\ncost, ease-of-use, and final quality with neither solution performing well\nacross-the-board. In this article, we first describe ICL and fine-tuning\nparadigms in a way that highlights their natural connections. Based on these\nconnections, we propose a new learning paradigm called FIAT that fuses the best\nof these paradigms together, enabling prompt-engineered instructions and\nchain-of-thought reasoning with the very largest models while also using\nsimilar methods to perform parameter updates on a modestly-sized LLM with\nparameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of\nmultilingual tasks and observe that FIAT performs better than both ICL and\nfine-tuning at scales ranging from 100-10,000 training examples. We hope that\nFIAT provides a practical way of harnessing the full potential of LLMs without\nneeding to make a hard choice between learning paradigms.",
        "score": -2.5605087280273438
      },
      {
        "title": "A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs\n  Using the CGC-LORA Algorithm,",
        "abstract": "With the productive evolution of large language models (LLMs) in the field of\nnatural language processing (NLP), tons of effort has been made to effectively\nfine-tune common pre-trained LLMs to fulfill a variety of tasks in one or\nmultiple specific domain. In practice, there are two prevailing ways, in which\nthe adaptation can be achieved: (i) Multiple Independent Models: Pre-trained\nLLMs are fine-tuned a few times independently using the corresponding training\nsamples from each task. (ii) An Integrated Model: Samples from all tasks are\nemployed to fine-tune a pre-trianed LLM unitedly. To address the high computing\ncost and seesawing issue simultaneously, we propose a unified framework that\nimplements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel\nCustomized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work\naims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme.\nFor a given cluster of tasks, we design an innovative layer that contains two\ntypes of experts as additional trainable parameters to make LoRA be compatible\nwith MTL. To comprehensively evaluate the proposed framework, we conduct\nwell-designed experiments on two public datasets. The experimental results\ndemonstrate that the unified framework with CGC-LoRA modules achieves higher\nevaluation scores than all benchmarks on both two datasets.",
        "score": -2.562929153442383
      },
      {
        "title": "From Tarzan to Tolkien: Controlling the Language Proficiency Level of\n  LLMs for Content Generation,",
        "abstract": "We study the problem of controlling the difficulty level of text generated by\nLarge Language Models (LLMs) for contexts where end-users are not fully\nproficient, such as language learners. Using a novel framework, we evaluate the\neffectiveness of several key approaches for this task, including few-shot\nprompting, supervised finetuning, and reinforcement learning (RL), utilising\nboth GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.\n  Our findings reveal a large performance gap between GPT-4 and the open source\nmodels when using prompt-based strategies. However, we show how to bridge this\ngap with a careful combination of finetuning and RL alignment. Our best model,\nCALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and\nother strategies, at only a fraction of the cost. We further validate the\nquality of our results through a small-scale human study.",
        "score": -2.5992777347564697
      },
      {
        "title": "Adaptable Logical Control for Large Language Models,",
        "abstract": "Despite the success of Large Language Models (LLMs) on various tasks\nfollowing human instructions, controlling model generation at inference time\nposes a persistent challenge. In this paper, we introduce Ctrl-G, an adaptable\nframework that facilitates tractable and flexible control of LLM generation to\nreliably follow logical constraints. Ctrl-G combines any production-ready LLM\nwith a Hidden Markov Model, enabling LLM outputs to adhere to logical\nconstraints represented as deterministic finite automata. We show that Ctrl-G,\nwhen applied to a TULU2-7B model, outperforms GPT3.5 and GPT4 on the task of\ninteractive text editing: specifically, for the task of generating text\ninsertions/continuations following logical constraints, Ctrl-G achieves over\n30% higher satisfaction rate in human evaluation compared to GPT4. When applied\nto medium-size language models (e.g., GPT2-large), Ctrl-G also beats its\ncounterparts for constrained generation by large margins on standard\nbenchmarks. Additionally, as a proof-of-concept study, we experiment Ctrl-G on\nthe Grade School Math benchmark to assist LLM reasoning, foreshadowing the\napplication of Ctrl-G, as well as other constrained generation approaches,\nbeyond traditional language generation tasks.",
        "score": -2.725924491882324
      },
      {
        "title": "The Wisdom of Hindsight Makes Language Models Better Instruction\n  Followers,",
        "abstract": "Reinforcement learning has seen wide success in finetuning large language\nmodels to better align with instructions via human feedback. The so-called\nalgorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates\nimpressive performance on the GPT series models. However, the underlying\nReinforcement Learning (RL) algorithm is complex and requires an additional\ntraining pipeline for reward and value networks. In this paper, we consider an\nalternative approach: converting feedback to instruction by relabeling the\noriginal one and training the model for better alignment in a supervised\nmanner. Such an algorithm doesn't require any additional parameters except for\nthe original language model and maximally reuses the pretraining pipeline. To\nachieve this, we formulate instruction alignment problem for language models as\na goal-reaching problem in decision making. We propose Hindsight Instruction\nRelabeling (HIR), a novel algorithm for aligning language models with\ninstructions. The resulting two-stage algorithm shed light to a family of\nreward-free approaches that utilize the hindsightly relabeled instructions\nbased on feedback. We evaluate the performance of HIR extensively on 12\nchallenging BigBench reasoning tasks and show that HIR outperforms the baseline\nalgorithms and is comparable to or even surpasses supervised finetuning.",
        "score": -3.1604080200195312
      },
      {
        "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback,",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong\nmethod to align Pretrained Large Language Models (LLMs) with human preferences.\nBut training models with RLHF is computationally expensive, and an overall\ncomplex process. In this work, we study RLHF where the underlying models are\ntrained using the parameter efficient method of Low-Rank Adaptation (LoRA)\nintroduced by Hu et al. [2021]. We investigate the setup of \"Parameter\nEfficient Reinforcement Learning\" (PERL), in which we perform reward model\ntraining and reinforcement learning using LoRA. We compare PERL to conventional\nfine-tuning (full-tuning) across various configurations for 7 benchmarks,\nincluding 2 novel datasets, of reward modeling and reinforcement learning. We\nfind that PERL performs on par with the conventional RLHF setting, while\ntraining faster, and with less memory. This enables the high performance of\nRLHF, while reducing the computational burden that limits its adoption as an\nalignment technique for Large Language Models. We also release 2 novel thumbs\nup/down preference datasets: \"Taskmaster Coffee\", and \"Taskmaster Ticketing\" to\npromote research around RLHF.",
        "score": -3.234264850616455
      },
      {
        "title": "RLSF: Reinforcement Learning via Symbolic Feedback,",
        "abstract": "In recent years, large language models (LLMs) have had a dramatic impact on\nvarious sub-fields of AI, most notably on natural language understanding tasks.\nHowever, there is widespread agreement that the logical reasoning capabilities\nof contemporary LLMs are, at best, fragmentary (i.e., may work well on some\nproblem instances but fail dramatically on others). While traditional LLM\nfine-tuning approaches (e.g., those that use human feedback) do address this\nproblem to some degree, they suffer from many issues, including unsound\nblack-box reward models, difficulties in collecting preference data, and sparse\nscalar reward values.\n  To address these challenges, we propose a new training/fine-tuning paradigm\nwe refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which is\naimed at enhancing the reasoning capabilities of LLMs. In the RLSF setting, the\nLLM that is being trained/fine-tuned is considered as the RL agent, while the\nenvironment is allowed access to reasoning or domain knowledge tools (e.g.,\nsolvers, algebra systems). Crucially, in RLSF, these reasoning tools can\nprovide feedback to the LLMs via poly-sized certificates (e.g., proofs), that\ncharacterize errors in the LLM-generated object with respect to some\ncorrectness specification. The ability of RLSF-based training/fine-tuning to\nleverage certificate-generating symbolic tools enables sound fine-grained\n(token-level) reward signals to LLMs, and thus addresses the limitations of\ntraditional reward models mentioned above. Via extensive evaluations, we show\nthat our RLSF-based fine-tuning of LLMs outperforms traditional approaches on\ntwo different applications, namely, program synthesis from natural language\npseudo-code to programming language (C++) and solving the Game of 24.",
        "score": -3.4421980381011963
      },
      {
        "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement\n  based Transformers,",
        "abstract": "The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA\nencounter limitations in domain-specific tasks, with these models often lacking\ndepth and accuracy in specialized areas, and exhibiting a decrease in general\ncapabilities when fine-tuned, particularly analysis ability in small sized\nmodels. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement\nLearning from Human Feedback (RLHF) grounded in Proximal Policy Optimization\n(PPO), demonstrating remarkable ability in in-domain scenarios without\ncompromising general task performance. Our exploration of ICE-GRT highlights\nits understanding and reasoning ability to not only generate robust answers but\nalso to provide detailed analyses of the reasons behind the answer. This\ncapability marks a significant progression beyond the scope of Supervised\nFine-Tuning models. The success of ICE-GRT is dependent on several crucial\nfactors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage\nNormalization, etc. The ICE-GRT model exhibits state-of-the-art performance in\ndomain-specific tasks and across 12 general Language tasks against equivalent\nsize and even larger size LLMs, highlighting the effectiveness of our approach.\nWe provide a comprehensive analysis of the ICE-GRT, underscoring the\nsignificant advancements it brings to the field of LLM.",
        "score": -3.4975101947784424
      },
      {
        "title": "Improving Large Language Models via Fine-grained Reinforcement Learning\n  with Minimum Editing Constraint,",
        "abstract": "Reinforcement learning (RL) has been widely used in training large language\nmodels (LLMs) for preventing unexpected outputs, eg reducing harmfulness and\nerrors. However, existing RL methods mostly adopt the instance-level reward,\nwhich is unable to provide fine-grained supervision for complex reasoning\ntasks, and can not focus on the few key tokens that lead to the incorrectness.\nTo address it, we propose a new RL method named RLMEC that incorporates a\ngenerative model as the reward model, which is trained by the erroneous\nsolution rewriting task under the minimum editing constraint, and can produce\ntoken-level rewards for RL training. Based on the generative reward model, we\ndesign the token-level RL objective for training and an imitation-based\nregularization for stabilizing RL process. And the both objectives focus on the\nlearning of the key tokens for the erroneous solution, reducing the effect of\nother unimportant tokens. The experiment results on mathematical tasks and\nquestion-answering tasks have demonstrated the effectiveness of our approach.\nOur code and data are available at https://github.com/RUCAIBox/RLMEC.",
        "score": -3.733938455581665
      },
      {
        "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages\n  with Reinforcement Learning from Human Feedback,",
        "abstract": "A key technology for the development of large language models (LLMs) involves\ninstruction tuning that helps align the models' responses with human\nexpectations to realize impressive learning abilities. Two major approaches for\ninstruction tuning characterize supervised fine-tuning (SFT) and reinforcement\nlearning from human feedback (RLHF), which are currently applied to produce the\nbest commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for\nresearch and development efforts, various instruction-tuned open-source LLMs\nhave also been introduced recently, e.g., Alpaca, Vicuna, to name a few.\nHowever, existing open-source LLMs have only been instruction-tuned for English\nand a few popular languages, thus hindering their impacts and accessibility to\nmany other languages in the world. Among a few very recent work to explore\ninstruction tuning for LLMs in multiple languages, SFT has been used as the\nonly approach to instruction-tune LLMs for multiple languages. This has left a\nsignificant gap for fine-tuned LLMs based on RLHF in diverse languages and\nraised important questions on how RLHF can boost the performance of\nmultilingual instruction tuning. To overcome this issue, we present Okapi, the\nfirst system with instruction-tuned LLMs based on RLHF for multiple languages.\nOkapi introduces instruction and response-ranked data in 26 diverse languages\nto facilitate the experiments and development of future multilingual LLM\nresearch. We also present benchmark datasets to enable the evaluation of\ngenerative LLMs in multiple languages. Our experiments demonstrate the\nadvantages of RLHF for multilingual instruction over SFT for different base\nmodels and datasets. Our framework and resources are released at\nhttps://github.com/nlp-uoregon/Okapi.",
        "score": -4.015002727508545
      },
      {
        "title": "ReFT: Reasoning with Reinforced Fine-Tuning,",
        "abstract": "One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT.",
        "score": -4.503208637237549
      },
      {
        "title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for\n  Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback,",
        "abstract": "Large Multimodal Models (LMMs) excel at comprehending human instructions and\ndemonstrate remarkable results across a broad spectrum of tasks. Reinforcement\nLearning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs\nby aligning them with specific preferences. These methods primarily use\nranking-based feedback for entire generations. With advanced AI models\n(Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of\ndetailed feedback that are expensive for humans to provide. We propose a\ntwo-stage algorithm ARES that Alternates REinforcement Learning (RL) and\nSupervised Fine-Tuning (SFT). First, we request the Teacher to score how much\neach sentence contributes to solving the problem in a Chain-of-Thought (CoT).\nThis sentence-level feedback allows us to consider individual valuable\nsegments, providing more granular rewards for the RL procedure. Second, we ask\nthe Teacher to correct the wrong reasoning after the RL stage. The RL procedure\nrequires massive efforts for hyperparameter tuning and often generates errors\nlike repetitive words and incomplete sentences. With the correction feedback,\nwe stabilize the RL fine-tuned model through SFT. We conduct experiments on\nmulti-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of\nour proposal. ARES rationale reasoning achieves around 70% win rate against\nbaseline models judged by GPT-4o. Additionally, we observe that the improved\nrationale reasoning leads to a 2.5% increase in inference answer accuracy on\naverage for the multi-modal datasets.",
        "score": -4.524509429931641
      },
      {
        "title": "Learning to Generate Better Than Your LLM,",
        "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for\nfine-tuning Large Language Models (LLMs) for text generation. In particular,\nrecent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with\nusers after finetuning with RL. Capitalizing on key properties of text\ngeneration, we seek to investigate RL algorithms beyond general purpose\nalgorithms like Proximal Policy Optimization (PPO). In particular, we extend RL\nalgorithms to allow them to interact with a dynamic black-box guide LLM and\npropose RL with guided feedback (RLGF), a suite of RL algorithms for LLM\nfine-tuning. We provide two ways for the guide LLM to interact with the LLM to\nbe optimized for maximizing rewards. The guide LLM can generate text which\nserves as additional starting states for the RL optimization procedure. The\nguide LLM can also be used to complete the partial sentences generated by the\nLLM that is being optimized, treating the guide LLM as an expert to imitate and\nsurpass eventually. We experiment on the IMDB positive sentiment, CommonGen,\nand TL;DR summarization tasks. We show that our RL algorithms achieve higher\nperformance than supervised learning (SL) and the RL baseline PPO,\ndemonstrating the benefit of interaction with the guide LLM. On both CommonGen\nand TL;DR, we not only outperform our SL baselines but also improve upon PPO\nacross a variety of metrics beyond the one we optimized for. Our code can be\nfound at https://github.com/Cornell-RL/tril.",
        "score": -4.722278118133545
      },
      {
        "title": "ReFT: Representation Finetuning for Language Models,",
        "abstract": "Parameter-efficient finetuning (PEFT) methods seek to adapt large neural\nmodels via updates to a small number of weights. However, much prior\ninterpretability work has shown that representations encode rich semantic\ninformation, suggesting that editing representations might be a more powerful\nalternative. We pursue this hypothesis by developing a family of Representation\nFinetuning (ReFT) methods. ReFT methods operate on a frozen base model and\nlearn task-specific interventions on hidden representations. We define a strong\ninstance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we\nidentify an ablation of this method that trades some performance for increased\nefficiency. Both are drop-in replacements for existing PEFTs and learn\ninterventions that are 15x--65x more parameter-efficient than LoRA. We showcase\nLoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,\ninstruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the\nbest balance of efficiency and performance, and almost always outperform\nstate-of-the-art PEFTs. We release a generic ReFT training library publicly at\nhttps://github.com/stanfordnlp/pyreft.",
        "score": -5.06187105178833
      },
      {
        "title": "Efficient RLHF: Reducing the Memory Usage of PPO,",
        "abstract": "Reinforcement Learning with Human Feedback (RLHF) has revolutionized language\nmodeling by aligning models with human preferences. However, the RL stage,\nProximal Policy Optimization (PPO), requires over 3x the memory of Supervised\nFine-Tuning (SFT), making it infeasible to use for most practitioners. To\naddress this issue, we present a comprehensive analysis the memory usage,\nperformance, and training time of memory-savings techniques for PPO. We\nintroduce Hydra-RLHF by first integrating the SFT and Reward models and then\ndynamically turning LoRA \"off\" during training. Our experiments show: 1. Using\nLoRA during PPO reduces its memory usage to be smaller than SFT while improving\nalignment across four public benchmarks, and 2. Hydra-PPO reduces the latency\nper sample of LoRA-PPO by up to 65% while maintaining its performance. Our\nresults demonstrate that Hydra-PPO is a simple and promising solution for\nenabling more widespread usage of RLHF.",
        "score": -5.944216251373291
      }
    ]
  },
  {
    "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large\n  Language Models",
    "abstract": "  Large language models (LLMs) have achieved remarkable performance in natural\nlanguage understanding and generation tasks. However, they often suffer from\nlimitations such as difficulty in incorporating new knowledge, generating\nhallucinations, and explaining their reasoning process. To address these\nchallenges, we propose a novel prompting pipeline, named \\method, that\nleverages knowledge graphs (KGs) to enhance LLMs' inference and transparency.\nOur method enables LLMs to comprehend KG inputs and infer with a combination of\nimplicit and external knowledge. Moreover, our method elicits the mind map of\nLLMs, which reveals their reasoning pathways based on the ontology of\nknowledge. We evaluate our method on diverse question \\& answering tasks,\nespecially in medical domains, and show significant improvements over\nbaselines. We also introduce a new hallucination evaluation benchmark and\nanalyze the effects of different components of our method. Our results\ndemonstrate the effectiveness and robustness of our method in merging knowledge\nfrom LLMs and KGs for combined inference. To reproduce our results and extend\nthe framework further, we make our codebase available at\nhttps://github.com/wyl-willing/MindMap.\n",
    "related_paper_titles": [
      "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
      "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge\n  Graph Question Answering",
      "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model\n  on Knowledge Graph"
    ],
    "related_paper_abstract": [
      "  Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.\n",
      "  Large Language Models (LLMs) are capable of performing zero-shot closed-book\nquestion answering tasks, based on their internal knowledge stored in\nparameters during pre-training. However, such internalized knowledge might be\ninsufficient and incorrect, which could lead LLMs to generate factually wrong\nanswers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.\nTo this end, we propose to augment the knowledge directly in the input of LLMs.\nSpecifically, we first retrieve the relevant facts to the input question from\nthe knowledge graph based on semantic similarities between the question and its\nassociated facts. After that, we prepend the retrieved facts to the input\nquestion in the form of the prompt, which is then forwarded to LLMs to generate\nthe answer. Our framework, Knowledge-Augmented language model PromptING\n(KAPING), requires no model training, thus completely zero-shot. We validate\nthe performance of our KAPING framework on the knowledge graph question\nanswering task, that aims to answer the user's question based on facts over a\nknowledge graph, on which ours outperforms relevant zero-shot baselines by up\nto 48% in average, across multiple LLMs of various sizes.\n",
      "  Although large language models (LLMs) have achieved significant success in\nvarious tasks, they often struggle with hallucination problems, especially in\nscenarios requiring deep and responsible reasoning. These issues could be\npartially addressed by introducing external knowledge graphs (KG) in LLM\nreasoning. In this paper, we propose a new LLM-KG integrating paradigm\n``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to\ninteractively explore related entities and relations on KGs and perform\nreasoning based on the retrieved knowledge. We further implement this paradigm\nby introducing a new approach called Think-on-Graph (ToG), in which the LLM\nagent iteratively executes beam search on KG, discovers the most promising\nreasoning paths, and returns the most likely reasoning results. We use a number\nof well-designed experiments to examine and illustrate the following advantages\nof ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has\nthe ability of knowledge traceability and knowledge correctability by\nleveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible\nplug-and-play framework for different LLMs, KGs and prompting strategies\nwithout any additional training cost; 4) the performance of ToG with small LLM\nmodels could exceed large LLM such as GPT-4 in certain scenarios and this\nreduces the cost of LLM deployment and application. As a training-free method\nwith lower computational cost and better generality, ToG achieves overall SOTA\nin 6 out of 9 datasets where most previous SOTAs rely on additional training.\n"
    ],
    "entities": [
      "Large Language Models Large",
      "Large Language Models Large Language Models",
      "KGE",
      "KV",
      "SFT",
      "ICL",
      "Direct Preference",
      "Mistral",
      "Verilog",
      "SQL",
      "LLaMA",
      "Global",
      "Large Language",
      "MATH",
      "Copilot",
      "Chinese",
      "IFT",
      "Gemini",
      "NLG",
      "ToM",
      "Korean",
      "MMLU",
      "GSM8K",
      "Llama",
      "APIs",
      "API",
      "Retrieval Augmented",
      "English",
      "KGs",
      "OpenAI"
    ],
    "retrieved_papers": [
      {
        "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large\n  Language Models,",
        "abstract": "Large language models (LLMs) have achieved remarkable performance in natural\nlanguage understanding and generation tasks. However, they often suffer from\nlimitations such as difficulty in incorporating new knowledge, generating\nhallucinations, and explaining their reasoning process. To address these\nchallenges, we propose a novel prompting pipeline, named \\method, that\nleverages knowledge graphs (KGs) to enhance LLMs' inference and transparency.\nOur method enables LLMs to comprehend KG inputs and infer with a combination of\nimplicit and external knowledge. Moreover, our method elicits the mind map of\nLLMs, which reveals their reasoning pathways based on the ontology of\nknowledge. We evaluate our method on diverse question \\& answering tasks,\nespecially in medical domains, and show significant improvements over\nbaselines. We also introduce a new hallucination evaluation benchmark and\nanalyze the effects of different components of our method. Our results\ndemonstrate the effectiveness and robustness of our method in merging knowledge\nfrom LLMs and KGs for combined inference. To reproduce our results and extend\nthe framework further, we make our codebase available at\nhttps://github.com/wyl-willing/MindMap.",
        "score": 6.525595188140869
      },
      {
        "title": "KnowGPT: Knowledge Graph based Prompting for Large Language Models,",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nmany real-world applications. Nonetheless, LLMs are often criticized for their\ntendency to produce hallucinations, wherein the models fabricate incorrect\nstatements on tasks beyond their knowledge and perception. To alleviate this\nissue, researchers have explored leveraging the factual knowledge in knowledge\ngraphs (KGs) to ground the LLM's responses in established facts and principles.\nHowever, most state-of-the-art LLMs are closed-source, making it challenging to\ndevelop a prompting framework that can efficiently and effectively integrate\nKGs into LLMs with hard prompts only. Generally, existing KG-enhanced LLMs\nusually suffer from three critical issues, including huge search space, high\nAPI costs, and laborious prompt engineering, that impede their widespread\napplication in practice. To this end, we introduce a novel Knowledge Graph\nbased PrompTing framework, namely KnowGPT, to enhance LLMs with domain\nknowledge. KnowGPT contains a knowledge extraction module to extract the most\ninformative knowledge from KGs, and a context-aware prompt construction module\nto automatically convert extracted knowledge into effective prompts.\nExperiments on three benchmarks demonstrate that KnowGPT significantly\noutperforms all competitors. Notably, KnowGPT achieves a 92.6% accuracy on\nOpenbookQA leaderboard, comparable to human-level performance.",
        "score": 4.506243705749512
      },
      {
        "title": "Large Language Models Can Better Understand Knowledge Graphs Than We\n  Thought,",
        "abstract": "As the parameter scale of large language models (LLMs) grows, jointly\ntraining knowledge graph (KG) embeddings with model parameters to enhance LLM\ncapabilities becomes increasingly costly. Consequently, the community has shown\ninterest in developing prompt strategies that effectively integrate KG\ninformation into LLMs. However, the format for incorporating KGs into LLMs\nlacks standardization; for instance, KGs can be transformed into linearized\ntriples or natural language (NL) text. Current prompting methods often rely on\na trial-and-error approach, leaving researchers with an incomplete\nunderstanding of which KG input format best facilitates LLM comprehension of KG\ncontent. To elucidate this, we design a series of experiments to explore LLMs'\nunderstanding of different KG input formats within the context of prompt\nengineering. Our analysis examines both literal and attention distribution\nlevels. Through extensive experiments, we indicate a counter-intuitive\nphenomenon: when addressing fact-related questions, unordered linearized\ntriples are more effective for LLMs' understanding of KGs compared to fluent NL\ntext. Furthermore, noisy, incomplete, or marginally relevant subgraphs can\nstill enhance LLM performance. Finally, different LLMs have distinct\npreferences for different formats of organizing unordered triples.",
        "score": 3.601059913635254
      },
      {
        "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models,",
        "abstract": "We introduce Graph of Thoughts (GoT): a framework that advances prompting\ncapabilities in large language models (LLMs) beyond those offered by paradigms\nsuch as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information generated by an LLM as\nan arbitrary graph, where units of information (\"LLM thoughts\") are vertices,\nand edges correspond to dependencies between these vertices. This approach\nenables combining arbitrary LLM thoughts into synergistic outcomes, distilling\nthe essence of whole networks of thoughts, or enhancing thoughts using feedback\nloops. We illustrate that GoT offers advantages over state of the art on\ndifferent tasks, for example increasing the quality of sorting by 62% over ToT,\nwhile simultaneously reducing costs by >31%. We ensure that GoT is extensible\nwith new thought transformations and thus can be used to spearhead new\nprompting schemes. This work brings the LLM reasoning closer to human thinking\nor brain mechanisms such as recurrence, both of which form complex networks.",
        "score": 3.5717766284942627
      },
      {
        "title": "Graph Elicitation for Guiding Multi-Step Reasoning in Large Language\n  Models,",
        "abstract": "Chain-of-Thought (CoT) prompting along with sub-question generation and\nanswering has enhanced multi-step reasoning capabilities of Large Language\nModels (LLMs). However, prompting the LLMs to directly generate sub-questions\nis suboptimal since they sometimes generate redundant or irrelevant questions.\nTo deal with them, we propose a GE-Reasoning method, which directs LLMs to\ngenerate proper sub-questions and corresponding answers. Concretely, given an\ninput question, we first prompt the LLM to generate knowledge triplets, forming\na graph representation of the question. Unlike conventional knowledge triplets,\nour approach allows variables as head or tail entities, effectively\nrepresenting a question as knowledge triplets. Second, for each triplet, the\nLLM generates a corresponding sub-question and answer along with using\nknowledge retrieval. If the prediction confidence exceeds a threshold, the\nsub-question and prediction are incorporated into the prompt for subsequent\nprocessing. This approach encourages that sub-questions are grounded in the\nextracted knowledge triplets, reducing redundancy and irrelevance. Our\nexperiments demonstrate that our approach outperforms previous CoT prompting\nmethods and their variants on multi-hop question answering benchmark datasets.",
        "score": 3.5628528594970703
      },
      {
        "title": "Graph Neural Prompting with Large Language Models,",
        "abstract": "Large language models (LLMs) have shown remarkable generalization capability\nwith exceptional performance in various language modeling tasks. However, they\nstill exhibit inherent limitations in precisely capturing and returning\ngrounded knowledge. While existing work has explored utilizing knowledge graphs\n(KGs) to enhance language modeling via joint training and customized model\narchitectures, applying this to LLMs is problematic owing to their large number\nof parameters and high computational cost. Therefore, how to enhance\npre-trained LLMs using grounded knowledge, e.g., retrieval-augmented\ngeneration, remains an open question. In this work, we propose Graph Neural\nPrompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in\nlearning beneficial knowledge from KGs. GNP encompasses various designs,\nincluding a standard graph neural network encoder, a cross-modality pooling\nmodule, a domain projector, and a self-supervised link prediction objective.\nExtensive experiments on multiple datasets demonstrate the superiority of GNP\non both commonsense and biomedical reasoning tasks across different LLM sizes\nand settings. Code is available at https://github.com/meettyj/GNP.",
        "score": 3.475781202316284
      },
      {
        "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge\n  Graph-Integrated Collaboration,",
        "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in a\nmultitude of Natural Language Processing (NLP) tasks, they encounter challenges\nin practical applications, including issues with hallucinations, inadequate\nknowledge updating, and limited transparency in the reasoning process. To\novercome these limitations, this study innovatively proposes a collaborative\ntraining-free reasoning scheme involving tight cooperation between Knowledge\nGraph (KG) and LLMs. This scheme first involves using LLMs to iteratively\nexplore KG, selectively retrieving a task-relevant knowledge subgraph to\nsupport reasoning. The LLMs are then guided to further combine inherent\nimplicit knowledge to reason on the subgraph while explicitly elucidating the\nreasoning process. Through such a cooperative approach, our scheme achieves\nmore reliable knowledge-based reasoning and facilitates the tracing of the\nreasoning results. Experimental results show that our scheme significantly\nprogressed across multiple datasets, notably achieving over a 10% improvement\non the QALD10 dataset compared to the best baseline and the fine-tuned\nstate-of-the-art (SOTA) work. Building on this success, this study hopes to\noffer a valuable reference for future research in the fusion of KG and LLMs,\nthereby enhancing LLMs' proficiency in solving complex issues.",
        "score": 3.4267256259918213
      },
      {
        "title": "Demystifying Chains, Trees, and Graphs of Thoughts,",
        "abstract": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.",
        "score": 3.2402122020721436
      },
      {
        "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model\n  on Knowledge Graph,",
        "abstract": "Although large language models (LLMs) have achieved significant success in\nvarious tasks, they often struggle with hallucination problems, especially in\nscenarios requiring deep and responsible reasoning. These issues could be\npartially addressed by introducing external knowledge graphs (KG) in LLM\nreasoning. In this paper, we propose a new LLM-KG integrating paradigm\n``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to\ninteractively explore related entities and relations on KGs and perform\nreasoning based on the retrieved knowledge. We further implement this paradigm\nby introducing a new approach called Think-on-Graph (ToG), in which the LLM\nagent iteratively executes beam search on KG, discovers the most promising\nreasoning paths, and returns the most likely reasoning results. We use a number\nof well-designed experiments to examine and illustrate the following advantages\nof ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has\nthe ability of knowledge traceability and knowledge correctability by\nleveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible\nplug-and-play framework for different LLMs, KGs and prompting strategies\nwithout any additional training cost; 4) the performance of ToG with small LLM\nmodels could exceed large LLM such as GPT-4 in certain scenarios and this\nreduces the cost of LLM deployment and application. As a training-free method\nwith lower computational cost and better generality, ToG achieves overall SOTA\nin 6 out of 9 datasets where most previous SOTAs rely on additional training.",
        "score": 3.2219088077545166
      },
      {
        "title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs,",
        "abstract": "Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.",
        "score": 3.214050769805908
      },
      {
        "title": "Large Knowledge Model: Perspectives and Challenges,",
        "abstract": "Humankind's understanding of the world is fundamentally linked to our\nperception and cognition, with \\emph{human languages} serving as one of the\nmajor carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language\nModels} (LLMs) like ChatGPT epitomize the pre-training of extensive,\nsequence-based world knowledge into neural networks, facilitating the\nprocessing and manipulation of this knowledge in a parametric space. This\narticle explores large models through the lens of \"knowledge\". We initially\ninvestigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in\nenhancing LLMs, covering aspects like knowledge-augmented language model,\nstructure-inducing pre-training, knowledgeable prompts, structured CoT,\nknowledge editing, semantic tools for LLM and knowledgeable AI agents.\nSubsequently, we examine how LLMs can boost traditional symbolic knowledge\nbases, encompassing aspects like using LLM as KG builder and controller,\nstructured knowledge pretraining, and LLM-enhanced symbolic reasoning.\nConsidering the intricate nature of human knowledge, we advocate for the\ncreation of \\emph{Large Knowledge Models} (LKM), specifically engineered to\nmanage diversified spectrum of knowledge structures. This promising undertaking\nwould entail several key challenges, such as disentangling knowledge base from\nlanguage models, cognitive alignment with human knowledge, integration of\nperception and cognition, and building large commonsense models for interacting\nwith physical world, among others. We finally propose a five-\"A\" principle to\ndistinguish the concept of LKM.",
        "score": 3.1436212062835693
      },
      {
        "title": "Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize\n  Encoded Knowledge,",
        "abstract": "Large Language Models (LLMs) have recently showcased remarkable\ngeneralizability in various domains. Despite their extensive knowledge, LLMs\nstill face challenges in efficiently utilizing encoded knowledge to develop\naccurate and logical reasoning processes. To mitigate this problem, we\nintroduced Hint-before-Solving Prompting (HSP), which guides the model to\ngenerate hints (e.g., specific knowledge or key ideas) for solving the problem\nand then generate solutions containing intermediate reasoning steps. Since HSP\nis orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied\nHSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results\nof extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs\ndemonstrate that HSP can effectively improve the accuracy of reasoning tasks:\n(1) By applying high-quality hint-enhanced HSP to CoT prompting,\nLlama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free\nLLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned\nLlemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We\nmake our code and dataset publicly available at\n\\url{https://github.com/jinlanfu/HSP}.",
        "score": 3.0156121253967285
      },
      {
        "title": "Semi-Structured Chain-of-Thought: Integrating Multiple Sources of\n  Knowledge for Improved Language Model Reasoning,",
        "abstract": "An important open question in the use of large language models for\nknowledge-intensive tasks is how to effectively integrate knowledge from three\nsources: the model's parametric memory, external structured knowledge, and\nexternal unstructured knowledge. Most existing prompting methods either rely on\none or two of these sources, or require repeatedly invoking large language\nmodels to generate similar or identical content. In this work, we overcome\nthese limitations by introducing a novel semi-structured prompting approach\nthat seamlessly integrates the model's parametric memory with unstructured\nknowledge from text documents and structured knowledge from knowledge graphs.\nExperimental results on open-domain multi-hop question answering datasets\ndemonstrate that our prompting method significantly surpasses existing\ntechniques, even exceeding those that require fine-tuning.",
        "score": 2.9369163513183594
      },
      {
        "title": "Think-on-Graph 2.0: Deep and Interpretable Large Language Model\n  Reasoning with Knowledge Graph-guided Retrieval,",
        "abstract": "Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.",
        "score": 2.892179012298584
      },
      {
        "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge\n  Graph-based Retrofitting,",
        "abstract": "Incorporating factual knowledge in knowledge graph is regarded as a promising\napproach for mitigating the hallucination of large language models (LLMs).\nExisting methods usually only use the user's input to query the knowledge\ngraph, thus failing to address the factual hallucination generated by LLMs\nduring its reasoning process. To address this problem, this paper proposes\nKnowledge Graph-based Retrofitting (KGR), a new framework that incorporates\nLLMs with KGs to mitigate factual hallucination during the reasoning process by\nretrofitting the initial draft responses of LLMs based on the factual knowledge\nstored in KGs. Specifically, KGR leverages LLMs to extract, select, validate,\nand retrofit factual statements within the model-generated responses, which\nenables an autonomous knowledge verifying and refining procedure without any\nadditional manual efforts. Experiments show that KGR can significantly improve\nthe performance of LLMs on factual QA benchmarks especially when involving\ncomplex reasoning processes, which demonstrates the necessity and effectiveness\nof KGR in mitigating hallucination and enhancing the reliability of LLMs.",
        "score": 2.855912923812866
      },
      {
        "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step\n  Reasoning by Exploring Graph Structure of the Text,",
        "abstract": "Although Large Language Models (LLMs) excel at addressing straightforward\nreasoning tasks, they frequently struggle with difficulties when confronted by\nmore complex multi-step reasoning due to a range of factors. Firstly, natural\nlanguage often encompasses complex relationships among entities, making it\nchallenging to maintain a clear reasoning chain over longer spans. Secondly,\nthe abundance of linguistic diversity means that the same entities and\nrelationships can be expressed using different terminologies and structures,\ncomplicating the task of identifying and establishing connections between\nmultiple pieces of information. Graphs provide an effective solution to\nrepresent data rich in relational information and capture long-term\ndependencies among entities. To harness the potential of graphs, our paper\nintroduces Structure Guided Prompt, an innovative three-stage task-agnostic\nprompting framework designed to improve the multi-step reasoning capabilities\nof LLMs in a zero-shot setting. This framework explicitly converts unstructured\ntext into a graph via LLMs and instructs them to navigate this graph using\ntask-specific strategies to formulate responses. By effectively organizing\ninformation and guiding navigation, it enables LLMs to provide more accurate\nand context-aware responses. Our experiments show that this framework\nsignificantly enhances the reasoning capabilities of LLMs, enabling them to\nexcel in a broader spectrum of natural language scenarios.",
        "score": 2.8347067832946777
      },
      {
        "title": "Why Can Large Language Models Generate Correct Chain-of-Thoughts?,",
        "abstract": "This paper delves into the capabilities of large language models (LLMs),\nspecifically focusing on advancing the theoretical comprehension of\nchain-of-thought prompting. We investigate how LLMs can be effectively induced\nto generate a coherent chain of thoughts. To achieve this, we introduce a\ntwo-level hierarchical graphical model tailored for natural language\ngeneration. Within this framework, we establish a compelling geometrical\nconvergence rate that gauges the likelihood of an LLM-generated chain of\nthoughts compared to those originating from the true language. Our findings\nprovide a theoretical justification for the ability of LLMs to produce the\ncorrect sequence of thoughts (potentially) explaining performance gains in\ntasks demanding reasoning skills.",
        "score": 2.8123717308044434
      },
      {
        "title": "ThoughtSource: A central hub for large language model reasoning data,",
        "abstract": "Large language models (LLMs) such as GPT-4 have recently demonstrated\nimpressive results across a wide range of tasks. LLMs are still limited,\nhowever, in that they frequently fail at complex reasoning, their reasoning\nprocesses are opaque, they are prone to 'hallucinate' facts, and there are\nconcerns about their underlying biases. Letting models verbalize reasoning\nsteps as natural language, a technique known as chain-of-thought prompting, has\nrecently been proposed as a way to address some of these issues. Here we\npresent ThoughtSource, a meta-dataset and software library for chain-of-thought\n(CoT) reasoning. The goal of ThoughtSource is to improve future artificial\nintelligence systems by facilitating qualitative understanding of CoTs,\nenabling empirical evaluations, and providing training data. This first release\nof ThoughtSource integrates seven scientific/medical, three general-domain and\nfive math word question answering datasets.",
        "score": 2.6961729526519775
      },
      {
        "title": "Infusing Knowledge into Large Language Models with Contextual Prompts,",
        "abstract": "Knowledge infusion is a promising method for enhancing Large Language Models\nfor domain-specific NLP tasks rather than pre-training models over large data\nfrom scratch. These augmented LLMs typically depend on additional pre-training\nor knowledge prompts from an existing knowledge graph, which is impractical in\nmany applications. In contrast, knowledge infusion directly from relevant\ndocuments is more generalisable and alleviates the need for structured\nknowledge graphs while also being useful for entities that are usually not\nfound in any knowledge graph. With this motivation, we propose a simple yet\ngeneralisable approach for knowledge infusion by generating prompts from the\ncontext in the input text. Our experiments show the effectiveness of our\napproach which we evaluate by probing the fine-tuned LLMs.",
        "score": 2.5618174076080322
      },
      {
        "title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning,",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in\nnatural language processing tasks by leveraging chain of thought (CoT) that\nenables step-by-step thinking. Extending LLMs with multimodal capabilities is\nthe recent interest, but incurs computational cost and requires substantial\nhardware resources. To address these challenges, we propose KAM-CoT a framework\nthat integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities\nfor a comprehensive understanding of multimodal tasks. KAM-CoT adopts a\ntwo-stage training process with KG grounding to generate effective rationales\nand answers. By incorporating external knowledge from KGs during reasoning, the\nmodel gains a deeper contextual understanding reducing hallucinations and\nenhancing the quality of answers. This knowledge-augmented CoT reasoning\nempowers the model to handle questions requiring external context, providing\nmore informed answers. Experimental findings show KAM-CoT outperforms the\nstate-of-the-art methods. On the ScienceQA dataset, we achieve an average\naccuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by\n10%. Remarkably, KAM-CoT achieves these results with only 280M trainable\nparameters at a time, demonstrating its cost-efficiency and effectiveness.",
        "score": 2.447848320007324
      },
      {
        "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning\n  in Large Language Models,",
        "abstract": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving\nrationales, has impressively unlocked the reasoning potential of large language\nmodels (LLMs). Yet, the standard CoT is less effective in problems demanding\nmultiple reasoning steps. This limitation arises from the complex reasoning\nprocess in multi-step problems: later stages often depend on the results of\nseveral steps earlier, not just the results of the immediately preceding step.\nSuch complexities suggest the reasoning process is naturally represented as a\ngraph. The almost linear and straightforward structure of CoT prompting,\nhowever, struggles to capture this complex reasoning graph. To address this\nchallenge, we propose Residual Connection Prompting (RESPROMPT), a new\nprompting strategy that advances multi-step reasoning in LLMs. Our key idea is\nto reconstruct the reasoning graph within prompts. We achieve this by\nintegrating necessary connections-links present in the reasoning graph but\nmissing in the linear CoT flow-into the prompts. Termed \"residual connections\",\nthese links are pivotal in morphing the linear CoT structure into a graph\nrepresentation, effectively capturing the complex reasoning graphs inherent in\nmulti-step problems. We evaluate RESPROMPT on six benchmarks across three\ndiverse domains: math, sequential, and commonsense reasoning. For the\nopen-sourced LLaMA family of models, RESPROMPT yields a significant average\nreasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B.\nBreakdown analysis further highlights RESPROMPT particularly excels in complex\nmulti-step reasoning: for questions demanding at least five reasoning steps,\nRESPROMPT outperforms the best CoT based benchmarks by a remarkable average\nimprovement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive\nablation studies and analyses, we pinpoint how to most effectively build\nresidual connections.",
        "score": 2.439478874206543
      },
      {
        "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model\n  Reasoning,",
        "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities\nin complex tasks. However, they lack up-to-date knowledge and experience\nhallucinations during reasoning, which can lead to incorrect reasoning\nprocesses and diminish their performance and trustworthiness. Knowledge graphs\n(KGs), which capture vast amounts of facts in a structured format, offer a\nreliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM\nreasoning methods only treat KGs as factual knowledge bases and overlook the\nimportance of their structural information for reasoning. In this paper, we\npropose a novel method called reasoning on graphs (RoG) that synergizes LLMs\nwith KGs to enable faithful and interpretable reasoning. Specifically, we\npresent a planning-retrieval-reasoning framework, where RoG first generates\nrelation paths grounded by KGs as faithful plans. These plans are then used to\nretrieve valid reasoning paths from the KGs for LLMs to conduct faithful\nreasoning. Furthermore, RoG not only distills knowledge from KGs to improve the\nreasoning ability of LLMs through training but also allows seamless integration\nwith any arbitrary LLMs during inference. Extensive experiments on two\nbenchmark KGQA datasets demonstrate that RoG achieves state-of-the-art\nperformance on KG reasoning tasks and generates faithful and interpretable\nreasoning results.",
        "score": 2.2552919387817383
      },
      {
        "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,",
        "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex\nreasoning tasks, which aims at designing a simple prompt like ``Let's think\nstep by step'' or multiple in-context exemplars with well-designed rationales\nto elicit Large Language Models (LLMs) to generate intermediate reasoning\nsteps. However, the generated rationales often come with mistakes, making\nunfactual and unfaithful reasoning chains. To mitigate this brittleness, we\npropose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting\nLLMs to generate explicit pieces of knowledge evidence in the form of structure\ntriple. This is inspired by our human behaviors, i.e., we can draw a mind map\nor knowledge map as the reasoning evidence in the brain before answering a\ncomplex question. Benefiting from CoK, we additionally introduce a\nF^2-Verification method to estimate the reliability of the reasoning chains in\nterms of factuality and faithfulness. For the unreliable response, the wrong\nevidence can be indicated to prompt the LLM to rethink. Extensive experiments\ndemonstrate that our method can further improve the performance of commonsense,\nfactual, symbolic, and arithmetic reasoning tasks.",
        "score": 1.8652851581573486
      },
      {
        "title": "DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation,",
        "abstract": "The method of training language models based on domain datasets has obtained\nsignificant achievements in the task of generating scientific paper abstracts.\nHowever, such models face problems of generalization and expensive training\ncosts. The use of large language models (LLMs) to solve the task of generating\npaper abstracts saves the cost of model training. However, due to the\nhallucination problem of LLM, it is often necessary to improve the reliability\nof the results through multi-round query prompt approach such as Graph of\nThoughts (GoT), which also brings additional reasoning costs. In this paper, we\npropose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages\nof the existing GoT prompt approach, but also dynamically adjust the graph\nstructure according to data characteristics while reducing model reasoning\ncost. Experimental results show that our method's cost-effectiveness in\nabstract generation tasks is only 43.7% to 56.4% of other multi-round query\nprompt approaches. Our code is available at https://github.com/JayceNing/DGoT.",
        "score": 1.5663206577301025
      },
      {
        "title": "Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with\n  Knowledge Graphs,",
        "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities when\nprompted to generate chain-of-thought (CoT) explanations alongside answers.\nHowever, previous research on evaluating LLMs has solely focused on answer\naccuracy, neglecting the correctness of the generated CoT. In this paper, we\ndelve deeper into the CoT reasoning capabilities of LLMs in multi-hop question\nanswering by utilizing knowledge graphs (KGs). We propose a novel\ndiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledge\nof reasoning and the accuracy of the generated CoT. Through experiments\nconducted on 5 different families of LLMs across 2 multi-hop question-answering\ndatasets, we find that LLMs possess sufficient knowledge to perform reasoning.\nHowever, there exists a significant disparity between answer accuracy and\nfaithfulness of the CoT reasoning generated by LLMs, indicating that they often\narrive at correct answers through incorrect reasoning.",
        "score": 1.4318662881851196
      },
      {
        "title": "On Exploring the Reasoning Capability of Large Language Models with\n  Knowledge Graphs,",
        "abstract": "This paper examines the capacity of LLMs to reason with knowledge graphs\nusing their internal knowledge graph, i.e., the knowledge graph they learned\nduring pre-training. Two research questions are formulated to investigate the\naccuracy of LLMs in recalling information from pre-training knowledge graphs\nand their ability to infer knowledge graph relations from context. To address\nthese questions, we employ LLMs to perform four distinct knowledge graph\nreasoning tasks. Furthermore, we identify two types of hallucinations that may\noccur during knowledge reasoning with LLMs: content and ontology hallucination.\nOur experimental results demonstrate that LLMs can successfully tackle both\nsimple and complex knowledge graph reasoning tasks from their own memory, as\nwell as infer from input context.",
        "score": 1.3493003845214844
      },
      {
        "title": "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\n  Language Models,",
        "abstract": "With the widespread use of language models (LMs) in NLP tasks, researchers\nhave discovered the potential of Chain-of-thought (CoT) to assist LMs in\naccomplishing complex reasoning tasks by generating intermediate steps.\nHowever, human thought processes are often non-linear, rather than simply\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\nreasoning, which models human thought processes not only as a chain but also as\na graph. By representing thought units as nodes and connections between them as\nedges, our approach captures the non-sequential nature of human thinking and\nallows for a more realistic modeling of thought processes. GoT adopts a\ntwo-stage framework with an additional GoT encoder for thought graph\nrepresentation and fuses the graph representation with the original input\nrepresentation through a gated fusion mechanism. We evaluate GoT's performance\non a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task\n(ScienceQA). Our model achieves significant improvement over the strong CoT\nbaseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59%\nusing the T5-base model over the state-of-the-art Multimodal-CoT on the\nScienceQA test set.",
        "score": 1.2687374353408813
      },
      {
        "title": "Chain-of-Thought Tuning: Masked Language Models can also Think Step By\n  Step in Natural Language Understanding,",
        "abstract": "Chain-of-Thought (CoT) is a technique that guides Large Language Models\n(LLMs) to decompose complex tasks into multi-step reasoning through\nintermediate steps in natural language form. Briefly, CoT enables LLMs to think\nstep by step. However, although many Natural Language Understanding (NLU) tasks\nalso require thinking step by step, LLMs perform less well than small-scale\nMasked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose\nChain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt\ntuning, to implement step-by-step thinking for MLMs on NLU tasks. From the\nperspective of CoT, CoTT's two-step framework enables MLMs to implement task\ndecomposition; CoTT's prompt tuning allows intermediate steps to be used in\nnatural language form. Thereby, the success of CoT can be extended to NLU tasks\nthrough MLMs. To verify the effectiveness of CoTT, we conduct experiments on\ntwo NLU tasks: hierarchical classification and relation extraction, and the\nresults show that CoTT outperforms baselines and achieves state-of-the-art\nperformance.",
        "score": 0.7881549000740051
      },
      {
        "title": "Distributional reasoning in LLMs: Parallel reasoning processes in\n  multi-hop reasoning,",
        "abstract": "Large language models (LLMs) have shown an impressive ability to perform\ntasks believed to require thought processes. When the model does not document\nan explicit thought process, it becomes difficult to understand the processes\noccurring within its hidden layers and to determine if these processes can be\nreferred to as reasoning. We introduce a novel and interpretable analysis of\ninternal multi-hop reasoning processes in LLMs. We demonstrate that the\nprediction process for compositional reasoning questions can be modeled using a\nsimple linear transformation between two semantic category spaces. We show that\nduring inference, the middle layers of the network generate highly\ninterpretable embeddings that represent a set of potential intermediate answers\nfor the multi-hop question. We use statistical analyses to show that a\ncorresponding subset of tokens is activated in the model's output, implying the\nexistence of parallel reasoning paths. These observations hold true even when\nthe model lacks the necessary knowledge to solve the task. Our findings can\nhelp uncover the strategies that LLMs use to solve reasoning tasks, offering\ninsights into the types of thought processes that can emerge from artificial\nintelligence. Finally, we also discuss the implication of cognitive modeling of\nthese results.",
        "score": 0.4778161942958832
      },
      {
        "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought,",
        "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world\nknowledge, they have been shown incapable of recalling these knowledge to solve\ntasks requiring complex & multi-step reasoning. Similar to how humans develop a\n\"chain of thought\" for these tasks, how can we equip PLMs with such abilities?\nIn this work, we explore an iterative prompting framework, a new prompting\nparadigm which progressively elicits relevant knowledge from PLMs for\nmulti-step inference. We identify key limitations of existing prompting\nmethods, namely they are either restricted to queries with a single\nidentifiable relation/predicate, or being agnostic to input contexts, which\nmakes it difficult to capture variabilities across different inference steps.\nWe propose an iterative context-aware prompter, which addresses these\nlimitations by learning to dynamically synthesize prompts conditioned on the\ncurrent step's contexts. Experiments on three datasets involving multi-step\nreasoning show the effectiveness of the iterative scheme and the context-aware\nprompter design.",
        "score": -0.14844465255737305
      }
    ]
  },
  {
    "title": "Are Emergent Abilities in Large Language Models just In-Context\n  Learning?",
    "abstract": "  Large language models, comprising billions of parameters and pre-trained on\nextensive web-scale corpora, have been claimed to acquire certain capabilities\nwithout having been specifically trained on them. These capabilities, referred\nto as \"emergent abilities,\" have been a driving force in discussions regarding\nthe potentials and risks of language models. A key challenge in evaluating\nemergent abilities is that they are confounded by model competencies that arise\nthrough alternative prompting techniques, including in-context learning, which\nis the ability of models to complete a task based on a few examples. We present\na novel theory that explains emergent abilities, taking into account their\npotential confounding factors, and rigorously substantiate this theory through\nover 1000 experiments. Our findings suggest that purported emergent abilities\nare not truly emergent, but result from a combination of in-context learning,\nmodel memory, and linguistic knowledge. Our work is a foundational step in\nexplaining language model performance, providing a template for their efficient\nuse and clarifying the paradox of their ability to excel in some instances\nwhile faltering in others. Thus, we demonstrate that their capabilities should\nnot be overestimated.\n",
    "related_paper_titles": [],
    "related_paper_abstract": [],
    "entities": [
      "ChatGPT",
      "LMMs",
      "RAG",
      "VLMs",
      "LoRA",
      "VQA",
      "CoT",
      "Gemini",
      "Large Language Model",
      "GPT",
      "Language Model",
      "MLLMs",
      "ToM",
      "KGs",
      "API",
      "Language",
      "Further",
      "IFT",
      "PEFT",
      "Code",
      "GNNs",
      "Natural Language",
      "Mistral",
      "Dynamic",
      "LMM",
      "MoE",
      "ML",
      "LLMs",
      "MLP",
      "KG"
    ],
    "retrieved_papers": [
      {
        "title": "Are Emergent Abilities in Large Language Models just In-Context\n  Learning?,",
        "abstract": "Large language models, comprising billions of parameters and pre-trained on\nextensive web-scale corpora, have been claimed to acquire certain capabilities\nwithout having been specifically trained on them. These capabilities, referred\nto as \"emergent abilities,\" have been a driving force in discussions regarding\nthe potentials and risks of language models. A key challenge in evaluating\nemergent abilities is that they are confounded by model competencies that arise\nthrough alternative prompting techniques, including in-context learning, which\nis the ability of models to complete a task based on a few examples. We present\na novel theory that explains emergent abilities, taking into account their\npotential confounding factors, and rigorously substantiate this theory through\nover 1000 experiments. Our findings suggest that purported emergent abilities\nare not truly emergent, but result from a combination of in-context learning,\nmodel memory, and linguistic knowledge. Our work is a foundational step in\nexplaining language model performance, providing a template for their efficient\nuse and clarifying the paradox of their ability to excel in some instances\nwhile faltering in others. Thus, we demonstrate that their capabilities should\nnot be overestimated.",
        "score": 6.478011131286621
      },
      {
        "title": "Emergent Abilities in Reduced-Scale Generative Language Models,",
        "abstract": "Large language models can solve new tasks without task-specific fine-tuning.\nThis ability, also known as in-context learning (ICL), is considered an\nemergent ability and is primarily seen in large language models with billions\nof parameters. This study investigates if such emergent properties are strictly\ntied to model size or can be demonstrated by smaller models trained on\nreduced-scale data. To explore this, we simplify pre-training data and\npre-train 36 causal language models with parameters varying from 1 million to\n165 million parameters. We show that models trained on this simplified\npre-training data demonstrate enhanced zero-shot capabilities across various\ntasks in simplified language, achieving performance comparable to that of\npre-trained models six times larger on unrestricted language. This suggests\nthat downscaling the language allows zero-shot learning capabilities to emerge\nin models with limited size. Additionally, we find that these smaller models\npre-trained on simplified data demonstrate a power law relationship between the\nevaluation loss and the three scaling factors: compute, dataset size, and model\nsize.",
        "score": 4.887363433837891
      },
      {
        "title": "Emergent Abilities of Large Language Models,",
        "abstract": "Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.",
        "score": 4.286789894104004
      },
      {
        "title": "Understanding Emergent Abilities of Language Models from the Loss\n  Perspective,",
        "abstract": "Recent studies have put into question the belief that emergent abilities in\nlanguage models are exclusive to large models. This skepticism arises from two\nobservations: 1) smaller models can also exhibit high performance on emergent\nabilities and 2) there is doubt on the discontinuous metrics used to measure\nthese abilities. In this paper, we propose to study emergent abilities in the\nlens of pre-training loss, instead of model size or training compute. We\ndemonstrate that the models with the same pre-training loss, but different\nmodel and data sizes, generate the same performance on various downstream\ntasks. We also discover that a model exhibits emergent abilities on certain\ntasks -- regardless of the continuity of metrics -- when its pre-training loss\nfalls below a specific threshold. Before reaching this threshold, its\nperformance remains at the level of random guessing. This inspires us to\nredefine emergent abilities as those that manifest in models with lower\npre-training losses, highlighting that these abilities cannot be predicted by\nmerely extrapolating the performance trends of models with higher pre-training\nlosses.",
        "score": 4.094391345977783
      },
      {
        "title": "Training on the Test Task Confounds Evaluation and Emergence,",
        "abstract": "We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.",
        "score": 3.9197123050689697
      },
      {
        "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction,",
        "abstract": "Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.",
        "score": 3.883864402770996
      },
      {
        "title": "ICLEval: Evaluating In-Context Learning Ability of Large Language Models,",
        "abstract": "In-Context Learning (ICL) is a critical capability of Large Language Models\n(LLMs) as it empowers them to comprehend and reason across interconnected\ninputs. Evaluating the ICL ability of LLMs can enhance their utilization and\ndeepen our understanding of how this ability is acquired at the training stage.\nHowever, existing evaluation frameworks primarily focus on language abilities\nand knowledge, often overlooking the assessment of ICL ability. In this work,\nwe introduce the ICLEval benchmark to evaluate the ICL abilities of LLMs, which\nencompasses two key sub-abilities: exact copying and rule learning. Through the\nICLEval benchmark, we demonstrate that ICL ability is universally present in\ndifferent LLMs, and model size is not the sole determinant of ICL efficacy.\nSurprisingly, we observe that ICL abilities, particularly copying, develop\nearly in the pretraining process and stabilize afterward. Our source codes and\nbenchmark are released at https://github.com/yiye3/ICLEval.",
        "score": 3.871523380279541
      },
      {
        "title": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates\n  before In-Context Learning,",
        "abstract": "How does scaling the number of parameters in large language models (LLMs)\naffect their core capabilities? We study two natural scaling techniques --\nweight pruning and simply training a smaller or larger model, which we refer to\nas dense scaling -- and their effects on two core capabilities of LLMs: (a)\nrecalling facts presented during pre-training and (b) processing information\npresented in-context during inference. By curating a suite of tasks that help\ndisentangle these two capabilities, we find a striking difference in how these\ntwo abilities evolve due to scaling. Reducing the model size by more than 30\\%\n(via either scaling approach) significantly decreases the ability to recall\nfacts seen in pre-training. Yet, a 60--70\\% reduction largely preserves the\nvarious ways the model can process in-context information, ranging from\nretrieving answers from a long context to learning parameterized functions from\nin-context exemplars. The fact that both dense scaling and weight pruning\nexhibit this behavior suggests that scaling model size has an inherently\ndisparate effect on fact recall and in-context learning.",
        "score": 3.6006228923797607
      },
      {
        "title": "Deconstructing In-Context Learning: Understanding Prompts via Corruption,",
        "abstract": "The ability of large language models (LLMs) to $``$learn in context$\"$ based\non the provided prompt has led to an explosive growth in their use, culminating\nin the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These\nAI assistants are known to be robust to minor prompt modifications, mostly due\nto alignment techniques that use human feedback. In contrast, the underlying\npre-trained LLMs they use as a backbone are known to be brittle in this\nrespect. Building high-quality backbone models remains a core challenge, and a\ncommon approach to assessing their quality is to conduct few-shot evaluation.\nSuch evaluation is notorious for being highly sensitive to minor prompt\nmodifications, as well as the choice of specific in-context examples. Prior\nwork has examined how modifying different elements of the prompt can affect\nmodel performance. However, these earlier studies tended to concentrate on a\nlimited number of specific prompt attributes and often produced contradictory\nresults. Additionally, previous research either focused on models with fewer\nthan 15 billion parameters or exclusively examined black-box models like GPT-3\nor PaLM, making replication challenging. In the present study, we decompose the\nentire prompt into four components: task description, demonstration inputs,\nlabels, and inline instructions provided for each demonstration. We investigate\nthe effects of structural and semantic corruptions of these elements on model\nperformance. We study models ranging from 1.5B to 70B in size, using ten\ndatasets covering classification and generation tasks. We find that repeating\ntext within the prompt boosts model performance, and bigger models ($\\geq$30B)\nare more sensitive to the semantics of the prompt. Finally, we observe that\nadding task and inline instructions to the demonstrations enhances model\nperformance even when the instructions are semantically corrupted.",
        "score": 3.5959761142730713
      },
      {
        "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large\n  Language Models,",
        "abstract": "We investigate how to elicit compositional generalization capabilities in\nlarge language models (LLMs). Compositional generalization empowers LLMs to\nsolve complex problems by combining foundational skills, a critical reasoning\nability akin to human intelligence. However, even the most advanced LLMs\ncurrently struggle with this form of reasoning. We examine this problem within\nthe framework of in-context learning and find that demonstrating both\nfoundational skills and compositional examples grounded in these skills within\nthe same prompt context is crucial. We refer to this prompt structure as\nskills-in-context (SKiC). With as few as two exemplars, this in-context\nlearning structure enables LLMs to tackle more challenging problems requiring\ninnovative skill combinations, achieving near-perfect systematic generalization\nacross a broad range of tasks. Intriguingly, SKiC also unlocks the latent\npotential of LLMs, allowing them to more actively utilize pre-existing internal\nskills acquired during earlier pretraining stages to solve complex reasoning\nproblems. The SKiC structure is robust across different skill constructions and\nexemplar choices and demonstrates strong transferability to new tasks. Finally,\ninspired by our in-context learning study, we show that fine-tuning LLMs with\nSKiC-style data can elicit zero-shot weak-to-strong generalization, enabling\nthe models to solve much harder problems directly with standard prompting.",
        "score": 3.4686129093170166
      },
      {
        "title": "A Latent Space Theory for Emergent Abilities in Large Language Models,",
        "abstract": "Languages are not created randomly but rather to communicate information.\nThere is a strong association between languages and their underlying meanings,\nresulting in a sparse joint distribution that is heavily peaked according to\ntheir correlations. Moreover, these peak values happen to match with the\nmarginal distribution of languages due to the sparsity. With the advent of LLMs\ntrained on big data and large models, we can now precisely assess the marginal\ndistribution of languages, providing a convenient means of exploring the sparse\nstructures in the joint distribution for effective inferences. In this paper,\nwe categorize languages as either unambiguous or {\\epsilon}-ambiguous and\npresent quantitative results to demonstrate that the emergent abilities of\nLLMs, such as language understanding, in-context learning, chain-of-thought\nprompting, and effective instruction fine-tuning, can all be attributed to\nBayesian inference on the sparse joint distribution of languages.",
        "score": 3.344682455062866
      },
      {
        "title": "Do Large Language Models Have Compositional Ability? An Investigation\n  into Limitations and Scalability,",
        "abstract": "Large language models (LLMs) have emerged as powerful tools for many AI\nproblems and exhibit remarkable in-context learning (ICL) capabilities.\nCompositional ability, solving unseen complex tasks that combine two or more\nsimple tasks, is an essential reasoning ability for Artificial General\nIntelligence. Despite LLM's tremendous success, how they approach composite\ntasks, especially those not encountered during the pretraining phase, remains\nan open question and largely ununderstood. In this study, we delve into the ICL\ncapabilities of LLMs on composite tasks, with only simple tasks as in-context\nexamples. We develop a test suite of composite tasks that include linguistic\nand logical challenges and perform empirical studies across different LLM\nfamilies. We observe that models exhibit divergent behaviors: (1) For simpler\ncomposite tasks that apply distinct mapping mechanisms to different input\nsegments, the models demonstrate decent compositional ability, while scaling up\nthe model enhances this ability; (2) for more complex composite tasks that\ninvolving reasoning multiple steps, where each step represent one task, models\ntypically underperform, and scaling up generally provide no improvements. We\noffer theoretical analysis in a simplified setting, explaining that models\nexhibit compositional capability when the task handles different input parts\nseparately. We believe our work sheds new light on the capabilities of LLMs in\nsolving composite tasks regarding the nature of the tasks and model scale. Our\ndataset and code are available at\n{\\url{https://github.com/OliverXUZY/LLM_Compose}}.",
        "score": 2.915736198425293
      },
      {
        "title": "In-context Learning Generalizes, But Not Always Robustly: The Case of\n  Syntax,",
        "abstract": "In-context learning (ICL) is now a common method for teaching large language\nmodels (LLMs) new tasks: given labeled examples in the input context, the LLM\nlearns to perform the task without weight updates. Do models guided via ICL\ninfer the underlying structure of the task defined by the context, or do they\nrely on superficial heuristics that only generalize to identically distributed\nexamples? We address this question using transformations tasks and an NLI task\nthat assess sensitivity to syntax - a requirement for robust language\nunderstanding. We further investigate whether out-of-distribution\ngeneralization can be improved via chain-of-thought prompting, where the model\nis provided with a sequence of intermediate computation steps that illustrate\nhow the task ought to be performed. In experiments with models from the GPT,\nPaLM, and Llama 2 families, we find large variance across LMs. The variance is\nexplained more by the composition of the pre-training corpus and supervision\nmethods than by model size; in particular, models pre-trained on code\ngeneralize better, and benefit more from chain-of-thought prompting.",
        "score": 2.7623395919799805
      },
      {
        "title": "On the Unexpected Abilities of Large Language Models,",
        "abstract": "Large Language Models (LLMs) are capable of displaying a wide range of\nabilities that are not directly connected with the task for which they are\ntrained: predicting the next words of human-written texts. In this article, I\nreview recent research investigating the cognitive abilities developed by LLMs\nand their relation to human cognition. I discuss the nature of the indirect\nprocess that leads to the acquisition of these cognitive abilities, their\nrelation to other indirect processes, and the implications for the acquisition\nof integrated abilities. Moreover, I propose the factors that enable the\ndevelopment of abilities that are related only very indirectly to the proximal\nobjective of the training task. Finally, I discuss whether the full set of\ncapabilities that LLMs could possibly develop is predictable.",
        "score": 2.6842668056488037
      },
      {
        "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models,",
        "abstract": "While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.",
        "score": 2.597881555557251
      },
      {
        "title": "Machine Psychology: Investigating Emergent Capabilities and Behavior in\n  Large Language Models Using Psychological Methods,",
        "abstract": "Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Due to rapid\ntechnological advances and their extreme versatility, LLMs nowadays have\nmillions of users and are at the cusp of being the main go-to technology for\ninformation retrieval, content generation, problem-solving, etc. Therefore, it\nis of great importance to thoroughly assess and scrutinize their capabilities.\nDue to increasingly complex and novel behavioral patterns in current LLMs, this\ncan be done by treating them as participants in psychology experiments that\nwere originally designed to test humans. For this purpose, the paper introduces\na new field of research called \"machine psychology\". The paper outlines how\ndifferent subfields of psychology can inform behavioral tests for LLMs. It\ndefines methodological standards for machine psychology research, especially by\nfocusing on policies for prompt designs. Additionally, it describes how\nbehavioral patterns discovered in LLMs are to be interpreted. In sum, machine\npsychology aims to discover emergent abilities in LLMs that cannot be detected\nby most traditional natural language processing benchmarks.",
        "score": 2.5760653018951416
      },
      {
        "title": "FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition,",
        "abstract": "Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.",
        "score": 2.340564727783203
      },
      {
        "title": "Revealing the structure of language model capabilities,",
        "abstract": "Building a theoretical understanding of the capabilities of large language\nmodels (LLMs) is vital for our ability to predict and explain the behavior of\nthese systems. Here, we investigate the structure of LLM capabilities by\nextracting latent capabilities from patterns of individual differences across a\nvaried population of LLMs. Using a combination of Bayesian and frequentist\nfactor analysis, we analyzed data from 29 different LLMs across 27 cognitive\ntasks. We found evidence that LLM capabilities are not monolithic. Instead,\nthey are better explained by three well-delineated factors that represent\nreasoning, comprehension and core language modeling. Moreover, we found that\nthese three factors can explain a high proportion of the variance in model\nperformance. These results reveal a consistent structure in the capabilities of\ndifferent LLMs and demonstrate the multifaceted nature of these capabilities.\nWe also found that the three abilities show different relationships to model\nproperties such as model size and instruction tuning. These patterns help\nrefine our understanding of scaling laws and indicate that changes to a model\nthat improve one ability might simultaneously impair others. Based on these\nfindings, we suggest that benchmarks could be streamlined by focusing on tasks\nthat tap into each broad model ability.",
        "score": 2.3039679527282715
      },
      {
        "title": "What Do Language Models Learn in Context? The Structured Task Hypothesis,",
        "abstract": "Large language models (LLMs) exhibit an intriguing ability to learn a novel\ntask from in-context examples presented in a demonstration, termed in-context\nlearning (ICL). Understandably, a swath of research has been dedicated to\nuncovering the theories underpinning ICL. One popular hypothesis explains ICL\nby task selection. LLMs identify the task based on the demonstration and\ngeneralize it to the prompt. Another popular hypothesis is that ICL is a form\nof meta-learning, i.e., the models learn a learning algorithm at pre-training\ntime and apply it to the demonstration. Finally, a third hypothesis argues that\nLLMs use the demonstration to select a composition of tasks learned during\npre-training to perform ICL. In this paper, we empirically explore these three\nhypotheses that explain LLMs' ability to learn in context with a suite of\nexperiments derived from common text classification tasks. We invalidate the\nfirst two hypotheses with counterexamples and provide evidence in support of\nthe last hypothesis. Our results suggest an LLM could learn a novel task in\ncontext via composing tasks learned during pre-training.",
        "score": 2.282301425933838
      },
      {
        "title": "Large Language Models: The Need for Nuance in Current Debates and a\n  Pragmatic Perspective on Understanding,",
        "abstract": "Current Large Language Models (LLMs) are unparalleled in their ability to\ngenerate grammatically correct, fluent text. LLMs are appearing rapidly, and\ndebates on LLM capacities have taken off, but reflection is lagging behind.\nThus, in this position paper, we first zoom in on the debate and critically\nassess three points recurring in critiques of LLM capacities: i) that LLMs only\nparrot statistical patterns in the training data; ii) that LLMs master formal\nbut not functional language competence; and iii) that language learning in LLMs\ncannot inform human language learning. Drawing on empirical and theoretical\narguments, we show that these points need more nuance. Second, we outline a\npragmatic perspective on the issue of `real' understanding and intentionality\nin LLMs. Understanding and intentionality pertain to unobservable mental states\nwe attribute to other humans because they have pragmatic value: they allow us\nto abstract away from complex underlying mechanics and predict behaviour\neffectively. We reflect on the circumstances under which it would make sense\nfor humans to similarly attribute mental states to LLMs, thereby outlining a\npragmatic philosophical context for LLMs as an increasingly prominent\ntechnology in society.",
        "score": 2.0338234901428223
      },
      {
        "title": "Emergent communication and learning pressures in language models: a\n  language evolution perspective,",
        "abstract": "Language models and humans are two types of learning systems. Finding or\nfacilitating commonalities could enable major breakthroughs in our\nunderstanding of the acquisition and evolution of language. Many theories of\nlanguage evolution rely heavily on learning biases and learning pressures. Yet\ndue to substantial differences in learning pressures, it is questionable\nwhether the similarity between humans and machines is sufficient for insights\nto carry over and to be worth testing with human participants. Here, we review\nthe emergent communication literature, a subfield of multi-agent reinforcement\nlearning, from a language evolution perspective. We find that the emergent\ncommunication literature excels at designing and adapting models to recover\ninitially absent linguistic phenomena of natural languages. Based on a short\nliterature review, we identify key pressures that have recovered initially\nabsent human patterns in emergent communication models: communicative success,\nefficiency, learnability, and other psycho-/sociolinguistic factors. We argue\nthat this may serve as inspiration for how to design language models for\nlanguage acquisition and language evolution research.",
        "score": 1.7478094100952148
      },
      {
        "title": "Do pretrained Transformers Learn In-Context by Gradient Descent?,",
        "abstract": "The emergence of In-Context Learning (ICL) in LLMs remains a remarkable\nphenomenon that is partially understood. To explain ICL, recent studies have\ncreated theoretical connections to Gradient Descent (GD). We ask, do such\nconnections hold up in actual pre-trained language models? We highlight the\nlimiting assumptions in prior works that make their setup considerably\ndifferent from the practical setup in which language models are trained. For\nexample, their experimental verification uses \\emph{ICL objective} (training\nmodels explicitly for ICL), which differs from the emergent ICL in the wild.\nFurthermore, the theoretical hand-constructed weights used in these studies\nhave properties that don't match those of real LLMs. We also look for evidence\nin real models. We observe that ICL and GD have different sensitivity to the\norder in which they observe demonstrations. Finally, we probe and compare the\nICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical\nanalyses on language models pre-trained on natural data (LLaMa-7B). Our\ncomparisons of three performance metrics highlight the inconsistent behavior of\nICL and GD as a function of various factors such as datasets, models, and the\nnumber of demonstrations. We observe that ICL and GD modify the output\ndistribution of language models differently. These results indicate that\n\\emph{the equivalence between ICL and GD remains an open hypothesis} and calls\nfor further studies.",
        "score": 1.5832432508468628
      },
      {
        "title": "The Confidence-Competence Gap in Large Language Models: A Cognitive\n  Study,",
        "abstract": "Large Language Models (LLMs) have acquired ubiquitous attention for their\nperformances across diverse domains. Our study here searches through LLMs'\ncognitive abilities and confidence dynamics. We dive deep into understanding\nthe alignment between their self-assessed confidence and actual performance. We\nexploit these models with diverse sets of questionnaires and real-world\nscenarios and extract how LLMs exhibit confidence in their responses. Our\nfindings reveal intriguing instances where models demonstrate high confidence\neven when they answer incorrectly. This is reminiscent of the Dunning-Kruger\neffect observed in human psychology. In contrast, there are cases where models\nexhibit low confidence with correct answers revealing potential underestimation\nbiases. Our results underscore the need for a deeper understanding of their\ncognitive processes. By examining the nuances of LLMs' self-assessment\nmechanism, this investigation provides noteworthy revelations that serve to\nadvance the functionalities and broaden the potential applications of these\nformidable language models.",
        "score": 1.568016767501831
      },
      {
        "title": "Too Much Information: Keeping Training Simple for BabyLMs,",
        "abstract": "This paper details the work of the University of Groningen for the BabyLM\nChallenge. We follow the idea that, like babies, language models should be\nintroduced to simpler concepts first and build off of that knowledge to\nunderstand more complex concepts. We examine this strategy of\nsimple-then-complex through a variety of lenses, namely context size,\nvocabulary, and overall linguistic complexity of the data. We find that only\none, context size, is truly beneficial to training a language model. However\nthis simple change to context size gives us improvements of 2 points on average\non (Super)GLUE tasks, 1 point on MSGS tasks, and 12\\% on average on BLiMP\ntasks. Our context-limited model outperforms the baseline that was trained on\n10$\\times$ the amount of data.",
        "score": 1.5277165174484253
      },
      {
        "title": "I Learn Better If You Speak My Language: Understanding the Superior\n  Performance of Fine-Tuning Large Language Models with LLM-Generated Responses,",
        "abstract": "This paper explores an intriguing observation: fine-tuning a large language\nmodel (LLM) with responses generated by a LLM often yields better results than\nusing responses generated by humans. We conduct an in-depth investigation to\nunderstand why this occurs. Contrary to the common belief that these instances\nis simply due to the more detailed nature of LLM-generated content, our study\nidentifies another contributing factor: an LLM is inherently more \"familiar\"\nwith LLM generated responses. This familiarity is evidenced by lower perplexity\nbefore fine-tuning. We design a series of experiments to understand the impact\nof the \"familiarity\" and our conclusion reveals that this \"familiarity\"\nsignificantly impacts learning performance. Training with LLM-generated\nresponses not only enhances performance but also helps maintain the model's\ncapabilities in other tasks after fine-tuning on a specific task.",
        "score": 1.4793202877044678
      },
      {
        "title": "Language acquisition: do children and language models follow similar\n  learning stages?,",
        "abstract": "During language acquisition, children follow a typical sequence of learning\nstages, whereby they first learn to categorize phonemes before they develop\ntheir lexicon and eventually master increasingly complex syntactic structures.\nHowever, the computational principles that lead to this learning trajectory\nremain largely unknown. To investigate this, we here compare the learning\ntrajectories of deep language models to those of children. Specifically, we\ntest whether, during its training, GPT-2 exhibits stages of language\nacquisition comparable to those observed in children aged between 18 months and\n6 years. For this, we train 48 GPT-2 models from scratch and evaluate their\nsyntactic and semantic abilities at each training step, using 96 probes curated\nfrom the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these\nevaluations with the behavior of 54 children during language production. Our\nanalyses reveal three main findings. First, similarly to children, the language\nmodels tend to learn linguistic skills in a systematic order. Second, this\nlearning scheme is parallel: the language tasks that are learned last improve\nfrom the very first training steps. Third, some - but not all - learning stages\nare shared between children and these language models. Overall, these results\nshed new light on the principles of language acquisition, and highlight\nimportant divergences in how humans and modern algorithms learn to process\nnatural language.",
        "score": 1.468093991279602
      },
      {
        "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and\n  Simplicity Bias in MLMs,",
        "abstract": "Most interpretability research in NLP focuses on understanding the behavior\nand features of a fully trained model. However, certain insights into model\nbehavior may only be accessible by observing the trajectory of the training\nprocess. We present a case study of syntax acquisition in masked language\nmodels (MLMs) that demonstrates how analyzing the evolution of interpretable\nartifacts throughout training deepens our understanding of emergent behavior.\nIn particular, we study Syntactic Attention Structure (SAS), a naturally\nemerging property of MLMs wherein specific Transformer heads tend to focus on\nspecific syntactic relations. We identify a brief window in pretraining when\nmodels abruptly acquire SAS, concurrent with a steep drop in loss. This\nbreakthrough precipitates the subsequent acquisition of linguistic\ncapabilities. We then examine the causal role of SAS by manipulating SAS during\ntraining, and demonstrate that SAS is necessary for the development of\ngrammatical capabilities. We further find that SAS competes with other\nbeneficial traits during training, and that briefly suppressing SAS improves\nmodel quality. These findings offer an interpretation of a real-world example\nof both simplicity bias and breakthrough training dynamics.",
        "score": 1.016400933265686
      },
      {
        "title": "A Theory for Emergence of Complex Skills in Language Models,",
        "abstract": "A major driver of AI products today is the fact that new skills emerge in\nlanguage models when their parameter set and training corpora are scaled up.\nThis phenomenon is poorly understood, and a mechanistic explanation via\nmathematical analysis of gradient-based training seems difficult. The current\npaper takes a different approach, analysing emergence using the famous (and\nempirical) Scaling Laws of LLMs and a simple statistical framework.\nContributions include: (a) A statistical framework that relates cross-entropy\nloss of LLMs to competence on the basic skills that underlie language tasks.\n(b) Mathematical analysis showing that the Scaling Laws imply a strong form of\ninductive bias that allows the pre-trained model to learn very efficiently. We\ninformally call this {\\em slingshot generalization} since naively viewed it\nappears to give competence levels at skills that violate usual generalization\ntheory. (c) A key example of slingshot generalization, that competence at\nexecuting tasks involving $k$-tuples of skills emerges essentially at the same\nscaling and same rate as competence on the elementary skills themselves.",
        "score": 0.9999526143074036
      },
      {
        "title": "Mind the instructions: a holistic evaluation of consistency and\n  interactions in prompt-based learning,",
        "abstract": "Finding the best way of adapting pre-trained language models to a task is a\nbig challenge in current NLP. Just like the previous generation of task-tuned\nmodels (TT), models that are adapted to tasks via in-context-learning (ICL) are\nrobust in some setups but not in others. Here, we present a detailed analysis\nof which design choices cause instabilities and inconsistencies in LLM\npredictions. First, we show how spurious correlations between input\ndistributions and labels -- a known issue in TT models -- form only a minor\nproblem for prompted models. Then, we engage in a systematic, holistic\nevaluation of different factors that have been found to influence predictions\nin a prompting setup. We test all possible combinations of a range of factors\non both vanilla and instruction-tuned (IT) LLMs of different scale and\nstatistically analyse the results to show which factors are the most\ninfluential, interactive or stable. Our results show which factors can be used\nwithout precautions and which should be avoided or handled with care in most\nsettings.",
        "score": 0.986051619052887
      },
      {
        "title": "Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for\n  Instruction Generation Models,",
        "abstract": "Recent work studies the cognitive capabilities of language models through\npsychological tests designed for humans. While these studies are helpful for\nunderstanding the general capabilities of these models, there is no guarantee\nthat a model possessing sufficient capabilities to pass those tests would\nactually use those capabilities in performing real-life tasks. In this work, we\nformulate task-oriented cognitive capabilities, which are human-like cognitive\ncapabilities that language models leverage to perform tasks. These capabilities\nare (i) the ability to quickly generate good candidate utterances (the search\ncapability) (ii) the ability to predict how a listener interprets those\nutterances and choose the most appropriate one (the pragmatic capability). We\ndesign an evaluation scheme for comparing these capabilities of a language\nmodel with those of a human. Applying this scheme to examine various models in\na navigation instruction generation problem, we find that their pragmatic\ncapability is severely lacking. This insight leads us to augment them with\nbetter models of the listener and obtain a significant boost of 11% in success\nrate in guiding real humans. Our work advocates for having a principled\nprocedure for aligning language models with humans that involves (i)\nformulating task-oriented capabilities, (ii) devising a method to quantify\ntheir deficiency, and (iii) iteratively improving them.",
        "score": 0.9763264656066895
      }
    ]
  },
  {
    "title": "Enhancing Large Language Models in Coding Through Multi-Perspective\n  Self-Consistency",
    "abstract": "  Large language models (LLMs) have exhibited remarkable ability in code\ngeneration. However, generating the correct solution in a single attempt still\nremains a challenge. Prior works utilize verification properties in software\nengineering to verify and re-rank solutions in a majority voting manner. But\nthe assumption behind them that generated verification properties have better\nqualities than solutions may not always hold. In this paper, we treat them\nequally as different perspectives of LLMs' reasoning processes. We propose the\nMulti-Perspective Self-Consistency (MPSC) framework incorporating both inter-\nand intra-consistency across outputs from multiple perspectives. Specifically,\nwe prompt LLMs to generate diverse outputs from three perspectives, Solution,\nSpecification and Test case, constructing a 3-partite graph. With two measure\nfunctions of consistency, we embed both inter- and intra-consistency\ninformation into the graph. The optimal choice of solutions is then determined\nbased on analysis in the graph. MPSC significantly boosts performance of\nfoundation models (ChatGPT in this paper) on various benchmarks, including\nHumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing\nGPT-4.\n",
    "related_paper_titles": [
      "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of\n  Large Language Models for Code Generation",
      "Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models",
      "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"
    ],
    "related_paper_abstract": [
      "  Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code.\nProgramming benchmarks, with curated synthesis problems and test-cases, are\nused to measure the performance of various LLMs on code synthesis. However,\nthese test-cases can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis evaluation framework to rigorously benchmark the functional\ncorrectness of LLM-synthesized code. EvalPlus augments a given evaluation\ndataset with large amounts of test-cases newly produced by an automatic test\ninput generator, powered by both LLM- and mutation-based strategies. While\nEvalPlus is general, we extend the test-cases of the popular HumanEval\nbenchmark by 80x to build HumanEval+. Our extensive evaluation across 26\npopular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to\ncatch significant amounts of previously undetected wrong code synthesized by\nLLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that\ntest insufficiency can lead to mis-ranking. For example, both\nWizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,\nwhile none of them could on HumanEval. Our work not only indicates that prior\npopular code synthesis evaluation results do not accurately reflect the true\nperformance of LLMs for code synthesis, but also opens up a new direction to\nimprove such programming benchmarks through automated testing. We have\nopen-sourced our tools, enhanced datasets as well as all LLM-generated code at\nhttps://github.com/evalplus/evalplus to facilitate and accelerate future\nLLM-for-code research.\n",
      "  Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.\n",
      "  Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM\n"
    ],
    "entities": [
      "ChatGPT",
      "ESG",
      "Persian",
      "KV",
      "KGE",
      "IFT",
      "ToM",
      "Direct Preference",
      "KG",
      "PDE",
      "NLG",
      "Large",
      "Mind",
      "PDEs",
      "PTQ",
      "Research",
      "Gemini",
      "Mistral",
      "Thought",
      "Knowledge",
      "ASR",
      "Llama",
      "Multimodal Large Language Models",
      "CoT",
      "Wikipedia",
      "Experts",
      "Automated",
      "Evaluation",
      "Indian",
      "Monte Carlo Tree Search"
    ],
    "retrieved_papers": [
      {
        "title": "Enhancing Large Language Models in Coding Through Multi-Perspective\n  Self-Consistency,",
        "abstract": "Large language models (LLMs) have exhibited remarkable ability in code\ngeneration. However, generating the correct solution in a single attempt still\nremains a challenge. Prior works utilize verification properties in software\nengineering to verify and re-rank solutions in a majority voting manner. But\nthe assumption behind them that generated verification properties have better\nqualities than solutions may not always hold. In this paper, we treat them\nequally as different perspectives of LLMs' reasoning processes. We propose the\nMulti-Perspective Self-Consistency (MPSC) framework incorporating both inter-\nand intra-consistency across outputs from multiple perspectives. Specifically,\nwe prompt LLMs to generate diverse outputs from three perspectives, Solution,\nSpecification and Test case, constructing a 3-partite graph. With two measure\nfunctions of consistency, we embed both inter- and intra-consistency\ninformation into the graph. The optimal choice of solutions is then determined\nbased on analysis in the graph. MPSC significantly boosts performance of\nfoundation models (ChatGPT in this paper) on various benchmarks, including\nHumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing\nGPT-4.",
        "score": 6.31380558013916
      },
      {
        "title": "Beyond Correctness: Benchmarking Multi-dimensional Code Generation for\n  Large Language Models,",
        "abstract": "In recent years, researchers have proposed numerous benchmarks to evaluate\nthe impressive coding capabilities of large language models (LLMs). However,\nexisting benchmarks primarily focus on assessing the correctness of code\ngenerated by LLMs, while neglecting other critical dimensions that also\nsignificantly impact code quality. Therefore, this paper proposes the RACE\nbenchmark, which comprehensively evaluates the quality of code generated by\nLLMs across 4 dimensions: Readability, mAintainability, Correctness, and\nEfficiency. Specifically, considering the demand-dependent nature of dimensions\nbeyond correctness, we design various types of user requirements for each\ndimension to assess the model's ability to generate correct code that also\nmeets user demands. We evaluate 18 representative LLMs on RACE and find that:\n1) the current LLMs' ability to generate high-quality code on demand does not\nyet meet the requirements of software development; 2) readability serves as a\ncritical indicator of the overall quality of generated code; 3) most LLMs\nexhibit an inherent preference for specific coding style. These findings can\nhelp researchers gain a deeper understanding of the coding capabilities of\ncurrent LLMs and shed light on future directions for model improvement.",
        "score": 3.259756565093994
      },
      {
        "title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language\n  Models with IdentityChain,",
        "abstract": "Code Large Language Models (Code LLMs) are being increasingly employed in\nreal-life applications, so evaluating them is critical. While the conventional\naccuracy evaluates the performance of Code LLMs on a set of individual tasks,\ntheir self-consistency across different tasks is overlooked. Intuitively, a\ntrustworthy model should be self-consistent when generating natural language\nspecifications for its own code and generating code for its own specifications.\nFailure to preserve self-consistency reveals a lack of understanding of the\nshared semantics underlying natural language and programming language, and\ntherefore undermines the trustworthiness of a model. In this paper, we first\nformally define the self-consistency of Code LLMs and then design a framework,\nIdentityChain, which effectively and efficiently evaluates the self-consistency\nand conventional accuracy of a model at the same time. We study eleven Code\nLLMs and show that they fail to preserve self-consistency, which is indeed a\ndistinct aspect from conventional accuracy. Furthermore, we show that\nIdentityChain can be used as a model debugging tool to expose weaknesses of\nCode LLMs by demonstrating three major weaknesses that we identify in current\nmodels using IdentityChain. Our code is available at\nhttps://github.com/marcusm117/IdentityChain.",
        "score": 3.0242972373962402
      },
      {
        "title": "Universal Self-Consistency for Large Language Model Generation,",
        "abstract": "Self-consistency with chain-of-thought prompting (CoT) has demonstrated\nremarkable performance gains on various challenging tasks, by utilizing\nmultiple reasoning paths sampled from large language models (LLMs). However,\nself-consistency relies on the answer extraction process to aggregate multiple\nsolutions, which is not applicable to free-form answers. In this work, we\npropose Universal Self-Consistency (USC), which leverages LLMs themselves to\nselect the most consistent answer among multiple candidates. We evaluate USC on\na variety of benchmarks, including mathematical reasoning, code generation,\nlong-context summarization, and open-ended question answering. On open-ended\ngeneration tasks where the original self-consistency method is not applicable,\nUSC effectively utilizes multiple samples and improves the performance. For\nmathematical reasoning, USC matches the standard self-consistency performance\nwithout requiring the answer formats to be similar. Finally, without access to\nexecution results, USC also matches the execution-based voting performance on\ncode generation.",
        "score": 2.6621124744415283
      },
      {
        "title": "Teaching Large Language Models to Self-Debug,",
        "abstract": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.",
        "score": 2.183358907699585
      },
      {
        "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning\n  and Coding with LLMs,",
        "abstract": "A popular approach for improving the correctness of output from large\nlanguage models (LLMs) is Self-Consistency - poll the LLM multiple times and\noutput the most frequent solution. Existing Self-Consistency techniques always\ngenerate a constant number of samples per question, where a better approach\nwill be to non-uniformly distribute the available budget based on the amount of\nagreement in the samples generated so far. In response, we introduce\nAdaptive-Consistency, a cost-efficient, model-agnostic technique that\ndynamically adjusts the number of samples per question using a lightweight\nstopping criterion. Our experiments over 17 reasoning and code generation\ndatasets and three LLMs demonstrate that Adaptive-Consistency reduces sample\nbudget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our\ncode and data are available at https://www.sample-step-by-step.info",
        "score": 2.095852851867676
      },
      {
        "title": "V-STaR: Training Verifiers for Self-Taught Reasoners,",
        "abstract": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated\nsolutions to improve their problem-solving ability. However, these approaches\ndiscard the large amounts of incorrect solutions generated during this process,\npotentially neglecting valuable information in such solutions. To address this\nshortcoming, we propose V-STaR that utilizes both the correct and incorrect\nsolutions generated during the self-improvement process to train a verifier\nusing DPO that judges correctness of model-generated solutions. This verifier\nis used at inference time to select one solution among many candidate\nsolutions. Running V-STaR for multiple iterations results in progressively\nbetter reasoners and verifiers, delivering a 4% to 17% test accuracy\nimprovement over existing self-improvement and verification approaches on\ncommon code generation and math reasoning benchmarks with LLaMA2 models.",
        "score": 1.941630482673645
      },
      {
        "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs,",
        "abstract": "Large language models (LLMs) have achieved widespread success on a variety of\nin-context few-shot tasks, but this success is typically evaluated via\ncorrectness rather than consistency. We argue that self-consistency is an\nimportant criteria for valid multi-step reasoning in tasks where the solution\nis composed of the answers to multiple sub-steps. We propose two types of\nself-consistency that are particularly important for multi-step reasoning --\nhypothetical consistency (a model's ability to predict what its output would be\nin a hypothetical other context) and compositional consistency (consistency of\na model's final outputs when intermediate sub-steps are replaced with the\nmodel's outputs for those steps). We demonstrate that multiple variants of the\nGPT-3/-4 models exhibit poor consistency rates across both types of consistency\non a variety of tasks.",
        "score": 1.9139068126678467
      },
      {
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation,",
        "abstract": "Large language models (LLMs) have demonstrated an impressive ability to\ngenerate codes on competitive programming tasks. However, with limited sample\nnumbers, LLMs still suffer from poor accuracy. Inspired by the process of human\nprogramming, we propose a generate-and-edit approach named Self-Edit that\nutilizes execution results of the generated code from LLMs to improve the code\nquality on the competitive programming task. We execute the generated code on\nthe example test case provided in the question and wrap execution results into\na supplementary comment. Utilizing this comment as guidance, our fault-aware\ncode editor is employed to correct errors in the generated code. We perform\nextensive evaluations across two competitive programming datasets with nine\ndifferent LLMs. Compared to directly generating from LLMs, our approach can\nimprove the average of pass@1 by 89\\% on APPS-dev, 31\\% on APPS-test, and 48\\%\non HumanEval over nine popular code generation LLMs with parameter sizes\nranging from 110M to 175B. Compared to other post-processing methods, our\nmethod demonstrates superior accuracy and efficiency.",
        "score": 1.8303635120391846
      },
      {
        "title": "Integrate the Essence and Eliminate the Dross: Fine-Grained\n  Self-Consistency for Free-Form Language Generation,",
        "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows\nsignificant gains on various reasoning tasks but struggles with free-form\ngeneration due to the difficulty of aggregating answers. Its variants, UCS and\nUSC, rely on sample selection or voting mechanisms to improve output quality.\nThese methods, however, face limitations due to their inability to fully\nutilize the nuanced consensus knowledge present within multiple candidate\nsamples, often resulting in suboptimal outputs. We propose Fine-Grained\nSelf-Consistency (FSC) to addresses these limitations by extracting and\nintegrating segment-level commonalities from candidate samples, enhancing the\nperformance of LLMs both in open-ended and reasoning tasks. Based on this, we\npresent two additional strategies: candidate filtering, which enhances overall\nquality by identifying highly similar candidate sets, and merging, which\nreduces input token requirements by combining similar samples. The\neffectiveness of FSC is demonstrated through extensive experiments on various\ntasks, including summarization, code generation, and mathematical reasoning,\nusing GPT-3.5-turbo and GPT-4. The results indicate significant improvements\nover baseline methods, showcasing the potential of FSC to optimize output\nquality by effectively synthesizing fine-grained consensus knowledge from\nmultiple samples.",
        "score": 1.5111466646194458
      },
      {
        "title": "Evaluating Large Language Models on Graphs: Performance Insights and\n  Comparative Analysis,",
        "abstract": "Large Language Models (LLMs) have garnered considerable interest within both\nacademic and industrial. Yet, the application of LLMs to graph data remains\nunder-explored. In this study, we evaluate the capabilities of four LLMs in\naddressing several analytical problems with graph data. We employ four distinct\nevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.\nOur results show that: 1) LLMs effectively comprehend graph data in natural\nlanguage and reason with graph topology. 2) GPT models can generate logical and\ncoherent results, outperforming alternatives in correctness. 3) All examined\nLLMs face challenges in structural reasoning, with techniques like zero-shot\nchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT\nmodels often produce erroneous answers in multi-answer tasks, raising concerns\nin fidelity. 5) GPT models exhibit elevated confidence in their outputs,\npotentially hindering their rectification capacities. Notably, GPT-4 has\ndemonstrated the capacity to rectify responses from GPT-3.5-turbo and its own\nprevious iterations. The code is available at:\nhttps://github.com/Ayame1006/LLMtoGraph.",
        "score": 1.3990942239761353
      },
      {
        "title": "Large Language Models are Better Reasoners with Self-Verification,",
        "abstract": "Recently, with the chain of thought (CoT) prompting, large language models\n(LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural\nlanguage processing tasks such as arithmetic, commonsense, and logical\nreasoning. However, LLMs with CoT require multi-step prompting and multi-token\nprediction, which is highly sensitive to individual mistakes and vulnerable to\nerror accumulation. The above issues make the LLMs need the ability to verify\nthe answers. In fact, after inferring conclusions in some thinking decision\ntasks, people often check them by re-verifying steps to avoid some mistakes. In\nthis paper, we propose and prove that LLMs also have similar self-verification\nabilities. We take the conclusion obtained by CoT as one of the conditions for\nsolving the original problem. By performing a backward verification of the\nanswers that LLM deduced for itself, we can obtain interpretable answer\nvalidation scores to select the candidate answer with the highest score.\nExperimental results demonstrate that the proposed method can improve the\nreasoning performance on various arithmetic, commonsense, and logical reasoning\ndatasets. Our code is publicly available at:\nhttps://github.com/WENGSYX/Self-Verification.",
        "score": 1.374199390411377
      },
      {
        "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning\n  Graph,",
        "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.",
        "score": 1.3485640287399292
      },
      {
        "title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through\n  Self-Correction in Language Models,",
        "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has\nbeen shown to be an effective way of boosting performance. In fact, it has been\nfound that instruction tuning on these intermediary reasoning steps improves\nmodel performance. In this work, we present a novel method of further improving\nperformance by requiring models to compare multiple reasoning chains before\ngenerating a solution in a single inference step. We call this method Divergent\nCoT (DCoT). We find that instruction tuning on DCoT datasets boosts the\nperformance of even smaller, and therefore more accessible, LLMs. Through a\nrigorous set of experiments spanning a wide range of tasks that require various\nreasoning types, we show that fine-tuning on DCoT consistently improves\nperformance over the CoT baseline across model families and scales (1.3B to\n70B). Through a combination of empirical and manual evaluation, we additionally\nshow that these performance gains stem from models generating multiple\ndivergent reasoning chains in a single inference step, indicative of the\nenabling of self-correction in language models. Our code and data are publicly\navailable at https://github.com/UKPLab/arxiv2024-divergent-cot.",
        "score": 1.3422837257385254
      },
      {
        "title": "RankPrompt: Step-by-Step Comparisons Make Language Models Better\n  Reasoners,",
        "abstract": "Large Language Models (LLMs) have achieved impressive performance across\nvarious reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT\nare prone to logical errors during their reasoning processes. Existing\nsolutions, such as deploying task-specific verifiers or voting over multiple\nreasoning paths, either require extensive human annotations or fail in\nscenarios with inconsistent responses. To address these challenges, we\nintroduce RankPrompt, a new prompting method that enables LLMs to self-rank\ntheir responses without additional resources. RankPrompt breaks down the\nranking problem into a series of comparisons among diverse responses,\nleveraging the inherent capabilities of LLMs to generate chains of comparison\nas contextual exemplars. Our experiments across 11 arithmetic and commonsense\nreasoning tasks show that RankPrompt significantly enhances the reasoning\nperformance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover,\nRankPrompt excels in LLM-based automatic evaluations for open-ended tasks,\naligning with human judgments 74% of the time in the AlpacaEval dataset. It\nalso exhibits robustness to variations in response order and consistency.\nCollectively, our results validate RankPrompt as an effective method for\neliciting high-quality feedback from language models.",
        "score": 1.275134563446045
      },
      {
        "title": "CodeMind: A Framework to Challenge Large Language Models for Code\n  Reasoning,",
        "abstract": "Solely relying on test passing to evaluate Large Language Models (LLMs) for\ncode synthesis may result in unfair assessment or promoting models with data\nleakage. As an alternative, we introduce CodeMind, a framework designed to\ngauge the code reasoning abilities of LLMs. CodeMind currently supports three\ncode reasoning tasks: Independent Execution Reasoning (IER), Dependent\nExecution Reasoning (DER), and Specification Reasoning (SR). The first two\nevaluate models to predict the execution output of an arbitrary code or code\nthe model could correctly synthesize. The third one evaluates the extent to\nwhich LLMs implement the specified expected behavior.\n  Our extensive evaluation of nine LLMs across five benchmarks in two different\nprogramming languages using CodeMind shows that LLMs fairly follow control flow\nconstructs and, in general, explain how inputs evolve to output, specifically\nfor simple programs and the ones they can correctly synthesize. However, their\nperformance drops for code with higher complexity, non-trivial logical and\narithmetic operators, non-primitive types, and API calls. Furthermore, we\nobserve that, while correlated, specification reasoning (essential for code\nsynthesis) does not imply execution reasoning (essential for broader\nprogramming tasks such as testing and debugging): ranking LLMs based on test\npassing can be different compared to code reasoning.",
        "score": 1.257351040840149
      },
      {
        "title": "MathGenie: Generating Synthetic Data with Question Back-translation for\n  Enhancing Mathematical Reasoning of LLMs,",
        "abstract": "Large language models (LLMs) have exhibited great potential in mathematical\nreasoning. However, there remains a performance gap in this area between\nexisting open-source models and closed-source models such as GPT-4. In this\npaper, we introduce MathGenie, a novel method for generating diverse and\nreliable math problems from a small-scale problem-solution dataset (denoted as\nseed data). We augment the ground-truth solutions of our seed data and train a\nback-translation model to translate the augmented solutions back into new\nquestions. Subsequently, we generate code-integrated solutions for the new\nquestions. To ensure the correctness of the code-integrated solutions, we\nemploy rationale-based strategy for solution verification. Various pretrained\nmodels, ranging from 7B to 70B, are trained on the newly curated data to test\nthe effectiveness of the proposed augmentation technique, resulting in a family\nof models known as MathGenieLM. These models consistently outperform previous\nopen-source models across five representative mathematical reasoning datasets,\nachieving state-of-the-art performance. In particular, MathGenieLM-InternLM2\nachieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best\noverall score among open-source language models.",
        "score": 1.2493712902069092
      },
      {
        "title": "Reasoning Runtime Behavior of a Program with LLM: How Far Are We?,",
        "abstract": "Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.",
        "score": 1.0981559753417969
      },
      {
        "title": "MIDGARD: Self-Consistency Using Minimum Description Length for\n  Structured Commonsense Reasoning,",
        "abstract": "We study the task of conducting structured reasoning as generating a\nreasoning graph from natural language input using large language models (LLMs).\nPrevious approaches have explored various prompting schemes, yet they suffer\nfrom error propagation due to the autoregressive nature and single-pass-based\ndecoding, which lack error correction capability. Additionally, relying solely\non a single sample may result in the omission of true nodes and edges. To\ncounter this, we draw inspiration from self-consistency (SC), which involves\nsampling a diverse set of reasoning chains and taking the majority vote as the\nfinal answer. To tackle the substantial challenge of applying SC on generated\ngraphs, we propose MIDGARD (MInimum Description length Guided Aggregation of\nReasoning in Directed acyclic graph) that leverages Minimum Description Length\n(MDL)-based formulation to identify consistent properties among the different\ngraph samples generated by an LLM. This formulation helps reject properties\nthat appear in only a few samples, which are likely to be erroneous, while\nenabling the inclusion of missing elements without compromising precision. Our\nmethod demonstrates superior performance than comparisons across various\nstructured reasoning tasks, including argument structure extraction,\nexplanation graph generation, inferring dependency relations among actions for\neveryday tasks, and semantic graph generation from natural texts.",
        "score": 0.8017076253890991
      },
      {
        "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond,",
        "abstract": "With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur.",
        "score": 0.6162983775138855
      },
      {
        "title": "On Measuring Faithfulness or Self-consistency of Natural Language\n  Explanations,",
        "abstract": "Large language models (LLMs) can explain their predictions through post-hoc\nor Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably\nsounding explanations that are unfaithful to its underlying reasoning. Recent\nwork has designed tests that aim to judge the faithfulness of post-hoc or CoT\nexplanations. In this work we argue that these faithfulness tests do not\nmeasure faithfulness to the models' inner workings -- but rather their\nself-consistency at output level. Our contributions are three-fold: i) We\nclarify the status of faithfulness tests in view of model explainability,\ncharacterising them as self-consistency tests instead. This assessment we\nunderline by ii) constructing a Comparative Consistency Bank for\nself-consistency tests that for the first time compares existing tests on a\ncommon suite of 11 open LLMs and 5 tasks -- including iii) our new\nself-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a\ntest) of LLM self-consistency. It compares how a model's input contributes to\nthe predicted answer and to generating the explanation. Our fine-grained\nCC-SHAP metric allows us iii) to compare LLM behaviour when making predictions\nand to analyse the effect of other consistency tests at a deeper level, which\ntakes us one step further towards measuring faithfulness by bringing us closer\nto the internals of the model than strictly surface output-oriented tests. Our\ncode is available at \\url{https://github.com/Heidelberg-NLP/CC-SHAP}",
        "score": 0.6021830439567566
      },
      {
        "title": "DevEval: A Manually-Annotated Code Generation Benchmark Aligned with\n  Real-World Code Repositories,",
        "abstract": "How to evaluate the coding abilities of Large Language Models (LLMs) remains\nan open question. We find that existing benchmarks are poorly aligned with\nreal-world code repositories and are insufficient to evaluate the coding\nabilities of LLMs.\n  To address the knowledge gap, we propose a new benchmark named DevEval, which\nhas three advances. (1) DevEval aligns with real-world repositories in multiple\ndimensions, e.g., code distributions and dependency distributions. (2) DevEval\nis annotated by 13 developers and contains comprehensive annotations (e.g.,\nrequirements, original repositories, reference code, and reference\ndependencies). (3) DevEval comprises 1,874 testing samples from 117\nrepositories, covering 10 popular domains (e.g., Internet, Database). Based on\nDevEval, we propose repository-level code generation and evaluate 8 popular\nLLMs on DevEval (e.g., gpt-4, gpt-3.5, StarCoder 2, DeepSeek Coder, CodeLLaMa).\nOur experiments reveal these LLMs' coding abilities in real-world code\nrepositories. For example, in our experiments, the highest Pass@1 of\ngpt-4-turbo is only 53.04%. We also analyze LLMs' failed cases and summarize\ntheir shortcomings. We hope DevEval can facilitate the development of LLMs in\nreal code repositories. DevEval, prompts, and LLMs' predictions have been\nreleased.",
        "score": 0.5450842380523682
      },
      {
        "title": "DiversiGATE: A Comprehensive Framework for Reliable Large Language\n  Models,",
        "abstract": "In this paper, we introduce DiversiGATE, a unified framework that\nconsolidates diverse methodologies for LLM verification. The proposed framework\ncomprises two main components: Diversification and Aggregation which provide a\nholistic perspective on existing verification approaches, such as\nSelf-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel\n`SelfLearner' model that conforms to the DiversiGATE framework which can learn\nfrom its own outputs and refine its performance over time, leading to improved\naccuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorous\nseries of experiments, including tests on synthetic data as well as on popular\narithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our\napproach outperforms traditional LLMs, achieving a considerable 54.8% -> 61.8%\nimprovement on the GSM8K benchmark.",
        "score": 0.5390264391899109
      },
      {
        "title": "DevEval: Evaluating Code Generation in Practical Software Projects,",
        "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open\nquestion. Many benchmarks have been proposed but are inconsistent with\npractical software projects, e.g., unreal program distributions, insufficient\ndependencies, and small-scale project contexts. Thus, the capabilities of LLMs\nin practical projects are still unclear. In this paper, we propose a new\nbenchmark named DevEval, aligned with Developers' experiences in practical\nprojects. DevEval is collected through a rigorous pipeline, containing 2,690\nsamples from 119 practical projects and covering 10 domains. Compared to\nprevious benchmarks, DevEval aligns to practical projects in multiple\ndimensions, e.g., real program distributions, sufficient dependencies, and\nenough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,\ngpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual\nabilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo\nonly is 42 in our experiments. We also discuss the challenges and future\ndirections of code generation in practical projects. We open-source DevEval and\nhope it can facilitate the development of code generation in practical\nprojects.",
        "score": 0.2649413049221039
      },
      {
        "title": "CodeS: Natural Language to Code Repository via Multi-Layer Sketch,",
        "abstract": "The impressive performance of large language models (LLMs) on code-related\ntasks has shown the potential of fully automated software development. In light\nof this, we introduce a new software engineering task, namely Natural Language\nto code Repository (NL2Repo). This task aims to generate an entire code\nrepository from its natural language requirements. To address this task, we\npropose a simple yet effective framework CodeS, which decomposes NL2Repo into\nmultiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three\nmodules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first\ngenerates a repository's directory structure for given requirements;\nFileSketcher then generates a file sketch for each file in the generated\nstructure; SketchFiller finally fills in the details for each function in the\ngenerated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry\nout evaluations through both automated benchmarking and manual feedback\nanalysis. For benchmark-based evaluation, we craft a repository-oriented\nbenchmark, SketchEval, and design an evaluation metric, SketchBLEU. For\nfeedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30\nparticipants in conducting empirical studies. Extensive experiments prove the\neffectiveness and practicality of CodeS on the NL2Repo task.",
        "score": 0.21162104606628418
      },
      {
        "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by\n  Reversing Chain-of-Thought,",
        "abstract": "Large language Models (LLMs) have achieved promising performance on\narithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting. However, LLMs face challenges in maintaining factual consistency\nduring reasoning, exhibiting tendencies to condition overlooking, question\nmisinterpretation, and condition hallucination over given problems. Existing\nmethods use coarse-grained feedback (e.g., whether the answer is correct) to\nimprove factual consistency. In this work, we propose RCoT (Reversing\nChain-of-Thought), a novel method to improve LLMs' reasoning abilities by\nautomatically detecting and rectifying factual inconsistency in LLMs, generated\nsolutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct\nthe problem based on generated solutions. Then fine-grained comparisons between\nthe original problem and the reconstructed problem expose the factual\ninconsistency in the original solutions. To rectify the solution, RCoT\nformulates detected factual inconsistency into fine-grained feedback to guide\nLLMs in revising solutions. Experimental results demonstrate improvements of\nRCoT over standard CoT, Self-Consistency and Self-Refine across seven\narithmetic datasets. Moreover, we find that manually written fine-grained\nfeedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT\nreaches 94.6% accuracy on GSM8K), encouraging the community to further explore\nthe fine-grained feedback generation methods.",
        "score": 0.06264401972293854
      },
      {
        "title": "Large Language Models Can Self-Correct with Minimal Effort,",
        "abstract": "Intrinsic self-correct was a method that instructed large language models\n(LLMs) to verify and correct their responses without external feedback.\nUnfortunately, the study concluded that the LLMs could not self-correct\nreasoning yet. We find that a simple yet effective verification method can\nunleash inherent capabilities of the LLMs. That is to mask a key condition in\nthe question, add the current response to construct a verification question,\nand predict the condition to verify the response. The condition can be an\nentity in an open-domain question or a numeric value in a math question, which\nrequires minimal effort (via prompting) to identify. We propose an iterative\nverify-then-correct framework to progressively identify and correct (probably)\nfalse responses, named ProCo. We conduct experiments on three reasoning tasks.\nOn average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact\nmatch on four open-domain question answering datasets, $+14.1$ accuracy on\nthree arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense\nreasoning dataset, compared to Self-Correct.",
        "score": -0.12343300133943558
      },
      {
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step\n  Reasoning,",
        "abstract": "The recent progress in large language models (LLMs), especially the invention\nof chain-of-thought prompting, has made it possible to automatically answer\nquestions by stepwise reasoning. However, when faced with more complicated\nproblems that require non-linear thinking, even the strongest LLMs make\nmistakes. To address this, we explore whether LLMs are able to recognize errors\nin their own step-by-step reasoning, without resorting to external resources.\nTo this end, we propose SelfCheck, a general-purpose zero-shot verification\nschema for recognizing such errors. We then use the results of these checks to\nimprove question-answering performance by conducting weighted voting on\nmultiple solutions to the question. We test SelfCheck on three datasets (GSM8K,\nMathQA, and MATH) and find that it successfully recognizes errors and, in turn,\nincreases final answer accuracies.",
        "score": -0.24228279292583466
      },
      {
        "title": "Benchmarking and Improving Generator-Validator Consistency of Language\n  Models,",
        "abstract": "As of September 2023, ChatGPT correctly answers \"what is 7+8\" with 15, but\nwhen asked \"7+8=15, True or False\" it responds with \"False\". This inconsistency\nbetween generating and validating an answer is prevalent in language models\n(LMs) and erodes trust. In this paper, we propose a framework for measuring the\nconsistency between generation and validation (which we call\ngenerator-validator consistency, or GV-consistency), finding that even GPT-4, a\nstate-of-the-art LM, is GV-consistent only 76% of the time. To improve the\nconsistency of LMs, we propose to finetune on the filtered generator and\nvalidator responses that are GV-consistent, and call this approach consistency\nfine-tuning. We find that this approach improves GV-consistency of Alpaca-30B\nfrom 60% to 93%, and the improvement extrapolates to unseen tasks and domains\n(e.g., GV-consistency for positive style transfers extrapolates to unseen\nstyles like humor). In addition to improving consistency, consistency\nfine-tuning improves both generator quality and validator accuracy without\nusing any labeled data. Evaluated across 6 tasks, including math questions,\nknowledge-intensive QA, and instruction following, our method improves the\ngenerator quality by 16% and the validator accuracy by 6.3% across all tasks.",
        "score": -1.5589122772216797
      },
      {
        "title": "Ranking LLM-Generated Loop Invariants for Program Verification,",
        "abstract": "Synthesizing inductive loop invariants is fundamental to automating program\nverification. In this work, we observe that Large Language Models (such as\ngpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of\nprograms in a 0-shot setting, yet require several samples to generate the\ncorrect invariants. This can lead to a large number of calls to a program\nverifier to establish an invariant. To address this issue, we propose a {\\it\nre-ranking} approach for the generated results of LLMs. We have designed a\nranker that can distinguish between correct inductive invariants and incorrect\nattempts based on the problem definition. The ranker is optimized as a\ncontrastive ranker. Experimental results demonstrate that this re-ranking\nmechanism significantly improves the ranking of correct invariants among the\ngenerated candidates, leading to a notable reduction in the number of calls to\na verifier. The source code and the experimental data for this paper are\navailable in \\url{https://github.com/microsoft/NeuralInvariantRanker}.",
        "score": -1.8670666217803955
      }
    ]
  },
  {
    "title": "How Abilities in Large Language Models are Affected by Supervised\n  Fine-tuning Data Composition",
    "abstract": "  Large language models (LLMs) with enormous pre-training tokens and parameters\nemerge diverse abilities, including math reasoning, code generation, and\ninstruction following. These abilities are further enhanced by supervised\nfine-tuning (SFT). While the open-source community has explored ad-hoc SFT for\nenhancing individual capabilities, proprietary LLMs exhibit versatility across\nvarious skills. Therefore, understanding the facilitation of multiple abilities\nvia SFT is paramount. In this study, we specifically focuses on the interplay\nof data composition between mathematical reasoning, code generation, and\ngeneral human-aligning abilities during SFT. We propose four intriguing\nresearch questions to explore the association between model performance and\nvarious factors including data amount, composition ratio, model size and SFT\nstrategies. Our experiments reveal that distinct capabilities scale differently\nand larger models generally show superior performance with same amount of data.\nMathematical reasoning and code generation consistently improve with increasing\ndata amount, whereas general abilities plateau after roughly a thousand\nsamples. Moreover, we observe data composition appears to enhance various\nabilities under limited data conditions, yet can lead to performance conflicts\nwhen data is plentiful. Our findings also suggest the amount of composition\ndata influences performance more than the composition ratio. In analysis of SFT\nstrategies, we find that sequentially learning multiple skills risks\ncatastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)\nstrategy offers a promising solution to learn multiple abilities with different\nscaling patterns.\n",
    "related_paper_titles": [
      "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of\n  Large Language Models",
      "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open\n  Resources",
      "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"
    ],
    "related_paper_abstract": [
      "  Foundation language models obtain the instruction-following ability through\nsupervised fine-tuning (SFT). Diversity and complexity are considered critical\nfactors of a successful SFT dataset, while their definitions remain obscure and\nlack quantitative analyses. In this work, we propose InsTag, an open-set\nfine-grained tagger, to tag samples within SFT datasets based on semantics and\nintentions and define instruction diversity and complexity regarding tags. We\nobtain 6.6K tags to describe comprehensive user queries. Then we analyze\npopular open-sourced SFT datasets and find that the model ability grows with\nmore diverse and complex data. Based on this observation, we propose a data\nselector based on InsTag to select 6K diverse and complex samples from\nopen-source datasets and fine-tune models on InsTag-selected data. The\nresulting models, TagLM, outperform open-source models based on considerably\nlarger SFT data evaluated by MT-Bench, echoing the importance of query\ndiversity and complexity. We open-source InsTag in\nhttps://github.com/OFA-Sys/InsTag.\n",
      "  In this work we explore recent advances in instruction-tuning language models\non a range of open instruction-following datasets. Despite recent claims that\nopen models can be on par with state-of-the-art proprietary models, these\nclaims are often accompanied by limited evaluation, making it difficult to\ncompare models across the board and determine the utility of various resources.\nWe provide a large set of instruction-tuned models from 6.7B to 65B parameters\nin size, trained on 12 instruction datasets ranging from manually curated\n(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and\nsystematically evaluate them on their factual knowledge, reasoning,\nmultilinguality, coding, and open-ended instruction following abilities through\na collection of automatic, model-based, and human-based metrics. We further\nintroduce T\\\"ulu, our best performing instruction-tuned model suite finetuned\non a combination of high-quality open resources. Our experiments show that\ndifferent instruction-tuning datasets can uncover or enhance specific skills,\nwhile no single dataset (or combination) provides the best performance across\nall evaluations. Interestingly, we find that model and human preference-based\nevaluations fail to reflect differences in model capabilities exposed by\nbenchmark-based evaluations, suggesting the need for the type of systemic\nevaluation performed in this work. Our evaluations show that the best model in\nany given evaluation reaches on average 87% of ChatGPT performance, and 73% of\nGPT-4 performance, suggesting that further investment in building better base\nmodels and instruction-tuning data is required to close the gap. We release our\ninstruction-tuned models, including a fully finetuned 65B T\\\"ulu, along with\nour code, data, and evaluation framework at\nhttps://github.com/allenai/open-instruct to facilitate future research.\n",
      "  Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM\n"
    ],
    "entities": [
      "Large Language Models Large",
      "Large Language Models Large Language Models",
      "Retrieval Augmented",
      "SQL",
      "ChatGPT",
      "Copilot",
      "GenAI",
      "Korean",
      "Multimodal",
      "TSF",
      "Mistral",
      "EHR",
      "QA",
      "Gemini",
      "Verilog",
      "KGC",
      "PDDL",
      "Methods",
      "EHRs",
      "ToM",
      "GPT",
      "GAI",
      "LLaMA",
      "DMs",
      "KGs",
      "XAI",
      "MMLU",
      "SFT",
      "NLG",
      "KG"
    ],
    "retrieved_papers": [
      {
        "title": "How Abilities in Large Language Models are Affected by Supervised\n  Fine-tuning Data Composition,",
        "abstract": "Large language models (LLMs) with enormous pre-training tokens and parameters\nemerge diverse abilities, including math reasoning, code generation, and\ninstruction following. These abilities are further enhanced by supervised\nfine-tuning (SFT). While the open-source community has explored ad-hoc SFT for\nenhancing individual capabilities, proprietary LLMs exhibit versatility across\nvarious skills. Therefore, understanding the facilitation of multiple abilities\nvia SFT is paramount. In this study, we specifically focuses on the interplay\nof data composition between mathematical reasoning, code generation, and\ngeneral human-aligning abilities during SFT. We propose four intriguing\nresearch questions to explore the association between model performance and\nvarious factors including data amount, composition ratio, model size and SFT\nstrategies. Our experiments reveal that distinct capabilities scale differently\nand larger models generally show superior performance with same amount of data.\nMathematical reasoning and code generation consistently improve with increasing\ndata amount, whereas general abilities plateau after roughly a thousand\nsamples. Moreover, we observe data composition appears to enhance various\nabilities under limited data conditions, yet can lead to performance conflicts\nwhen data is plentiful. Our findings also suggest the amount of composition\ndata influences performance more than the composition ratio. In analysis of SFT\nstrategies, we find that sequentially learning multiple skills risks\ncatastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)\nstrategy offers a promising solution to learn multiple abilities with different\nscaling patterns.",
        "score": 5.807745456695557
      },
      {
        "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large\n  Language Models Reasoning,",
        "abstract": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost reasoning abilities during LLM pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during the IFT stage? To explore this, we thoroughly examine the\nimpact of coding data across different coding data proportions, model families,\nsizes, and reasoning domains, from various perspectives. Specifically, we\ncreate three IFT datasets with increasing coding data proportions, fine-tune\nsix LLM backbones across different families and scales on these datasets,\nevaluate the tuned models' performance across twelve tasks in three reasoning\ndomains, and analyze the outcomes from three broad-to-granular perspectives:\noverall, domain-level, and task-specific. Our holistic analysis provides\nvaluable insights in each perspective. First, coding data tuning enhances the\noverall reasoning capabilities of LLMs across different model families and\nscales. Moreover, the effect of coding data varies among different domains but\nshows consistent trends across model families and scales within each domain.\nAdditionally, coding data generally yields comparable task-specific benefits\nacross different model families, with the optimal coding data proportions in\nIFT datasets being task-specific.",
        "score": 3.7550764083862305
      },
      {
        "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models,",
        "abstract": "While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.",
        "score": 3.432929039001465
      },
      {
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large\n  Language Models,",
        "abstract": "Mathematical reasoning is a challenging task for large language models\n(LLMs), while the scaling relationship of it with respect to LLM capacity is\nunder-explored. In this paper, we investigate how the pre-training loss,\nsupervised data amount, and augmented data amount influence the reasoning\nperformances of a supervised LLM. We find that pre-training loss is a better\nindicator of the model's performance than the model's parameter count. We apply\nsupervised fine-tuning (SFT) with different amounts of supervised data and\nempirically find a log-linear relation between data amount and model\nperformance, and we find better models improve less with enlarged supervised\ndatasets. To augment more data samples for improving model performances without\nany human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT\nuses supervised models to generate and collect correct reasoning paths as\naugmented fine-tuning datasets. We find with augmented samples containing more\ndistinct reasoning paths, RFT improves mathematical reasoning performance more\nfor LLMs. We also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push\nLLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised\nfine-tuning (SFT) accuracy of 35.9\\% significantly.",
        "score": 3.398874521255493
      },
      {
        "title": "Assessing the Emergent Symbolic Reasoning Abilities of Llama Large\n  Language Models,",
        "abstract": "Large Language Models (LLMs) achieve impressive performance in a wide range\nof tasks, even if they are often trained with the only objective of chatting\nfluently with users. Among other skills, LLMs show emergent abilities in\nmathematical reasoning benchmarks, which can be elicited with appropriate\nprompting methods. In this work, we systematically investigate the capabilities\nand limitations of popular open-source LLMs on different symbolic reasoning\ntasks. We evaluate three models of the Llama 2 family on two datasets that\nrequire solving mathematical formulas of varying degrees of difficulty. We test\na generalist LLM (Llama 2 Chat) as well as two fine-tuned versions of Llama 2\n(MAmmoTH and MetaMath) specifically designed to tackle mathematical problems.\nWe observe that both increasing the scale of the model and fine-tuning it on\nrelevant tasks lead to significant performance gains. Furthermore, using\nfine-grained evaluation measures, we find that such performance gains are\nmostly observed with mathematical formulas of low complexity, which\nnevertheless often remain challenging even for the largest fine-tuned models.",
        "score": 3.232159376144409
      },
      {
        "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models,",
        "abstract": "Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.",
        "score": 2.915024757385254
      },
      {
        "title": "INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large\n  Language Models,",
        "abstract": "Instruction-tuned large language models have revolutionized natural language\nprocessing and have shown great potential in applications such as\nconversational agents. These models, such as GPT-4, can not only master\nlanguage but also solve complex tasks in areas like mathematics, coding,\nmedicine, and law. Despite their impressive capabilities, there is still a lack\nof comprehensive understanding regarding their full potential, primarily due to\nthe black-box nature of many models and the absence of holistic evaluation\nstudies. To address these challenges, we present INSTRUCTEVAL, a more\ncomprehensive evaluation suite designed specifically for instruction-tuned\nlarge language models. Unlike previous works, our evaluation involves a\nrigorous assessment of models based on problem-solving, writing ability, and\nalignment to human values. We take a holistic approach to analyze various\nfactors affecting model performance, including the pretraining foundation,\ninstruction-tuning data, and training methods. Our findings reveal that the\nquality of instruction data is the most crucial factor in scaling model\nperformance. While open-source models demonstrate impressive writing abilities,\nthere is substantial room for improvement in problem-solving and alignment. We\nare encouraged by the rapid development of models by the open-source community,\nbut we also highlight the need for rigorous evaluation to support claims made\nabout these models. Through INSTRUCTEVAL, we aim to foster a deeper\nunderstanding of instruction-tuned models and advancements in their\ncapabilities. INSTRUCTEVAL is publicly available at\nhttps://github.com/declare-lab/instruct-eval.",
        "score": 2.9059877395629883
      },
      {
        "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large\n  Language Models,",
        "abstract": "Recently, large language models (LLMs), especially those that are pretrained\non code, have demonstrated strong capabilities in generating programs from\nnatural language inputs in a few-shot or even zero-shot manner. Despite\npromising results, there is a notable lack of a comprehensive evaluation of\nthese models language-to-code generation capabilities. Existing studies often\nfocus on specific tasks, model architectures, or learning paradigms, leading to\na fragmented understanding of the overall landscape. In this work, we present\nL2CEval, a systematic evaluation of the language-to-code generation\ncapabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,\nmath reasoning and Python programming, analyzing the factors that potentially\naffect their performance, such as model size, pretraining data, instruction\ntuning, and different prompting methods. In addition to assessing model\nperformance, we measure confidence calibration for the models and conduct human\nevaluations of the output programs. This enables us to identify and analyze the\ntypical failure modes across various tasks and models. L2CEval offers a\ncomprehensive understanding of the capabilities and limitations of LLMs in\nlanguage-to-code generation. We also release the evaluation framework and all\nmodel outputs, hoping to lay the groundwork for further future research in this\ndomain.",
        "score": 2.894423723220825
      },
      {
        "title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and\n  Multi-Objective Instruction Tuning,",
        "abstract": "Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.",
        "score": 2.8865129947662354
      },
      {
        "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low\n  Training Data Instruction Tuning,",
        "abstract": "Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.",
        "score": 2.7898385524749756
      },
      {
        "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for\n  Language Models,",
        "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and\nscalable method for instruction tuning of Large Language Models (LLMs). Unlike\nprior work that relies on seed examples or existing datasets to construct\ninstruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of\nhuman knowledge and capabilities as input and generates large-scale synthetic\ninstruction data across all disciplines. Specifically, inspired by the\nsystematic structure in human education system, we build the taxonomy by\ndecomposing human knowledge and capabilities to various fields, sub-fields and\nultimately, distinct disciplines semi-automatically, facilitated by LLMs.\nSubsequently, we generate a comprehensive list of subjects for every discipline\nand proceed to design a syllabus tailored to each subject, again utilizing\nLLMs. With the fine-grained key concepts detailed in every class session of the\nsyllabus, we are able to generate diverse instructions with a broad coverage\nacross the entire spectrum of human knowledge and skills. Extensive experiments\non large language models (e.g., Mistral) demonstrate that GLAN excels in\nmultiple dimensions from mathematical reasoning, coding, academic exams,\nlogical reasoning to general instruction following without using task-specific\ntraining data of these tasks. In addition, GLAN allows for easy customization\nand new fields or skills can be added by simply incorporating a new node into\nour taxonomy.",
        "score": 2.5937771797180176
      },
      {
        "title": "AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight\n  Tuning on Multi-source Data,",
        "abstract": "Open-source Large Language Models (LLMs) and their specialized variants,\nparticularly Code LLMs, have recently delivered impressive performance.\nHowever, previous Code LLMs are typically fine-tuned on single-source data with\nlimited quality and diversity, which may insufficiently elicit the potential of\npre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of\nCode LLMs with enhanced code generation and generalization capabilities\nfine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent\nconflicts among the various styles and qualities in multi-source code corpora\nand introduce data-specific prompts with hindsight relabeling, termed\nAlchemistPrompts, to harmonize different data sources and instruction-response\npairs. Additionally, we propose incorporating the data construction process\ninto the fine-tuning data as code comprehension tasks, including instruction\nevolution, data filtering, and code review. Extensive experiments demonstrate\nthat AlchemistCoder holds a clear lead among all models of the same size\n(6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing\nthe efficacy of our method in refining instruction-following capabilities and\nadvancing the boundaries of code intelligence.",
        "score": 2.5470550060272217
      },
      {
        "title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction\n  Following Ability of Large Language Models,",
        "abstract": "It is imperative for Large language models (LLMs) to follow instructions with\nelaborate requirements (i.e. Complex Instructions Following). Yet, it remains\nunder-explored how to enhance the ability of LLMs to follow complex\ninstructions with multiple constraints. To bridge the gap, we initially study\nwhat training data is effective in enhancing complex constraints following\nabilities. We found that training LLMs with instructions containing multiple\nconstraints enhances their understanding of complex instructions, especially\nthose with lower complexity levels. The improvement can even generalize to\ncompositions of out-of-domain constraints. Additionally, we further propose\nmethods addressing how to obtain and utilize the effective training data.\nFinally, we conduct extensive experiments to prove the effectiveness of our\nmethods in terms of overall performance and training efficiency. We also\ndemonstrate that our methods improve models' ability to follow instructions\ngenerally and generalize effectively across out-of-domain, in-domain, and\nadversarial settings, while maintaining general capabilities.",
        "score": 2.4042840003967285
      },
      {
        "title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery,",
        "abstract": "Computer Science (CS) stands as a testament to the intricacies of human\nintelligence, profoundly advancing the development of artificial intelligence\nand modern society. However, the current community of large language models\n(LLMs) overly focuses on benchmarks for analyzing specific foundational skills\n(e.g. mathematics and code generation), neglecting an all-round evaluation of\nthe computer science field. To bridge this gap, we introduce CS-Bench, the\nfirst bilingual (Chinese-English) benchmark dedicated to evaluating the\nperformance of LLMs in computer science. CS-Bench comprises approximately 5K\nmeticulously curated test samples, covering 26 subfields across 4 key areas of\ncomputer science, encompassing various task forms and divisions of knowledge\nand reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of\nover 30 mainstream LLMs, revealing the relationship between CS performance and\nmodel scales. We also quantitatively analyze the reasons for failures in\nexisting LLMs and highlight directions for improvements, including knowledge\nsupplementation and CS-specific reasoning. Further cross-capability experiments\nshow a high correlation between LLMs' capabilities in computer science and\ntheir abilities in mathematics and coding. Moreover, expert LLMs specialized in\nmathematics and coding also demonstrate strong performances in several CS\nsubfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM\napplications in the CS field and paving new avenues in assessing LLMs' diverse\nreasoning capabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench.",
        "score": 2.1169018745422363
      },
      {
        "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large\n  Language Models,",
        "abstract": "Aligned large language models (LLMs) demonstrate exceptional capabilities in\ntask-solving, following instructions, and ensuring safety. However, the\ncontinual learning aspect of these aligned LLMs has been largely overlooked.\nExisting continual learning benchmarks lack sufficient challenge for leading\naligned LLMs, owing to both their simplicity and the models' potential exposure\nduring instruction tuning. In this paper, we introduce TRACE, a novel benchmark\ndesigned to evaluate continual learning in LLMs. TRACE consists of 8 distinct\ndatasets spanning challenging tasks including domain-specific tasks,\nmultilingual capabilities, code generation, and mathematical reasoning. All\ndatasets are standardized into a unified format, allowing for effortless\nautomatic evaluation of LLMs. Our experiments show that after training on\nTRACE, aligned LLMs exhibit significant declines in both general ability and\ninstruction-following capabilities. For example, the accuracy of llama2-chat\n13B on gsm8k dataset declined precipitously from 28.8\\% to 2\\% after training\non our datasets. This highlights the challenge of finding a suitable tradeoff\nbetween achieving performance on specific tasks while preserving the original\nprowess of LLMs. Empirical findings suggest that tasks inherently equipped with\nreasoning paths contribute significantly to preserving certain capabilities of\nLLMs against potential declines. Motivated by this, we introduce the\nReasoning-augmented Continual Learning (RCL) approach. RCL integrates\ntask-specific cues with meta-rationales, effectively reducing catastrophic\nforgetting in LLMs while expediting convergence on novel tasks.",
        "score": 2.108764886856079
      },
      {
        "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of\n  Language Models,",
        "abstract": "While large language models have achieved remarkable performance on various\ncode generation benchmarks, there have been growing concerns regarding\npotential contamination of these benchmarks as they may be leaked into\npretraining and finetuning data. While recent work has investigated\ncontamination in natural language generation and understanding tasks, there has\nbeen less extensive research into how data contamination impacts the evaluation\nof code generation, which is critical for understanding the robustness and\nreliability of LLMs in programming contexts. In this work, we perform a\ncomprehensive study of data contamination of popular code generation\nbenchmarks, and precisely quantify their overlap with pretraining corpus\nthrough both surface-level and semantic-level matching. In our experiments, we\nshow that there are substantial overlap between popular code generation\nbenchmarks and open training corpus, and models perform significantly better on\nthe subset of the benchmarks where similar solutions are seen during training.\nWe also conduct extensive analysis on the factors that affects model\nmemorization and generalization, such as model size, problem difficulty, and\nquestion length. We release all resulting files from our matching pipeline for\nfuture research.",
        "score": 2.0538790225982666
      },
      {
        "title": "Dynamics of Instruction Tuning: Each Ability of Large Language Models\n  Has Its Own Growth Pace,",
        "abstract": "Instruction tuning is a burgeoning method to elicit the general intelligence\nof Large Language Models (LLMs). However, the creation of instruction data is\nstill largely heuristic, leading to significant variation in quantity and\nquality across existing datasets. While some research advocates for expanding\nthe number of instructions, others suggest that a small set of well-chosen\nexamples is adequate. To better understand data construction guidelines, our\nresearch provides a granular analysis of how data volume, parameter size, and\ndata construction methods influence the development of each underlying ability\nof LLM, such as creative writing, code generation, and logical reasoning. We\npresent a meticulously curated dataset with over 40k instances across ten\nabilities and examine instruction-tuned models with 7b to 33b parameters. Our\nstudy reveals three primary findings: (i) Despite the models' overall\nperformance being tied to data and parameter scale, individual abilities have\ndifferent sensitivities to these factors. (ii) Human-curated data strongly\noutperforms synthetic data from GPT-4 in efficiency and can constantly enhance\nmodel performance with volume increases, but is unachievable with synthetic\ndata. (iii) Instruction data brings powerful cross-ability generalization, as\nevidenced by out-of-domain evaluations. Furthermore, we demonstrate how these\nfindings can guide more efficient data constructions, leading to practical\nperformance improvements on two public benchmarks.",
        "score": 2.0090620517730713
      },
      {
        "title": "Specializing Smaller Language Models towards Multi-Step Reasoning,",
        "abstract": "The surprising ability of Large Language Models (LLMs) to perform well on\ncomplex reasoning with only few-shot chain-of-thought prompts is believed to\nemerge only in very large-scale models (100+ billion parameters). We show that\nsuch abilities can, in fact, be distilled down from GPT-3.5 ($\\ge$ 175B) to T5\nvariants ($\\le$ 11B). We propose model specialization, to specialize the\nmodel's ability towards a target task. The hypothesis is that large models\n(commonly viewed as larger than 100B) have strong modeling power, but are\nspread on a large spectrum of tasks. Small models (commonly viewed as smaller\nthan 10B) have limited model capacity, but if we concentrate their capacity on\na specific target task, the model can achieve a decent improved performance. We\nuse multi-step math reasoning as our testbed because it is a very typical\nemergent ability. We show two important aspects of model abilities: (1). there\nexists a very complex balance/ tradeoff between language models'\nmulti-dimensional abilities; (2). by paying the price of decreased generic\nability, we can clearly lift up the scaling curve of models smaller than 10B\ntowards a specialized multi-step math reasoning ability. We further give\ncomprehensive discussions about important design choices for better\ngeneralization, including the tuning data format, the start model checkpoint,\nand a new model selection method. We hope our practice and discoveries can\nserve as an important attempt towards specialized smaller models in the new\nresearch paradigm set by LLMs.",
        "score": 1.852518916130066
      },
      {
        "title": "Large Language Models Are Reasoning Teachers,",
        "abstract": "Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model's ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher.",
        "score": 1.7914284467697144
      },
      {
        "title": "Revealing the structure of language model capabilities,",
        "abstract": "Building a theoretical understanding of the capabilities of large language\nmodels (LLMs) is vital for our ability to predict and explain the behavior of\nthese systems. Here, we investigate the structure of LLM capabilities by\nextracting latent capabilities from patterns of individual differences across a\nvaried population of LLMs. Using a combination of Bayesian and frequentist\nfactor analysis, we analyzed data from 29 different LLMs across 27 cognitive\ntasks. We found evidence that LLM capabilities are not monolithic. Instead,\nthey are better explained by three well-delineated factors that represent\nreasoning, comprehension and core language modeling. Moreover, we found that\nthese three factors can explain a high proportion of the variance in model\nperformance. These results reveal a consistent structure in the capabilities of\ndifferent LLMs and demonstrate the multifaceted nature of these capabilities.\nWe also found that the three abilities show different relationships to model\nproperties such as model size and instruction tuning. These patterns help\nrefine our understanding of scaling laws and indicate that changes to a model\nthat improve one ability might simultaneously impair others. Based on these\nfindings, we suggest that benchmarks could be streamlined by focusing on tasks\nthat tap into each broad model ability.",
        "score": 1.732953429222107
      },
      {
        "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?,",
        "abstract": "Large Language Models (LLMs) have exhibited remarkable reasoning capabilities\nand become the foundation of language technologies. Inspired by the great\nsuccess of code data in training LLMs, we naturally wonder at which training\nstage introducing code data can really help LLMs reasoning. To this end, this\npaper systematically explores the impact of code data on LLMs at different\nstages. Concretely, we introduce the code data at the pre-training stage,\ninstruction-tuning stage, and both of them, respectively. Then, the reasoning\ncapability of LLMs is comprehensively and fairly evaluated via six reasoning\ntasks in five domains. We critically analyze the experimental results and\nprovide conclusions with insights. First, pre-training LLMs with the mixture of\ncode and text can significantly enhance LLMs' general reasoning capability\nalmost without negative transfer on other tasks. Besides, at the\ninstruction-tuning stage, code data endows LLMs the task-specific reasoning\ncapability. Moreover, the dynamic mixing strategy of code and text data assists\nLLMs to learn reasoning capability step-by-step during training. These insights\ndeepen the understanding of LLMs regarding reasoning ability for their\napplication, such as scientific question answering, legal support, etc. The\nsource code and model parameters are released at the\nlink:~\\url{https://github.com/yingweima2022/CodeLLM}.",
        "score": 1.692473292350769
      },
      {
        "title": "MHPP: Exploring the Capabilities and Limitations of Language Models\n  Beyond Basic Code Generation,",
        "abstract": "Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4 has\nachieved an 88.4% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 140 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 22\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. Dataset and code are available at\nhttps://github.com/SparksofAGI/MHPP.",
        "score": 1.5382353067398071
      },
      {
        "title": "Are Large Language Models Good Statisticians?,",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of scientific tasks including mathematics, physics, and chemistry.\nDespite their successes, the effectiveness of LLMs in handling complex\nstatistical tasks remains systematically under-explored. To bridge this gap, we\nintroduce StatQA, a new benchmark designed for statistical analysis tasks.\nStatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in\nspecialized statistical tasks and their applicability assessment capabilities,\nparticularly for hypothesis testing methods. We systematically experiment with\nrepresentative LLMs using various prompting strategies and show that even\nstate-of-the-art models such as GPT-4o achieve a best performance of only\n64.83%, indicating significant room for improvement. Notably, while open-source\nLLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit\nmarked improvements, outperforming all in-context learning-based methods (e.g.\nGPT-4o). Moreover, our comparative human experiments highlight a striking\ncontrast in error types between LLMs and humans: LLMs primarily make\napplicability errors, whereas humans mostly make statistical task confusion\nerrors. This divergence highlights distinct areas of proficiency and\ndeficiency, suggesting that combining LLM and human expertise could lead to\ncomplementary strengths, inviting further investigation into their\ncollaborative potential.",
        "score": 1.4300274848937988
      },
      {
        "title": "From Symbolic Tasks to Code Generation: Diversification Yields Better\n  Task Performers,",
        "abstract": "Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.",
        "score": 1.3492481708526611
      },
      {
        "title": "WizardLM: Empowering Large Language Models to Follow Complex\n  Instructions,",
        "abstract": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM",
        "score": 1.3216584920883179
      },
      {
        "title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through\n  Self-Correction in Language Models,",
        "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has\nbeen shown to be an effective way of boosting performance. In fact, it has been\nfound that instruction tuning on these intermediary reasoning steps improves\nmodel performance. In this work, we present a novel method of further improving\nperformance by requiring models to compare multiple reasoning chains before\ngenerating a solution in a single inference step. We call this method Divergent\nCoT (DCoT). We find that instruction tuning on DCoT datasets boosts the\nperformance of even smaller, and therefore more accessible, LLMs. Through a\nrigorous set of experiments spanning a wide range of tasks that require various\nreasoning types, we show that fine-tuning on DCoT consistently improves\nperformance over the CoT baseline across model families and scales (1.3B to\n70B). Through a combination of empirical and manual evaluation, we additionally\nshow that these performance gains stem from models generating multiple\ndivergent reasoning chains in a single inference step, indicative of the\nenabling of self-correction in language models. Our code and data are publicly\navailable at https://github.com/UKPLab/arxiv2024-divergent-cot.",
        "score": 0.9326134920120239
      },
      {
        "title": "A Preliminary Study of the Intrinsic Relationship between Complexity and\n  Alignment,",
        "abstract": "Training large language models (LLMs) with open-domain instruction data has\nyielded remarkable success in aligning to end tasks and human preferences.\nExtensive research has highlighted the importance of the quality and diversity\nof instruction data. However, the impact of data complexity, as a crucial\nmetric, remains relatively unexplored from three aspects: (1)where the\nsustainability of performance improvements with increasing complexity is\nuncertain; (2)whether the improvement brought by complexity merely comes from\nintroducing more training tokens; and (3)where the potential benefits of\nincorporating instructions from easy to difficult are not yet fully understood.\nIn this paper, we propose Tree-Instruct to systematically enhance the\ninstruction complexity in a controllable manner. By adding a specified number\nof nodes to instructions' semantic trees, this approach not only yields new\ninstruction data from the modified tree but also allows us to control the\ndifficulty level of modified instructions. Our preliminary experiments reveal\nthe following insights: (1)Increasing complexity consistently leads to\nsustained performance improvements of LLMs. (2)Under the same token budget, a\nfew complex instructions outperform diverse yet simple instructions.\n(3)Curriculum instruction tuning might not yield the anticipated results;\nfocusing on increasing complexity appears to be the key.",
        "score": 0.8171674609184265
      },
      {
        "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning,",
        "abstract": "Generating diverse and sophisticated instructions for downstream tasks by\nLarge Language Models (LLMs) is pivotal for advancing the effect. Current\napproaches leverage closed-source LLMs, employing in-context prompting for\ninstruction generation. However, in this paper, we found that in-context\nprompting cannot generate complex instructions with length $\\ge 100$ for tasks\nlike code completion.\n  To solve this problem, we introduce Ada-Instruct, an adaptive instruction\ngenerator developed by fine-tuning open-source LLMs. Our pivotal finding\nillustrates that fine-tuning open-source LLMs with a mere ten samples generates\nlong instructions that maintain distributional consistency for complex\nreasoning tasks. We empirically validated Ada-Instruct's efficacy across\ndifferent applications, including code completion, mathematical reasoning, and\ncommonsense reasoning. The results underscore Ada-Instruct's superiority,\nevidencing its improvements over its base models, current self-instruct\nmethods, and other state-of-the-art models.",
        "score": 0.8045516014099121
      },
      {
        "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open\n  Resources,",
        "abstract": "In this work we explore recent advances in instruction-tuning language models\non a range of open instruction-following datasets. Despite recent claims that\nopen models can be on par with state-of-the-art proprietary models, these\nclaims are often accompanied by limited evaluation, making it difficult to\ncompare models across the board and determine the utility of various resources.\nWe provide a large set of instruction-tuned models from 6.7B to 65B parameters\nin size, trained on 12 instruction datasets ranging from manually curated\n(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and\nsystematically evaluate them on their factual knowledge, reasoning,\nmultilinguality, coding, and open-ended instruction following abilities through\na collection of automatic, model-based, and human-based metrics. We further\nintroduce T\\\"ulu, our best performing instruction-tuned model suite finetuned\non a combination of high-quality open resources. Our experiments show that\ndifferent instruction-tuning datasets can uncover or enhance specific skills,\nwhile no single dataset (or combination) provides the best performance across\nall evaluations. Interestingly, we find that model and human preference-based\nevaluations fail to reflect differences in model capabilities exposed by\nbenchmark-based evaluations, suggesting the need for the type of systemic\nevaluation performed in this work. Our evaluations show that the best model in\nany given evaluation reaches on average 87% of ChatGPT performance, and 73% of\nGPT-4 performance, suggesting that further investment in building better base\nmodels and instruction-tuning data is required to close the gap. We release our\ninstruction-tuned models, including a fully finetuned 65B T\\\"ulu, along with\nour code, data, and evaluation framework at\nhttps://github.com/allenai/open-instruct to facilitate future research.",
        "score": 0.7195206880569458
      },
      {
        "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning,",
        "abstract": "Pretrained Language Models (LMs) have demonstrated ability to perform\nnumerical reasoning by extrapolating from a few examples in few-shot settings.\nHowever, the extent to which this extrapolation relies on robust reasoning is\nunclear. In this paper, we investigate how well these models reason with terms\nthat are less frequent in the pretraining data. In particular, we examine the\ncorrelations between the model performance on test instances and the frequency\nof terms from those instances in the pretraining data. We measure the strength\nof this correlation for a number of GPT-based language models (pretrained on\nthe Pile dataset) on various numerical deduction tasks (e.g., arithmetic and\nunit conversion). Our results consistently demonstrate that models are more\naccurate on instances whose terms are more prevalent, in some cases above\n$70\\%$ (absolute) more accurate on the top 10\\% frequent terms in comparison to\nthe bottom 10\\%. Overall, although LMs exhibit strong performance at few-shot\nnumerical reasoning tasks, our results raise the question of how much models\nactually generalize beyond pretraining data, and we encourage researchers to\ntake the pretraining data into account when interpreting evaluation results.",
        "score": -0.6457017064094543
      }
    ]
  },
  {
    "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in\n  Large Language Models",
    "abstract": "  This paper identifies a cultural dominance issue within large language models\n(LLMs) due to the predominant use of English data in model training (e.g.,\nChatGPT). LLMs often provide inappropriate English-culture-related answers that\nare not relevant to the expected culture when users ask in non-English\nlanguages. To systematically evaluate the cultural dominance issue, we build a\nbenchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and\nopinions) cultural objects. Empirical results show that the representative GPT\nmodels suffer from the culture dominance problem, where GPT-4 is the most\naffected while text-davinci-003 suffers the least from this problem. Our study\nemphasizes the need to critically examine cultural dominance and ethical\nconsideration in their development and deployment. We show that two\nstraightforward methods in model development (i.e., pretraining on more diverse\ndata) and deployment (e.g., culture-aware prompting) can significantly mitigate\nthe cultural dominance issue in LLMs.\n",
    "related_paper_titles": [],
    "related_paper_abstract": [],
    "entities": [
      "KV",
      "OOD",
      "TSF",
      "Monte Carlo Tree Search",
      "Persian",
      "KGC",
      "PDDL",
      "MCTS",
      "EHRs",
      "ToM",
      "GAI",
      "DMs",
      "MATH",
      "XAI",
      "VAE",
      "Korean",
      "KG",
      "Domain Adaptation",
      "SVM",
      "fMRI",
      "Mind",
      "Robust",
      "NTK",
      "Large Language Models Large Language Models",
      "Autonomous",
      "GUI",
      "RNN",
      "Spanish",
      "Direct Preference",
      "SFT"
    ],
    "retrieved_papers": [
      {
        "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in\n  Large Language Models,",
        "abstract": "This paper identifies a cultural dominance issue within large language models\n(LLMs) due to the predominant use of English data in model training (e.g.,\nChatGPT). LLMs often provide inappropriate English-culture-related answers that\nare not relevant to the expected culture when users ask in non-English\nlanguages. To systematically evaluate the cultural dominance issue, we build a\nbenchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and\nopinions) cultural objects. Empirical results show that the representative GPT\nmodels suffer from the culture dominance problem, where GPT-4 is the most\naffected while text-davinci-003 suffers the least from this problem. Our study\nemphasizes the need to critically examine cultural dominance and ethical\nconsideration in their development and deployment. We show that two\nstraightforward methods in model development (i.e., pretraining on more diverse\ndata) and deployment (e.g., culture-aware prompting) can significantly mitigate\nthe cultural dominance issue in LLMs.",
        "score": 4.623192310333252
      },
      {
        "title": "Cultural Bias and Cultural Alignment of Large Language Models,",
        "abstract": "Culture fundamentally shapes people's reasoning, behavior, and communication.\nAs people increasingly use generative artificial intelligence (AI) to expedite\nand automate personal and professional tasks, cultural values embedded in AI\nmodels may bias people's authentic expression and contribute to the dominance\nof certain cultures. We conduct a disaggregated evaluation of cultural bias for\nfive widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3)\nby comparing the models' responses to nationally representative survey data.\nAll models exhibit cultural values resembling English-speaking and Protestant\nEuropean countries. We test cultural prompting as a control strategy to\nincrease cultural alignment for each country/territory. For recent models\n(GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models'\noutput for 71-81% of countries and territories. We suggest using cultural\nprompting and ongoing evaluation to reduce cultural bias in the output of\ngenerative AI.",
        "score": 1.7474151849746704
      },
      {
        "title": "Cultural Alignment in Large Language Models: An Explanatory Analysis\n  Based on Hofstede's Cultural Dimensions,",
        "abstract": "The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals and\nsocieties with diverse cultural backgrounds. While the discourse has focused\nmainly on political and social biases, our research proposes a Cultural\nAlignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's\ncultural dimension framework, which offers an explanatory cross-cultural\ncomparison through the latent variable analysis. We apply our approach to\nquantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the\ncultural dimensions of regions like the United States, China, and Arab\ncountries, using different prompting styles and exploring the effects of\nlanguage-specific fine-tuning on the models' behavioural tendencies and\ncultural values. Our results quantify the cultural alignment of LLMs and reveal\nthe difference between LLMs in explanatory cultural dimensions. Our study\ndemonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows\na unique capability to adapt to cultural nuances, particularly in Chinese\nsettings. However, it faces challenges with American and Arab cultures. The\nresearch also highlights that fine-tuning LLama 2 models with different\nlanguages changes their responses to cultural questions, emphasizing the need\nfor culturally diverse development in AI for worldwide acceptance and ethical\nuse. For more details or to contribute to this research, visit our GitHub page\nhttps://github.com/reemim/Hofstedes_CAT/",
        "score": 0.9282438158988953
      },
      {
        "title": "The Ghost in the Machine has an American accent: value conflict in GPT-3,",
        "abstract": "The alignment problem in the context of large language models must consider\nthe plurality of human values in our world. Whilst there are many resonant and\noverlapping values amongst the world's cultures, there are also many\nconflicting, yet equally valid, values. It is important to observe which\ncultural values a model exhibits, particularly when there is a value conflict\nbetween input prompts and generated outputs. We discuss how the co-creation of\nlanguage and cultural value impacts large language models (LLMs). We explore\nthe constitution of the training data for GPT-3 and compare that to the world's\nlanguage and internet access demographics, as well as to reported statistical\nprofiles of dominant values in some Nation-states. We stress tested GPT-3 with\na range of value-rich texts representing several languages and nations;\nincluding some with values orthogonal to dominant US public opinion as reported\nby the World Values Survey. We observed when values embedded in the input text\nwere mutated in the generated outputs and noted when these conflicting values\nwere more aligned with reported dominant US values. Our discussion of these\nresults uses a moral value pluralism (MVP) lens to better understand these\nvalue mutations. Finally, we provide recommendations for how our work may\ncontribute to other current work in the field.",
        "score": 0.700184166431427
      },
      {
        "title": "NormAd: A Benchmark for Measuring the Cultural Adaptability of Large\n  Language Models,",
        "abstract": "The integration of large language models (LLMs) into various global cultures\nfundamentally presents a challenge: LLMs must navigate interactions, respect\nsocial norms, and avoid transgressing cultural boundaries. However, it is still\nunclear if LLMs can adapt their outputs to diverse cultural norms. Our study\nfocuses on this aspect. We introduce NormAd, a novel dataset, which includes\n2.6k stories that represent social and cultural norms from 75 countries, to\nassess the ability of LLMs to adapt to different granular levels of\nsocio-cultural contexts such as the country of origin, its associated cultural\nvalues, and prevalent social norms. Our study reveals that LLMs struggle with\ncultural reasoning across all contextual granularities, showing stronger\nadaptability to English-centric cultures over those from the Global South. Even\nwith explicit social norms, the top-performing model, Mistral-7b-Instruct,\nachieves only 81.8% accuracy, lagging behind the 95.6% achieved by humans.\nEvaluation on NormAd further reveals that LLMs struggle to adapt to stories\ninvolving gift-giving across cultures. Due to inherent agreement or sycophancy\nbiases, LLMs find it considerably easier to assess the social acceptability of\nstories that adhere to norms than those that deviate. Our benchmark measures\nthe cultural adaptability (or lack thereof) of LLMs, emphasizing the potential\nto make these technologies more equitable and useful for global audiences. We\nrelease the NormAd dataset and its associated code on GitHub.",
        "score": 0.6234021186828613
      },
      {
        "title": "Nationality Bias in Text Generation,",
        "abstract": "Little attention is placed on analyzing nationality bias in language models,\nespecially when nationality is highly used as a factor in increasing the\nperformance of social NLP models. This paper examines how a text generation\nmodel, GPT-2, accentuates pre-existing societal biases about country-based\ndemonyms. We generate stories using GPT-2 for various nationalities and use\nsensitivity analysis to explore how the number of internet users and the\ncountry's economic status impacts the sentiment of the stories. To reduce the\npropagation of biases through large language models (LLM), we explore the\ndebiasing method of adversarial triggering. Our results show that GPT-2\ndemonstrates significant bias against countries with lower internet users, and\nadversarial triggering effectively reduces the same.",
        "score": 0.5206521153450012
      },
      {
        "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models,",
        "abstract": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.",
        "score": 0.4631425440311432
      },
      {
        "title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of\n  LLM Responses Based on Hofstede Cultural Dimensions,",
        "abstract": "Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs.",
        "score": 0.3526746928691864
      },
      {
        "title": "CDEval: A Benchmark for Measuring the Cultural Dimensions of Large\n  Language Models,",
        "abstract": "As the scaling of Large Language Models (LLMs) has dramatically enhanced\ntheir capabilities, there has been a growing focus on the alignment problem to\nensure their responsible and ethical use. While existing alignment efforts\npredominantly concentrate on universal values such as the HHH principle, the\naspect of culture, which is inherently pluralistic and diverse, has not\nreceived adequate attention. This work introduces a new benchmark, CDEval,\naimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by\nincorporating both GPT-4's automated generation and human verification,\ncovering six cultural dimensions across seven domains. Our comprehensive\nexperiments provide intriguing insights into the culture of mainstream LLMs,\nhighlighting both consistencies and variations across different dimensions and\ndomains. The findings underscore the importance of integrating cultural\nconsiderations in LLM development, particularly for applications in diverse\ncultural settings. Through CDEval, we aim to broaden the horizon of LLM\nalignment research by including cultural dimensions, thus providing a more\nholistic framework for the future development and evaluation of LLMs. This\nbenchmark serves as a valuable resource for cultural studies in LLMs, paving\nthe way for more culturally aware and sensitive models.",
        "score": 0.1001029759645462
      },
      {
        "title": "CultureLLM: Incorporating Cultural Differences into Large Language\n  Models,",
        "abstract": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation.",
        "score": -0.031173449009656906
      },
      {
        "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in\n  Generative LLMs,",
        "abstract": "Generative large language models (LLMs) have been shown to exhibit harmful\nbiases and stereotypes. While safety fine-tuning typically takes place in\nEnglish, if at all, these models are being used by speakers of many different\nlanguages. There is existing evidence that the performance of these models is\ninconsistent across languages and that they discriminate based on demographic\nfactors of the user. Motivated by this, we investigate whether the social\nstereotypes exhibited by LLMs differ as a function of the language used to\nprompt them, while controlling for cultural differences and task accuracy. To\nthis end, we present MBBQ (Multilingual Bias Benchmark for Question-answering),\na carefully curated version of the English BBQ dataset extended to Dutch,\nSpanish, and Turkish, which measures stereotypes commonly held across these\nlanguages. We further complement MBBQ with a parallel control dataset to\nmeasure task performance on the question-answering task independently of bias.\nOur results based on several open-source and proprietary LLMs confirm that some\nnon-English languages suffer from bias more than English, even when controlling\nfor cultural shifts. Moreover, we observe significant cross-lingual differences\nin bias behaviour for all except the most accurate models. With the release of\nMBBQ, we hope to encourage further research on bias in multilingual settings.\nThe dataset and code are available at https://github.com/Veranep/MBBQ.",
        "score": -0.049035511910915375
      },
      {
        "title": "Quite Good, but Not Enough: Nationality Bias in Large Language Models --\n  A Case Study of ChatGPT,",
        "abstract": "While nationality is a pivotal demographic element that enhances the\nperformance of language models, it has received far less scrutiny regarding\ninherent biases. This study investigates nationality bias in ChatGPT (GPT-3.5),\na large language model (LLM) designed for text generation. The research covers\n195 countries, 4 temperature settings, and 3 distinct prompt types, generating\n4,680 discourses about nationality descriptions in Chinese and English.\nAutomated metrics were used to analyze the nationality bias, and expert\nannotators alongside ChatGPT itself evaluated the perceived bias. The results\nshow that ChatGPT's generated discourses are predominantly positive, especially\ncompared to its predecessor, GPT-2. However, when prompted with negative\ninclinations, it occasionally produces negative content. Despite ChatGPT\nconsidering its generated text as neutral, it shows consistent self-awareness\nabout nationality bias when subjected to the same pair-wise comparison\nannotation framework used by human annotators. In conclusion, while ChatGPT's\ngenerated texts seem friendly and positive, they reflect the inherent\nnationality biases in the real world. This bias may vary across different\nlanguage versions of ChatGPT, indicating diverse cultural perspectives. The\nstudy highlights the subtle and pervasive nature of biases within LLMs,\nemphasizing the need for further scrutiny.",
        "score": -0.08363837003707886
      },
      {
        "title": "Understanding the Capabilities and Limitations of Large Language Models\n  for Cultural Commonsense,",
        "abstract": "Large language models (LLMs) have demonstrated substantial commonsense\nunderstanding through numerous benchmark evaluations. However, their\nunderstanding of cultural commonsense remains largely unexamined. In this\npaper, we conduct a comprehensive examination of the capabilities and\nlimitations of several state-of-the-art LLMs in the context of cultural\ncommonsense tasks. Using several general and cultural commonsense benchmarks,\nwe find that (1) LLMs have a significant discrepancy in performance when tested\non culture-specific commonsense knowledge for different cultures; (2) LLMs'\ngeneral commonsense capability is affected by cultural context; and (3) The\nlanguage used to query the LLMs can impact their performance on\ncultural-related tasks. Our study points to the inherent bias in the cultural\nunderstanding of LLMs and provides insights that can help develop culturally\naware language models.",
        "score": -0.17835311591625214
      },
      {
        "title": "Cultural Value Differences of LLMs: Prompt, Language, and Model Size,",
        "abstract": "Our study aims to identify behavior patterns in cultural values exhibited by\nlarge language models (LLMs). The studied variants include question ordering,\nprompting language, and model size. Our experiments reveal that each tested LLM\ncan efficiently behave with different cultural values. More interestingly: (i)\nLLMs exhibit relatively consistent cultural values when presented with prompts\nin a single language. (ii) The prompting language e.g., Chinese or English, can\ninfluence the expression of cultural values. The same question can elicit\ndivergent cultural values when the same LLM is queried in a different language.\n(iii) Differences in sizes of the same model (e.g., Llama2-7B vs 13B vs 70B)\nhave a more significant impact on their demonstrated cultural values than model\ndifferences (e.g., Llama2 vs Mixtral). Our experiments reveal that query\nlanguage and model size of LLM are the main factors resulting in cultural value\ndifferences.",
        "score": -0.19563178718090057
      },
      {
        "title": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting,",
        "abstract": "As the utilization of large language models (LLMs) has proliferated\nworldwide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found in: https://github.com/huihanlhh/Culture-Gen/",
        "score": -0.26713892817497253
      },
      {
        "title": "Assessing Cross-Cultural Alignment between ChatGPT and Human Societies:\n  An Empirical Study,",
        "abstract": "The recent release of ChatGPT has garnered widespread recognition for its\nexceptional ability to generate human-like responses in dialogue. Given its\nusage by users from various nations and its training on a vast multilingual\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\nevaluate its effectiveness in cultural adaptation. In this paper, we\ninvestigate the underlying cultural background of ChatGPT by analyzing its\nresponses to questions designed to quantify human cultural differences. Our\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\nstrong alignment with American culture, but it adapts less effectively to other\ncultural contexts. Furthermore, by using different prompts to probe the model,\nwe show that English prompts reduce the variance in model responses, flattening\nout cultural differences and biasing them towards American culture. This study\nprovides valuable insights into the cultural implications of ChatGPT and\nhighlights the necessity of greater diversity and cultural awareness in\nlanguage technologies.",
        "score": -0.4148207902908325
      },
      {
        "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the\n  Knowledge of Pretrained Language Models,",
        "abstract": "A few benchmarking datasets have been released to evaluate the factual\nknowledge of pretrained language models. These benchmarks (e.g., LAMA, and\nParaRel) are mainly developed in English and later are translated to form new\nmultilingual versions (e.g., mLAMA, and mParaRel). Results on these\nmultilingual benchmarks suggest that using English prompts to recall the facts\nfrom multilingual models usually yields significantly better and more\nconsistent performance than using non-English prompts. Our analysis shows that\nmLAMA is biased toward facts from Western countries, which might affect the\nfairness of probing models. We propose a new framework for curating factual\ntriples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is\nbuilt of factual triples from three pairs of contrasting cultures having a\ntotal of 78,259 triples from 20 relation predicates. The three pairs comprise\nfacts representing the (Arab and Western), (Asian and Western), and (South\nAmerican and Western) countries respectively. Having a more balanced benchmark\n(DLAMA-v1) supports that mBERT performs better on Western facts than\nnon-Western ones, while monolingual Arabic, English, and Korean models tend to\nperform better on their culturally proximate facts. Moreover, both monolingual\nand multilingual models tend to make a prediction that is culturally or\ngeographically relevant to the correct label, even if the prediction is wrong.",
        "score": -0.42007818818092346
      },
      {
        "title": "CaLMQA: Exploring culturally specific long-form question answering\n  across 23 languages,",
        "abstract": "Large language models (LLMs) are used for long-form question answering\n(LFQA), which requires them to generate paragraph-length answers to complex\nquestions. While LFQA has been well-studied in English, this research has not\nbeen extended to other languages. To bridge this gap, we introduce CaLMQA, a\ncollection of 1.5K complex culturally specific questions spanning 23 languages\nand 51 culturally agnostic questions translated from English into 22 other\nlanguages. We define culturally specific questions as those uniquely or more\nlikely to be asked by people from cultures associated with the question's\nlanguage. We collect naturally-occurring questions from community web forums\nand hire native speakers to write questions to cover under-resourced,\nrarely-studied languages such as Fijian and Kirundi. Our dataset contains\ndiverse, complex questions that reflect cultural topics (e.g. traditions, laws,\nnews) and the language usage of native speakers. We automatically evaluate a\nsuite of open- and closed-source models on CaLMQA by detecting incorrect\nlanguage and token repetitions in answers, and observe that the quality of\nLLM-generated answers degrades significantly for some low-resource languages.\nLastly, we perform human evaluation on a subset of models and languages. Manual\nevaluation reveals that model performance is significantly worse for culturally\nspecific questions than for culturally agnostic questions. Our findings\nhighlight the need for further research in non-English LFQA and provide an\nevaluation framework.",
        "score": -0.5959612727165222
      },
      {
        "title": "Extrinsic Evaluation of Cultural Competence in Large Language Models,",
        "abstract": "Productive interactions between diverse users and language technologies\nrequire outputs from the latter to be culturally relevant and sensitive. Prior\nworks have evaluated models' knowledge of cultural norms, values, and\nartifacts, without considering how this knowledge manifests in downstream\napplications. In this work, we focus on extrinsic evaluation of cultural\ncompetence in two text generation tasks, open-ended question answering and\nstory generation. We quantitatively and qualitatively evaluate model outputs\nwhen an explicit cue of culture, specifically nationality, is perturbed in the\nprompts. Although we find that model outputs do vary when varying nationalities\nand feature culturally relevant words, we also find weak correlations between\ntext similarity of outputs for different countries and the cultural values of\nthese countries. Finally, we discuss important considerations in designing\ncomprehensive evaluation of cultural competence in user-facing tasks.",
        "score": -0.6654421091079712
      },
      {
        "title": "GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained\n  Language Models,",
        "abstract": "Recent work has shown that Pre-trained Language Models (PLMs) store the\nrelational knowledge learned from data and utilize it for performing downstream\ntasks. However, commonsense knowledge across different regions may vary. For\ninstance, the color of bridal dress is white in American weddings whereas it is\nred in Chinese weddings. In this paper, we introduce a benchmark dataset,\nGeo-Diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for\nprobing the diversity of the relational knowledge in multilingual PLMs.\nGeoMLAMA contains 3,125 prompts in English, Chinese, Hindi, Persian, and\nSwahili, with a wide coverage of concepts shared by people from American,\nChinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard\nmultilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger\nmultilingual PLMs variants do not necessarily store geo-diverse concepts better\nthan its smaller variant; 2) multilingual PLMs are not intrinsically biased\ntowards knowledge from the Western countries (the United States); 3) the native\nlanguage of a country may not be the best language to probe its knowledge and\n4) a language may better probe knowledge about a non-native country than its\nnative country. Code and data are released at\nhttps://github.com/WadeYin9712/GeoMLAMA.",
        "score": -0.668904721736908
      },
      {
        "title": "Cultural Commonsense Knowledge for Intercultural Dialogues,",
        "abstract": "Despite recent progress, large language models (LLMs) still face the\nchallenge of appropriately reacting to the intricacies of social and cultural\nconventions. This paper presents MANGO, a methodology for distilling\nhigh-accuracy, high-recall assertions of cultural knowledge. We judiciously and\niteratively prompt LLMs for this purpose from two entry points, concepts and\ncultures. Outputs are consolidated via clustering and generative summarization.\nRunning the MANGO method with GPT-3.5 as underlying LLM yields 167K\nhigh-accuracy assertions for 30K concepts and 11K cultures, surpassing prior\nresources by a large margin in quality and size. In an extrinsic evaluation for\nintercultural dialogues, we explore augmenting dialogue systems with cultural\nknowledge assertions. Notably, despite LLMs inherently possessing cultural\nknowledge, we find that adding knowledge from MANGO improves the overall\nquality, specificity, and cultural sensitivity of dialogue responses, as judged\nby human annotators. Data and code are available for download.",
        "score": -0.7919825315475464
      },
      {
        "title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in\n  Indonesian and Sundanese,",
        "abstract": "Large Language Models (LLMs) are increasingly being used to generate\nsynthetic data for training and evaluating models. However, it is unclear\nwhether they can generate a good quality of question answering (QA) dataset\nthat incorporates knowledge and cultural nuance embedded in a language,\nespecially for low-resource languages. In this study, we investigate the\neffectiveness of using LLMs in generating culturally relevant commonsense QA\ndatasets for Indonesian and Sundanese languages. To do so, we create datasets\nfor these languages using various methods involving both LLMs and human\nannotators, resulting in ~4.5K questions per language (~9K in total), making\nour dataset the largest of its kind. Our experiments show that automatic data\nadaptation from an existing English dataset is less effective for Sundanese.\nInterestingly, using the direct generation method on the target language, GPT-4\nTurbo can generate questions with adequate general knowledge in both languages,\nalbeit not as culturally 'deep' as humans. We also observe a higher occurrence\nof fluency errors in the Sundanese dataset, highlighting the discrepancy\nbetween medium- and lower-resource languages.",
        "score": -0.8324055671691895
      },
      {
        "title": "CIVICS: Building a Dataset for Examining Culturally-Informed Values in\n  Large Language Models,",
        "abstract": "This paper introduces the \"CIVICS: Culturally-Informed & Values-Inclusive\nCorpus for Societal impacts\" dataset, designed to evaluate the social and\ncultural variation of Large Language Models (LLMs) across multiple languages\nand value-sensitive topics. We create a hand-crafted, multilingual dataset of\nvalue-laden prompts which address specific socially sensitive topics, including\nLGBTQI rights, social welfare, immigration, disability rights, and surrogacy.\nCIVICS is designed to generate responses showing LLMs' encoded and implicit\nvalues. Through our dynamic annotation processes, tailored prompt design, and\nexperiments, we investigate how open-weight LLMs respond to value-sensitive\nissues, exploring their behavior across diverse linguistic and cultural\ncontexts. Using two experimental set-ups based on log-probabilities and\nlong-form responses, we show social and cultural variability across different\nLLMs. Specifically, experiments involving long-form responses demonstrate that\nrefusals are triggered disparately across models, but consistently and more\nfrequently in English or translated statements. Moreover, specific topics and\nsources lead to more pronounced differences across model answers, particularly\non immigration, LGBTQI rights, and social welfare. As shown by our experiments,\nthe CIVICS dataset aims to serve as a tool for future research, promoting\nreproducibility and transparency across broader linguistic settings, and\nfurthering the development of AI technologies that respect and reflect global\ncultural diversities and value pluralism. The CIVICS dataset and tools will be\nmade available upon publication under open licenses; an anonymized version is\ncurrently available at https://huggingface.co/CIVICS-dataset.",
        "score": -0.8451995253562927
      },
      {
        "title": "WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural\n  Value Awareness of Language Models,",
        "abstract": "The awareness of multi-cultural human values is critical to the ability of\nlanguage models (LMs) to generate safe and personalized responses. However,\nthis awareness of LMs has been insufficiently studied, since the computer\nscience community lacks access to the large-scale real-world data about\nmulti-cultural values. In this paper, we present WorldValuesBench, a globally\ndiverse, large-scale benchmark dataset for the multi-cultural value prediction\ntask, which requires a model to generate a rating response to a value question\nbased on demographic contexts. Our dataset is derived from an influential\nsocial science project, World Values Survey (WVS), that has collected answers\nto hundreds of value questions (e.g., social, economic, ethical) from 94,728\nparticipants worldwide. We have constructed more than 20 million examples of\nthe type \"(demographic attributes, value question) $\\rightarrow$ answer\" from\nthe WVS responses. We perform a case study using our dataset and show that the\ntask is challenging for strong open and closed-source models. On merely\n$11.1\\%$, $25.0\\%$, $72.2\\%$, and $75.0\\%$ of the questions, Alpaca-7B,\nVicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively\nachieve $<0.2$ Wasserstein 1-distance from the human normalized answer\ndistributions. WorldValuesBench opens up new research avenues in studying\nlimitations and opportunities in multi-cultural value awareness of LMs.",
        "score": -1.0670263767242432
      },
      {
        "title": "Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models,",
        "abstract": "As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel",
        "score": -1.3242326974868774
      },
      {
        "title": "Are Generative Language Models Multicultural? A Study on Hausa Culture\n  and Emotions using ChatGPT,",
        "abstract": "Large Language Models (LLMs), such as ChatGPT, are widely used to generate\ncontent for various purposes and audiences. However, these models may not\nreflect the cultural and emotional diversity of their users, especially for\nlow-resource languages. In this paper, we investigate how ChatGPT represents\nHausa's culture and emotions. We compare responses generated by ChatGPT with\nthose provided by native Hausa speakers on 37 culturally relevant questions. We\nconducted experiments using emotion analysis and applied two similarity metrics\nto measure the alignment between human and ChatGPT responses. We also collected\nhuman participants ratings and feedback on ChatGPT responses. Our results show\nthat ChatGPT has some level of similarity to human responses, but also exhibits\nsome gaps and biases in its knowledge and awareness of the Hausa culture and\nemotions. We discuss the implications and limitations of our methodology and\nanalysis and suggest ways to improve the performance and evaluation of LLMs for\nlow-resource languages.",
        "score": -1.3428969383239746
      },
      {
        "title": "SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes,",
        "abstract": "While generative multilingual models are rapidly being deployed, their safety\nand fairness evaluations are largely limited to resources collected in English.\nThis is especially problematic for evaluations targeting inherently\nsocio-cultural phenomena such as stereotyping, where it is important to build\nmulti-lingual resources that reflect the stereotypes prevalent in respective\nlanguage communities. However, gathering these resources, at scale, in varied\nlanguages and regions pose a significant challenge as it requires broad\nsocio-cultural knowledge and can also be prohibitively expensive. To overcome\nthis critical gap, we employ a recently introduced approach that couples LLM\ngenerations for scale with culturally situated validations for reliability, and\nbuild SeeGULL Multilingual, a global-scale multilingual dataset of social\nstereotypes, containing over 25K stereotypes, spanning 20 languages, with human\nannotations across 23 regions, and demonstrate its utility in identifying gaps\nin model evaluations. Content warning: Stereotypes shared in this paper can be\noffensive.",
        "score": -1.4055358171463013
      },
      {
        "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey,",
        "abstract": "We present a survey of more than 90 recent papers that aim to study cultural\nrepresentation and inclusion in large language models (LLMs). We observe that\nnone of the studies explicitly define \"culture, which is a complex,\nmultifaceted concept; instead, they probe the models on some specially designed\ndatasets which represent certain aspects of \"culture\". We call these aspects\nthe proxies of culture, and organize them across two dimensions of demographic\nand semantic proxies. We also categorize the probing methods employed. Our\nanalysis indicates that only certain aspects of ``culture,'' such as values and\nobjectives, have been studied, leaving several other interesting and important\nfacets, especially the multitude of semantic domains (Thompson et al., 2020)\nand aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps\nare the lack of robustness of probing techniques and situated studies on the\nimpact of cultural mis- and under-representation in LLM-based applications.",
        "score": -1.4357433319091797
      },
      {
        "title": "CultureBank: An Online Community-Driven Knowledge Base Towards\n  Culturally Aware Language Technologies,",
        "abstract": "To enhance language models' cultural awareness, we design a generalizable\npipeline to construct cultural knowledge bases from different online\ncommunities on a massive scale. With the pipeline, we construct CultureBank, a\nknowledge base built upon users' self-narratives with 12K cultural descriptors\nsourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge\nresources, CultureBank contains diverse views on cultural descriptors to allow\nflexible interpretation of cultural knowledge, and contextualized cultural\nscenarios to help grounded evaluation. With CultureBank, we evaluate different\nLLMs' cultural awareness, and identify areas for improvement. We also fine-tune\na language model on CultureBank: experiments show that it achieves better\nperformances on two downstream cultural tasks in a zero-shot setting. Finally,\nwe offer recommendations based on our findings for future culturally aware\nlanguage technologies. The project page is https://culturebank.github.io . The\ncode and model is at https://github.com/SALT-NLP/CultureBank . The released\nCultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .",
        "score": -1.4451109170913696
      },
      {
        "title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in\n  Values,",
        "abstract": "Language embeds information about social, cultural, and political values\npeople hold. Prior work has explored social and potentially harmful biases\nencoded in Pre-Trained Language models (PTLMs). However, there has been no\nsystematic study investigating how values embedded in these models vary across\ncultures. In this paper, we introduce probes to study which values across\ncultures are embedded in these models, and whether they align with existing\ntheories and cross-cultural value surveys. We find that PTLMs capture\ndifferences in values across cultures, but those only weakly align with\nestablished value surveys. We discuss implications of using mis-aligned models\nin cross-cultural settings, as well as ways of aligning PTLMs with value\nsurveys.",
        "score": -1.505454421043396
      }
    ]
  },
  {
    "title": "Black-Box Prompt Optimization: Aligning Large Language Models without\n  Model Training",
    "abstract": "  Large language models (LLMs) have shown impressive success in various\napplications. However, these models are often not well aligned with human\nintents, which calls for additional treatments on them; that is, the alignment\nproblem. To make LLMs better follow user instructions, existing alignment\nmethods primarily focus on further training them. However, the extra training\nof LLMs is usually expensive in terms of GPU computing; even worse, some LLMs\nare not accessible for user-demanded training, such as GPTs. In this work, we\ntake a different perspective -- Black-Box Prompt Optimization (BPO) -- to\nperform alignments. The idea is to optimize user prompts to suit LLMs' input\nunderstanding, so as to best realize users' intents without updating LLMs'\nparameters. BPO leverages human preferences to optimize prompts, thus making it\nsuperior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is\nmodel-agnostic, and the empirical results demonstrate that the BPO-aligned\nChatGPT yields a 22% increase in the win rate against its original version and\n10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models\naligned by PPO and DPO, and it also brings additional performance gains when\ncombining BPO with PPO or DPO. Code and datasets are released at\nhttps://github.com/thu-coai/BPO.\n",
    "related_paper_titles": [
      "Large Language Models as Optimizers",
      "Large Language Models Are Human-Level Prompt Engineers",
      "Plum: Prompt Learning using Metaheuristic"
    ],
    "related_paper_abstract": [
      "  Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to our main application in prompt optimization,\nwhere the goal is to find instructions that maximize the task accuracy. With a\nvariety of LLMs, we demonstrate that the best prompts optimized by OPRO\noutperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on\nBig-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.\n",
      "  By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.\n",
      "  Since the emergence of large language models, prompt learning has become a\npopular method for optimizing and customizing these models. Special prompts,\nsuch as Chain-of-Thought, have even revealed previously unknown reasoning\ncapabilities within these models. However, the progress of discovering\neffective prompts has been slow, driving a desire for general prompt\noptimization methods. Unfortunately, few existing prompt learning methods\nsatisfy the criteria of being truly \"general\", i.e., automatic, discrete,\nblack-box, gradient-free, and interpretable all at once. In this paper, we\nintroduce metaheuristics, a branch of discrete non-convex optimization methods\nwith over 100 options, as a promising approach to prompt learning. Within our\nparadigm, we test six typical methods: hill climbing, simulated annealing,\ngenetic algorithms with/without crossover, tabu search, and harmony search,\ndemonstrating their effectiveness in white-box and black-box prompt learning.\nFurthermore, we show that these methods can be used to discover more\nhuman-understandable prompts that were previously unknown in both reasoning and\nimage generation tasks, opening the door to a cornucopia of possibilities in\nprompt optimization. We release all the codes in\n\\url{https://github.com/research4pan/Plum}.\n"
    ],
    "entities": [
      "Adam",
      "MTS",
      "Persian",
      "NeRF",
      "KAN",
      "DTs",
      "SVM",
      "GCN",
      "YOLO",
      "ODE",
      "ResNet",
      "Neural Networks",
      "ESG",
      "Networks",
      "WSIs",
      "CAD",
      "Deep Reinforcement Learning",
      "ToM",
      "KGE",
      "FID",
      "ANN",
      "Mind",
      "IFT",
      "MMLU",
      "LiDAR",
      "PDE",
      "DNN",
      "KV",
      "PDEs",
      "Wikipedia"
    ],
    "retrieved_papers": [
      {
        "title": "Black-Box Prompt Optimization: Aligning Large Language Models without\n  Model Training,",
        "abstract": "Large language models (LLMs) have shown impressive success in various\napplications. However, these models are often not well aligned with human\nintents, which calls for additional treatments on them; that is, the alignment\nproblem. To make LLMs better follow user instructions, existing alignment\nmethods primarily focus on further training them. However, the extra training\nof LLMs is usually expensive in terms of GPU computing; even worse, some LLMs\nare not accessible for user-demanded training, such as GPTs. In this work, we\ntake a different perspective -- Black-Box Prompt Optimization (BPO) -- to\nperform alignments. The idea is to optimize user prompts to suit LLMs' input\nunderstanding, so as to best realize users' intents without updating LLMs'\nparameters. BPO leverages human preferences to optimize prompts, thus making it\nsuperior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is\nmodel-agnostic, and the empirical results demonstrate that the BPO-aligned\nChatGPT yields a 22% increase in the win rate against its original version and\n10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models\naligned by PPO and DPO, and it also brings additional performance gains when\ncombining BPO with PPO or DPO. Code and datasets are released at\nhttps://github.com/thu-coai/BPO.",
        "score": 6.731343746185303
      },
      {
        "title": "CycleAlign: Iterative Distillation from Black-box LLM to White-box\n  Models for Better Human Alignment,",
        "abstract": "Language models trained on large-scale corpus often generate content that is\nharmful, toxic, or contrary to human preferences, making their alignment with\nhuman values a critical concern. Reinforcement learning from human feedback\n(RLHF) with algorithms like PPO is a prevalent approach for alignment but is\noften complex, unstable, and resource-intensive. Recently, ranking-based\nalignment methods have emerged, offering stability and effectiveness by\nreplacing the RL framework with supervised fine-tuning, but they are costly due\nto the need for annotated data. Considering that existing large language models\n(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,\nresearchers have begun to align the language model with human preference from\nAI feedback. The common practices, which unidirectionally distill the\ninstruction-following responses from LLMs, are constrained by their bottleneck.\nThus we introduce CycleAlign to distill alignment capabilities from\nparameter-invisible LLMs (black-box) to a parameter-visible model (white-box)\nin an iterative manner. With in-context learning (ICL) as the core of the\ncycle, the black-box models are able to rank the model-generated responses\nguided by human-craft instruction and demonstrations about their preferences.\nDuring iterative interaction, the white-box models also have a judgment about\nresponses generated by them. Consequently, the agreement ranking could be\nviewed as a pseudo label to dynamically update the in-context demonstrations\nand improve the preference ranking ability of black-box models. Through\nmultiple interactions, the CycleAlign framework could align the white-box model\nwith the black-box model effectively in a low-resource way. Empirical results\nillustrate that the model fine-tuned by CycleAlign remarkably exceeds existing\nmethods, and achieves the state-of-the-art performance in alignment with human\nvalue.",
        "score": 4.438464164733887
      },
      {
        "title": "The Unreasonable Effectiveness of Eccentric Automatic Prompts,",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable problem-solving and\nbasic mathematics abilities. However, their efficacy is highly contingent on\nthe formulation of the prompt. This study endeavors to quantify the influence\nof incorporating \"positive thinking\" into the system message of the prompt,\nthen compare that to systematic prompt optimization. We assess the performance\nof 60 combinations of system message snippets, tested with and without Chain of\nThought prompting, across three models with parameters ranging from 7 to 70\nbillion on the GSM8K dataset. Our findings reveal that results do not\nuniversally generalize across models. In most instances, the inclusion of\n\"positive thinking\" prompts positively affected model performance. Notably,\nhowever, Llama2-70B exhibited an exception when not utilizing Chain of Thought,\nas the optimal system message was found to be none at all. Given the\ncombinatorial complexity, and thus computation time, of experimenting with\nhand-tuning prompts for large black-box models, we then compared the\nperformance of the best \"positive thinking\" prompt against the output of\nsystematic prompt optimization. We show that employing an automated prompt\noptimizer emerges as the most effective method for enhancing performance, even\nwhen working with smaller open-source models. Additionally, our findings reveal\nthat the highest-scoring, automatically-optimized prompt exhibits a degree of\npeculiarity far beyond expectations.",
        "score": 3.9139211177825928
      },
      {
        "title": "Survival of the Most Influential Prompts: Efficient Black-Box Prompt\n  Search via Clustering and Pruning,",
        "abstract": "Prompt-based learning has been an effective paradigm for large pretrained\nlanguage models (LLM), enabling few-shot or even zero-shot learning. Black-box\nprompt search has received growing interest recently for its distinctive\nproperties of gradient-free optimization, proven particularly useful and\npowerful for model-as-a-service usage. However, the discrete nature and the\ncomplexity of combinatorial optimization hinder the efficiency of modern\nblack-box approaches. Despite extensive research on search algorithms, the\ncrucial aspect of search space design and optimization has been largely\noverlooked. In this paper, we first conduct a sensitivity analysis by prompting\nLLM, revealing that only a small number of tokens exert a disproportionate\namount of influence on LLM predictions. Leveraging this insight, we propose the\nClustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple\nblack-box search method that first clusters and prunes the search space to\nfocus exclusively on influential prompt tokens. By employing even simple search\nmethods within the pruned search space, ClaPS achieves state-of-the-art\nperformance across various tasks and LLMs, surpassing the performance of\ncomplex approaches while significantly reducing search costs. Our findings\nunderscore the critical role of search space design and optimization in\nenhancing both the usefulness and the efficiency of black-box prompt-based\nlearning.",
        "score": 3.766556739807129
      },
      {
        "title": "Deconstructing In-Context Learning: Understanding Prompts via Corruption,",
        "abstract": "The ability of large language models (LLMs) to $``$learn in context$\"$ based\non the provided prompt has led to an explosive growth in their use, culminating\nin the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These\nAI assistants are known to be robust to minor prompt modifications, mostly due\nto alignment techniques that use human feedback. In contrast, the underlying\npre-trained LLMs they use as a backbone are known to be brittle in this\nrespect. Building high-quality backbone models remains a core challenge, and a\ncommon approach to assessing their quality is to conduct few-shot evaluation.\nSuch evaluation is notorious for being highly sensitive to minor prompt\nmodifications, as well as the choice of specific in-context examples. Prior\nwork has examined how modifying different elements of the prompt can affect\nmodel performance. However, these earlier studies tended to concentrate on a\nlimited number of specific prompt attributes and often produced contradictory\nresults. Additionally, previous research either focused on models with fewer\nthan 15 billion parameters or exclusively examined black-box models like GPT-3\nor PaLM, making replication challenging. In the present study, we decompose the\nentire prompt into four components: task description, demonstration inputs,\nlabels, and inline instructions provided for each demonstration. We investigate\nthe effects of structural and semantic corruptions of these elements on model\nperformance. We study models ranging from 1.5B to 70B in size, using ten\ndatasets covering classification and generation tasks. We find that repeating\ntext within the prompt boosts model performance, and bigger models ($\\geq$30B)\nare more sensitive to the semantics of the prompt. Finally, we observe that\nadding task and inline instructions to the demonstrations enhances model\nperformance even when the instructions are semantically corrupted.",
        "score": 3.6139283180236816
      },
      {
        "title": "Black-Box Tuning for Language-Model-as-a-Service,",
        "abstract": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually\nreleased as a service. It allows users to design task-specific prompts to query\nthe PTMs through some black-box APIs. In such a scenario, which we call\nLanguage-Model-as-a-Service (LMaaS), the gradients of PTMs are usually\nunavailable. Can we optimize the task prompts by only accessing the model\ninference APIs? This paper proposes the black-box tuning framework to optimize\nthe continuous prompt prepended to the input text via derivative-free\noptimization. Instead of optimizing in the original high-dimensional prompt\nspace, which is intractable for traditional derivative-free optimization, we\nperform optimization in a randomly generated subspace due to the low intrinsic\ndimensionality of large PTMs. The experimental results show that the black-box\ntuning with RoBERTa on a few labeled samples not only significantly outperforms\nmanual prompt and GPT-3's in-context learning, but also surpasses the\ngradient-based counterparts, i.e., prompt tuning and full model tuning.",
        "score": 3.2959203720092773
      },
      {
        "title": "MAPO: Boosting Large Language Model Performance with Model-Adaptive\n  Prompt Optimization,",
        "abstract": "Prompt engineering, as an efficient and effective way to leverage Large\nLanguage Models (LLM), has drawn a lot of attention from the research\ncommunity. The existing research primarily emphasizes the importance of\nadapting prompts to specific tasks, rather than specific LLMs. However, a good\nprompt is not solely defined by its wording, but also binds to the nature of\nthe LLM in question. In this work, we first quantitatively demonstrate that\ndifferent prompts should be adapted to different LLMs to enhance their\ncapabilities across various downstream tasks in NLP. Then we novelly propose a\nmodel-adaptive prompt optimizer (MAPO) method that optimizes the original\nprompts for each specific LLM in downstream tasks. Extensive experiments\nindicate that the proposed method can effectively refine prompts for an LLM,\nleading to significant improvements over various downstream tasks.",
        "score": 3.190732955932617
      },
      {
        "title": "Training language models to follow instructions with human feedback,",
        "abstract": "Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.",
        "score": 3.1634795665740967
      },
      {
        "title": "DeAL: Decoding-time Alignment for Large Language Models,",
        "abstract": "Large Language Models (LLMs) are nowadays expected to generate content\naligned with human preferences. Current work focuses on alignment at model\ntraining time, through techniques such as Reinforcement Learning with Human\nFeedback (RLHF). However, it is unclear if such methods are an effective choice\nto teach alignment objectives to the model. First, the inability to incorporate\nmultiple, custom rewards and reliance on a model developer's view of universal\nand static principles are key limitations. Second, the residual gaps in model\ntraining and the reliability of such approaches are also questionable (e.g.\nsusceptibility to jail-breaking even after safety training). To address these,\nwe propose DeAL, a framework that allows the user to customize reward functions\nand enables Decoding-time Alignment of LLMs (DeAL). At its core, we view\ndecoding as a heuristic-guided search process and facilitate the use of a wide\nvariety of alignment objectives. Our experiments with programmatic constraints\nsuch as keyword and length constraints (studied widely in the pre-LLM era) and\nabstract objectives such as harmlessness and helpfulness (proposed in the\npost-LLM era) show that we can DeAL with fine-grained trade-offs, improve\nadherence to alignment objectives, and address residual gaps in LLMs. Lastly,\nwhile DeAL can be effectively paired with RLHF and prompting techniques, its\ngenerality makes decoding slower, an optimization we leave for future work.",
        "score": 3.1402230262756348
      },
      {
        "title": "Aligning to Thousands of Preferences via System Message Generalization,",
        "abstract": "Although humans inherently have diverse values, current large language model\n(LLM) alignment methods often assume that aligning LLMs with the general\npublic's preferences is optimal. A major challenge in adopting a more\nindividualized approach to LLM alignment is its lack of scalability, as it\ninvolves repeatedly acquiring preference data and training new reward models\nand LLMs for each individual's preferences. To address these challenges, we\npropose a new paradigm where users specify what they value most within the\nsystem message, steering the LLM's generation behavior to better align with the\nuser's intentions. However, a naive application of such an approach is\nnon-trivial since LLMs are typically trained on a uniform system message (e.g.,\n\"You are a helpful assistant\") which limits their ability to generalize to\ndiverse, unseen system messages. To improve this generalization, we create the\nMultifaceted Collection, a preference dataset with 192k combinations of values\nbeyond generic helpfulness and harmlessness, spanning 65k user instructions.\nUsing this dataset, we train a 7B LLM called Janus and test it on 921 prompts\nfrom 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\nby adding various unseen system messages that reflect user preferences. Janus\nachieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct\nv0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks\nfocused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto\nv0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%\nmargin, underscoring that training with a vast array of system messages could\nalso enhance alignment to the general public's preference as well. Our code,\ndataset, benchmark, and models are available at\nhttps://github.com/kaistAI/Janus.",
        "score": 3.128261089324951
      },
      {
        "title": "Are Large Language Models Good Prompt Optimizers?,",
        "abstract": "LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as\nPrompt Optimizers to self-reflect and refine prompts, has shown promising\nperformance in recent studies. Despite the success, the underlying mechanism of\nthis approach remains unexplored, and the true effectiveness of LLMs as Prompt\nOptimizers requires further validation. In this work, we conducted a\ncomprehensive study to uncover the actual mechanism of LLM-based Prompt\nOptimization. Our findings reveal that the LLM optimizers struggle to identify\nthe true causes of errors during reflection, tending to be biased by their own\nprior knowledge rather than genuinely reflecting on the errors. Furthermore,\neven when the reflection is semantically valid, the LLM optimizers often fail\nto generate appropriate prompts for the target models with a single prompt\nrefinement step, partly due to the unpredictable behaviors of the target\nmodels. Based on the observations, we introduce a new \"Automatic Behavior\nOptimization\" paradigm, which directly optimizes the target model's behavior in\na more controllable manner. We hope our study can inspire new directions for\nautomatic prompt optimization development.",
        "score": 3.0573675632476807
      },
      {
        "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema,",
        "abstract": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and three testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project.",
        "score": 2.9457666873931885
      },
      {
        "title": "Zephyr: Direct Distillation of LM Alignment,",
        "abstract": "We aim to produce a smaller language model that is aligned to user intent.\nPrevious research has shown that applying distilled supervised fine-tuning\n(dSFT) on larger models significantly improves task accuracy; however, these\nmodels are unaligned, i.e. they do not respond well to natural prompts. To\ndistill this property, we experiment with the use of preference data from AI\nFeedback (AIF). Starting from a dataset of outputs ranked by a teacher model,\nwe apply distilled direct preference optimization (dDPO) to learn a chat model\nwith significantly improved intent alignment. The approach requires only a few\nhours of training without any additional sampling during fine-tuning. The final\nresult, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B\nparameter models, and requires no human annotation. In particular, results on\nMT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access\nRLHF-based model. Code, models, data, and tutorials for the system are\navailable at https://github.com/huggingface/alignment-handbook.",
        "score": 2.855881690979004
      },
      {
        "title": "Unleashing the Potential of Large Language Models as Prompt Optimizers:\n  An Analogical Analysis with Gradient-based Model Optimizers,",
        "abstract": "Automatic prompt optimization is an important approach to improving the\nperformance of large language models (LLMs). Recent research demonstrates the\npotential of using LLMs as prompt optimizers, which can generate improved task\nprompts via iterative refinement. In this paper, we propose a novel perspective\nto investigate the design of LLM-based prompt optimizers, by drawing an analogy\nwith gradient-based model optimizers. To connect these two approaches, we\nidentify two pivotal factors in model parameter learning: update direction and\nupdate method. Focused on the two aspects, we borrow the theoretical framework\nand learning methods from gradient-based optimization to design improved\nstrategies for LLM-based prompt optimizers. By systematically analyzing a rich\nset of improvement strategies, we further develop a capable Gradient-inspired\nLLM-based Prompt Optimizer called GPO. At each step, it first retrieves\nrelevant prompts from the optimization trajectory as the update direction.\nThen, it utilizes the generation-based refinement strategy to perform the\nupdate, while controlling the edit distance through a cosine-based decay\nstrategy. Extensive experiments demonstrate the effectiveness and efficiency of\nGPO. In particular, GPO brings an additional improvement of up to 56.8% on\nBig-Bench Hard and 55.3% on MMLU compared to baseline methods.",
        "score": 2.8513214588165283
      },
      {
        "title": "Connecting Large Language Models with Evolutionary Algorithms Yields\n  Powerful Prompt Optimizers,",
        "abstract": "Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.",
        "score": 2.62815260887146
      },
      {
        "title": "Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers,",
        "abstract": "Numerous recent works aim to enhance the efficacy of Large Language Models\n(LLMs) through strategic prompting. In particular, the Optimization by\nPROmpting (OPRO) approach provides state-of-the-art performance by leveraging\nLLMs as optimizers where the optimization task is to find instructions that\nmaximize the task accuracy. In this paper, we revisit OPRO for automated\nprompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral\n7B. Our investigation reveals that OPRO shows limited effectiveness in\nsmall-scale LLMs, with limited inference capabilities constraining optimization\nability. We suggest future automatic prompting engineering to consider both\nmodel capabilities and computational costs. Additionally, for small-scale LLMs,\nwe recommend direct instructions that clearly outline objectives and\nmethodologies as robust prompt baselines, ensuring efficient and effective\nprompt engineering in ongoing research.",
        "score": 2.522742748260498
      },
      {
        "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language\n  Models Agents,",
        "abstract": "In this past year, large language models (LLMs) have had remarkable success\nin domains outside the traditional natural language processing, and people are\nstarting to explore the usage of LLMs in more general and close to application\ndomains like code generation, travel planning, and robot controls. Connecting\nthese LLMs with great capacity and external tools, people are building the\nso-called LLM agents, which are supposed to help people do all kinds of work in\neveryday life. In all these domains, the prompt to the LLMs has been shown to\nmake a big difference in what the LLM would generate and thus affect the\nperformance of the LLM agents. Therefore, automatic prompt engineering has\nbecome an important question for many researchers and users of LLMs. In this\npaper, we propose a novel method, \\textsc{RePrompt}, which does \"gradient\ndescent\" to optimize the step-by-step instructions in the prompt of the LLM\nagents based on the chat history obtained from interactions with LLM agents. By\noptimizing the prompt, the LLM will learn how to plan in specific domains. We\nhave used experiments in PDDL generation and travel planning to show that our\nmethod could generally improve the performance for different reasoning tasks\nwhen using the updated prompt as the initial prompt.",
        "score": 2.4245986938476562
      },
      {
        "title": "From Tarzan to Tolkien: Controlling the Language Proficiency Level of\n  LLMs for Content Generation,",
        "abstract": "We study the problem of controlling the difficulty level of text generated by\nLarge Language Models (LLMs) for contexts where end-users are not fully\nproficient, such as language learners. Using a novel framework, we evaluate the\neffectiveness of several key approaches for this task, including few-shot\nprompting, supervised finetuning, and reinforcement learning (RL), utilising\nboth GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.\n  Our findings reveal a large performance gap between GPT-4 and the open source\nmodels when using prompt-based strategies. However, we show how to bridge this\ngap with a careful combination of finetuning and RL alignment. Our best model,\nCALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and\nother strategies, at only a fraction of the cost. We further validate the\nquality of our results through a small-scale human study.",
        "score": 2.287180185317993
      },
      {
        "title": "Xwin-LM: Strong and Scalable Alignment Practice for LLMs,",
        "abstract": "In this work, we present Xwin-LM, a comprehensive suite of alignment\nmethodologies for large language models (LLMs). This suite encompasses several\nkey techniques, including supervised finetuning (SFT), reward modeling (RM),\nrejection sampling finetuning (RS), and direct preference optimization (DPO).\nThe key components are as follows: (1) Xwin-LM-SFT, models initially finetuned\nwith high-quality instruction data; (2) Xwin-Pair, a large-scale, multi-turn\npreference dataset meticulously annotated using GPT-4; (3) Xwin-RM, reward\nmodels trained on Xwin-Pair, developed at scales of 7B, 13B, and 70B\nparameters; (4) Xwin-Set, a multiwise preference dataset in which each prompt\nis linked to 64 unique responses generated by Xwin-LM-SFT and scored by\nXwin-RM; (5) Xwin-LM-RS, models finetuned with the highest-scoring responses\nfrom Xwin-Set; (6) Xwin-LM-DPO, models further optimized on Xwin-Set using the\nDPO algorithm. Our evaluations on AlpacaEval and MT-bench demonstrate\nconsistent and significant improvements across the pipeline, demonstrating the\nstrength and scalability of Xwin-LM. The repository\nhttps://github.com/Xwin-LM/Xwin-LM will be continually updated to foster\ncommunity research.",
        "score": 2.100334405899048
      },
      {
        "title": "Intent-based Prompt Calibration: Enhancing prompt optimization with\n  synthetic boundary cases,",
        "abstract": "Prompt engineering is a challenging and important task due to the high\nsensitivity of Large Language Models (LLMs) to the given prompt and the\ninherent ambiguity of a textual task instruction. Automatic prompt engineering\nis essential to achieve optimized performance from LLMs. Recent studies have\ndemonstrated the capabilities of LLMs to automatically conduct prompt\nengineering by employing a meta-prompt that incorporates the outcomes of the\nlast trials and proposes an improved prompt. However, this requires a\nhigh-quality benchmark to compare different prompts, which is difficult and\nexpensive to acquire in many real-world use cases. In this work, we introduce a\nnew method for automatic prompt engineering, using a calibration process that\niteratively refines the prompt to the user intent. During the optimization\nprocess, the system jointly generates synthetic data of boundary use cases and\noptimizes the prompt according to the generated dataset. We demonstrate the\neffectiveness of our method with respect to strong proprietary models on\nreal-world tasks such as moderation and generation. Our method outperforms\nstate-of-the-art methods with a limited number of annotated samples.\nFurthermore, we validate the advantages of each one of the system's key\ncomponents. Our system is built in a modular way, facilitating easy adaptation\nto other tasks. The code is available\n$\\href{https://github.com/Eladlev/AutoPrompt}{here}$.",
        "score": 2.0223629474639893
      },
      {
        "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling,",
        "abstract": "Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PROMST that incorporates human-designed feedback rules\nto automatically offer direct suggestions for improvement. We also use an extra\nlearned heuristic model that predicts prompt performance to efficiently sample\nfrom prompt candidates. This approach significantly outperforms both\nhuman-engineered prompts and several other prompt optimization methods across\n11 representative multi-step tasks (an average 10.6\\%-29.3\\% improvement to\ncurrent best methods on five LLMs respectively). We believe our work can serve\nas a benchmark for automatic prompt optimization for LLM-driven multi-step\ntasks. Datasets and Codes are available at\nhttps://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST/.",
        "score": 1.7678135633468628
      },
      {
        "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt\n  Templates,",
        "abstract": "Public LLMs such as the Llama 2-Chat have driven huge activity in LLM\nresearch. These models underwent alignment training and were considered safe.\nRecently Qi et al. (2023) reported that even benign fine-tuning (e.g., on\nseemingly safe datasets) can give rise to unsafe behaviors in the models. The\ncurrent paper is about methods and best practices to mitigate such loss of\nalignment. Through extensive experiments on several chat models (Meta's Llama\n2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo),\nthis paper uncovers that the prompt templates used during fine-tuning and\ninference play a crucial role in preserving safety alignment, and proposes the\n\"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a\nsafety prompt, but include it at test time. Fine-tuning experiments on GSM8K,\nChatDoctor, and OpenOrca show that PTST significantly reduces the rise of\nunsafe behaviors, and even almost eliminates them in some cases.",
        "score": 1.6248230934143066
      },
      {
        "title": "DynaMaR: Dynamic Prompt with Mask Token Representation,",
        "abstract": "Recent research has shown that large language models pretrained using\nunsupervised approaches can achieve significant performance improvement on many\ndownstream tasks. Typically when adapting these language models to downstream\ntasks, like a classification or regression task, we employ a fine-tuning\nparadigm in which the sentence representation from the language model is input\nto a task-specific head; the model is then fine-tuned end-to-end. However, with\nthe emergence of models like GPT-3, prompt-based fine-tuning has been proven to\nbe a successful approach for few-shot tasks. Inspired by this work, we study\ndiscrete prompt technologies in practice. There are two issues that arise with\nthe standard prompt approach. First, it can overfit on the prompt template.\nSecond, it requires manual effort to formulate the downstream task as a\nlanguage model problem. In this paper, we propose an improvement to\nprompt-based fine-tuning that addresses these two issues. We refer to our\napproach as DynaMaR -- Dynamic Prompt with Mask Token Representation. Results\nshow that DynaMaR can achieve an average improvement of 10% in few-shot\nsettings and improvement of 3.7% in data-rich settings over the standard\nfine-tuning approach on four e-commerce applications.",
        "score": 1.532219648361206
      },
      {
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study,",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) is currently the most\nwidely used method to align large language models (LLMs) with human\npreferences. Existing RLHF methods can be roughly categorized as either\nreward-based or reward-free. Novel applications such as ChatGPT and Claude\nleverage reward-based methods that first learn a reward model and apply\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). However,\nin academic benchmarks, state-of-the-art results are often achieved via\nreward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly\nsuperior to PPO? Why does PPO perform poorly on these benchmarks? In this\npaper, we first conduct both theoretical and empirical studies on the\nalgorithmic properties of DPO and show that DPO may have fundamental\nlimitations. Moreover, we also comprehensively examine PPO and reveal the key\nfactors for the best performances of PPO in fine-tuning LLMs. Finally, we\nbenchmark DPO and PPO across a collection of RLHF testbeds, ranging from\ndialogue to code generation. Experiment results demonstrate that PPO is able to\nsurpass other alignment methods in all cases and achieve state-of-the-art\nresults in challenging code competitions.",
        "score": 1.4201552867889404
      },
      {
        "title": "Refining the Responses of LLMs by Themselves,",
        "abstract": "In this paper, we propose a simple yet efficient approach based on prompt\nengineering that leverages the large language model itself to optimize its\nanswers without relying on auxiliary models. We introduce an iterative\nself-evaluating optimization mechanism, with the potential for improved output\nquality as iterations progress, removing the need for manual intervention. The\nexperiment's findings indicate that utilizing our response refinement framework\non the GPT-3.5 model yields results that are on par with, or even surpass,\nthose generated by the cutting-edge GPT-4 model. Detailed implementation\nstrategies and illustrative examples are provided to demonstrate the\nsuperiority of our proposed solution.",
        "score": 1.4127156734466553
      },
      {
        "title": "PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching,",
        "abstract": "Instruction fine-tuning has conventionally been employed to adapt Large\nLanguage Models (LLMs) to a variety of tasks. Nonetheless, this technique often\nnecessitates substantial computational resources, making it impractical for\ndeployment by individuals or small-scale entities. Recently, Low-Rank\nAdaptation (LoRA) has become a promising alternative, offering high\ncapabilities on par with full tuning with reduced resource overhead. However,\nattaining satisfactory performance through the fine-tuning of LoRA is a\nnon-trivial challenge. In this paper, we propose PILLOW, which aims to improve\nLoRA's performance by a discrimination-based prompting method, leveraging LLMs'\nIn-Context Learning ability. PILLOW incorporates a matching network that\nselects prompts from a user-defined prompt pool, concatenates the selected\nprompts with the user instruction as input, and performs inference using the\nLoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits\ncommensurate performance on various evaluation metrics compared with typical\ninstruction fine-tuning methods, utilizing only consumer-grade GPU resources\nand exhibiting a large reduction in computational costs.",
        "score": 1.2606569528579712
      },
      {
        "title": "Teach Better or Show Smarter? On Instructions and Exemplars in Automatic\n  Prompt Optimization,",
        "abstract": "Large language models have demonstrated remarkable capabilities, but their\nperformance is heavily reliant on effective prompt engineering. Automatic\nprompt optimization (APO) methods are designed to automate this and can be\nbroadly categorized into those targeting instructions (instruction\noptimization, IO) vs. those targeting exemplars (exemplar selection, ES).\nDespite their shared objective, these have evolved rather independently, with\nIO recently receiving more research attention. This paper seeks to bridge this\ngap by comprehensively comparing the performance of representative IO and ES\ntechniques, both isolation and combination, on a diverse set of challenging\ntasks. Our findings reveal that intelligently reusing model-generated\ninput-output pairs obtained from evaluating prompts on the validation set as\nexemplars consistently improves performance over IO methods but is currently\nunder-investigated. We also find that despite the recent focus on IO, how we\nselect exemplars can outweigh how we optimize instructions, with ES strategies\nas simple as random search outperforming state-of-the-art IO methods with seed\ninstructions without any optimization. Moreover, we observe synergy between ES\nand IO, with optimal combinations surpassing individual contributions. We\nconclude that studying exemplar selection as a standalone method and its\noptimal combination with instruction optimization remains a crucial aspect of\nAPO and deserves greater consideration in future research, even in the era of\nhighly capable instruction-following models.",
        "score": 1.133595585823059
      },
      {
        "title": "XPrompt: Exploring the Extreme of Prompt Tuning,",
        "abstract": "Prompt tuning learns soft prompts to condition frozen Pre-trained Language\nModels (PLMs) for performing downstream tasks in a parameter-efficient manner.\nWhile prompt tuning has gradually reached the performance level of fine-tuning\nas the model scale increases, there is still a large performance gap between\nprompt tuning and fine-tuning for models of moderate and small scales\n(typically less than 11B parameters). In this paper, we empirically show that\nthe trained prompt tokens can have a negative impact on a downstream task and\nthus degrade its performance. To bridge the gap, we propose a novel Prompt\ntuning model with an eXtremely small scale (XPrompt) under the regime of\nlottery tickets hypothesis. Specifically, XPrompt eliminates the negative\nprompt tokens at different granularity levels through a hierarchical structured\npruning, yielding a more parameter-efficient prompt yet with a competitive\nperformance. Comprehensive experiments are carried out on SuperGLUE tasks, and\nthe extensive results indicate that XPrompt is able to close the performance\ngap at smaller model scales.",
        "score": 0.6318212747573853
      },
      {
        "title": "Impact of Preference Noise on the Alignment Performance of Generative\n  Language Models,",
        "abstract": "A key requirement in developing Generative Language Models (GLMs) is to have\ntheir values aligned with human values. Preference-based alignment is a widely\nused paradigm for this purpose, in which preferences over generation pairs are\nfirst elicited from human annotators or AI systems, and then fed into some\nalignment techniques, e.g., Direct Preference Optimization. However, a\nsubstantial percent (20 - 40%) of the preference pairs used in GLM alignment\nare noisy, and it remains unclear how the noise affects the alignment\nperformance and how to mitigate its negative impact. In this paper, we propose\na framework to inject desirable amounts and types of noise to the preferences,\nand systematically study the impact of preference noise on the alignment\nperformance in two tasks (summarization and dialogue generation). We find that\nthe alignment performance can be highly sensitive to the noise rates in the\npreference data: e.g., a 10 percentage points (pp) increase of the noise rate\ncan lead to 30 pp drop in the alignment performance (in win rate). To mitigate\nthe impact of noise, confidence-based data filtering shows significant benefit\nwhen certain types of noise are present. We hope our work can help the\ncommunity better understand and mitigate the impact of preference noise in GLM\nalignment.",
        "score": 0.18169112503528595
      },
      {
        "title": "Demystifying Prompts in Language Models via Perplexity Estimation,",
        "abstract": "Language models can be prompted to perform a wide variety of zero- and\nfew-shot learning problems. However, performance varies significantly with the\nchoice of prompt, and we do not yet understand why this happens or how to pick\nthe best prompts. In this work, we analyze the factors that contribute to this\nvariance and establish a new empirical hypothesis: the performance of a prompt\nis coupled with the extent to which the model is familiar with the language it\ncontains. Over a wide range of tasks, we show that the lower the perplexity of\nthe prompt is, the better the prompt is able to perform the task. As a result,\nwe devise a method for creating prompts: (1) automatically extend a small seed\nset of manually written prompts by paraphrasing using GPT3 and backtranslation\nand (2) choose the lowest perplexity prompts to get significant gains in\nperformance.",
        "score": -0.0038958117365837097
      }
    ]
  },
  {
    "title": "Forgetting before Learning: Utilizing Parametric Arithmetic for\n  Knowledge Updating in Large Language Models",
    "abstract": "  Recent advancements in Large Language Models (LLMs) have showcased their\nremarkable capabilities in text understanding and generation. However, even\nstronger LLMs are susceptible to acquiring erroneous or obsolete information\nfrom the training corpus. Direct secondary fine-tuning with data containing new\nknowledge may be ineffective in updating knowledge due to the conflict between\nold and new knowledge. In this paper, we propose a new paradigm for fine-tuning\ncalled F-Learning (Forgetting before Learning), which employs parametric\narithmetic to facilitate the forgetting of old knowledge and learning of new\nknowledge. Experimental results on two publicly available datasets demonstrate\nthat our proposed F-Learning can obviously improve the knowledge updating\nperformance of both full fine-tuning and LoRA fine-tuning, simultaneously\noutperforming the existing baselines in most cases. Moreover, we have also\ndiscovered that forgetting old knowledge by subtracting the parameters of LoRA\ncan yield a similar effect to subtracting the parameters of full fine-tuning,\nand occasionally even surpass it significantly.\n",
    "related_paper_titles": [
      "Can We Edit Factual Knowledge by In-Context Learning?",
      "Modifying Memories in Transformer Models",
      "Knowledge Editing for Large Language Models: A Survey"
    ],
    "related_paper_abstract": [
      "  Previous studies have shown that large language models (LLMs) like GPTs store\nmassive factual knowledge in their parameters. However, the stored knowledge\ncould be false or out-dated. Traditional knowledge editing methods refine LLMs\nvia fine-tuning on texts containing specific knowledge. However, with the\nincreasing scales of LLMs, these gradient-based approaches bring large\ncomputation costs. The trend of model-as-a-service also makes it impossible to\nmodify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new\nparadigm based on demonstration contexts without parameter updating, we explore\nwhether ICL can edit factual knowledge. To answer this question, we give a\ncomprehensive empirical study of ICL strategies. Experiments show that\nin-context knowledge editing (IKE), without any gradient and parameter\nupdating, achieves a competitive success rate compared to gradient-based\nmethods on GPT-J (6B) but with much fewer side effects, including less\nover-editing on similar but unrelated facts and less knowledge forgetting on\npreviously stored knowledge. We also apply the method to larger LMs with tens\nor hundreds of parameters like OPT-175B, which shows the scalability of our\nmethod. The code is available at https://github.com/Zce1112zslx/IKE.\n",
      "  Large Transformer models have achieved impressive performance in many natural\nlanguage tasks. In particular, Transformer based language models have been\nshown to have great capabilities in encoding factual knowledge in their vast\namount of parameters. While the tasks of improving the memorization and\ngeneralization of Transformers have been widely studied, it is not well known\nhow to make transformers forget specific old facts and memorize new ones. In\nthis paper, we propose a new task of \\emph{explicitly modifying specific\nfactual knowledge in Transformer models while ensuring the model performance\ndoes not degrade on the unmodified facts}. This task is useful in many\nscenarios, such as updating stale knowledge, protecting privacy, and\neliminating unintended biases stored in the models. We benchmarked several\napproaches that provide natural baseline performances on this task. This leads\nto the discovery of key components of a Transformer model that are especially\neffective for knowledge modifications. The work also provides insights into the\nrole that different training phases (such as pretraining and fine-tuning) play\ntowards memorization and knowledge modification.\n",
      "  Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.\n"
    ],
    "entities": [
      "Retrieval Augmented",
      "SQL",
      "GenAI",
      "KV",
      "XAI",
      "HumanEval",
      "EHR",
      "MCTS",
      "Copilot",
      "Meta",
      "Korean",
      "Monte Carlo Tree Search",
      "TSF",
      "GAI",
      "DMs",
      "Artificial",
      "QA",
      "KGC",
      "Methods",
      "PDDL",
      "LVLMs",
      "EHRs",
      "MARL",
      "fMRI",
      "Markov",
      "Robust",
      "EEG",
      "Automatic",
      "Spanish",
      "Causal"
    ],
    "retrieved_papers": [
      {
        "title": "Forgetting before Learning: Utilizing Parametric Arithmetic for\n  Knowledge Updating in Large Language Models,",
        "abstract": "Recent advancements in Large Language Models (LLMs) have showcased their\nremarkable capabilities in text understanding and generation. However, even\nstronger LLMs are susceptible to acquiring erroneous or obsolete information\nfrom the training corpus. Direct secondary fine-tuning with data containing new\nknowledge may be ineffective in updating knowledge due to the conflict between\nold and new knowledge. In this paper, we propose a new paradigm for fine-tuning\ncalled F-Learning (Forgetting before Learning), which employs parametric\narithmetic to facilitate the forgetting of old knowledge and learning of new\nknowledge. Experimental results on two publicly available datasets demonstrate\nthat our proposed F-Learning can obviously improve the knowledge updating\nperformance of both full fine-tuning and LoRA fine-tuning, simultaneously\noutperforming the existing baselines in most cases. Moreover, we have also\ndiscovered that forgetting old knowledge by subtracting the parameters of LoRA\ncan yield a similar effect to subtracting the parameters of full fine-tuning,\nand occasionally even surpass it significantly.",
        "score": 5.75131368637085
      },
      {
        "title": "Pre-trained Vision and Language Transformers Are Few-Shot Incremental\n  Learners,",
        "abstract": "Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model\nto learn new classes incrementally without forgetting when only a few samples\nfor each class are given. FSCIL encounters two significant challenges:\ncatastrophic forgetting and overfitting, and these challenges have driven prior\nstudies to primarily rely on shallow models, such as ResNet-18. Even though\ntheir limited capacity can mitigate both forgetting and overfitting issues, it\nleads to inadequate knowledge transfer during few-shot incremental sessions. In\nthis paper, we argue that large models such as vision and language transformers\npre-trained on large datasets can be excellent few-shot incremental learners.\nTo this end, we propose a novel FSCIL framework called PriViLege, Pre-trained\nVision and Language transformers with prompting functions and knowledge\ndistillation. Our framework effectively addresses the challenges of\ncatastrophic forgetting and overfitting in large models through new pre-trained\nknowledge tuning (PKT) and two losses: entropy-based divergence loss and\nsemantic knowledge distillation loss. Experimental results show that the\nproposed PriViLege significantly outperforms the existing state-of-the-art\nmethods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and\n+13.36% in miniImageNet. Our implementation code is available at\nhttps://github.com/KHU-AGI/PriViLege.",
        "score": 1.8109530210494995
      },
      {
        "title": "Train-Attention: Meta-Learning Where to Focus in Continual Knowledge\n  Learning,",
        "abstract": "Previous studies on continual knowledge learning (CKL) in large language\nmodels (LLMs) have predominantly focused on approaches such as regularization,\narchitectural modifications, and rehearsal techniques to mitigate catastrophic\nforgetting. However, these methods naively inherit the inefficiencies of\nstandard training procedures, indiscriminately applying uniform weight across\nall tokens, which can lead to unnecessary parameter updates and increased\nforgetting. To address these shortcomings, we propose a novel CKL approach\ntermed Train-Attention-Augmented Language Model (TAALM), which enhances\nlearning efficiency by dynamically predicting and applying weights to tokens\nbased on their usefulness. This method employs a meta-learning framework that\noptimizes token importance predictions, facilitating targeted knowledge updates\nand minimizing forgetting. Also, we observe that existing benchmarks do not\nclearly exhibit the trade-off between learning and retaining, therefore we\npropose a new benchmark, \\textsc{LAMA-ckl}, to address this issue. Through\nexperiments conducted on both newly introduced and established CKL benchmarks,\nTAALM proves the state-of-the-art performance upon the baselines, and also\nshows synergistic compatibility when integrated with previous CKL approaches.",
        "score": 1.7513514757156372
      },
      {
        "title": "Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications,",
        "abstract": "This paper studies the evolving domain of Continual Learning (CL) in large\nlanguage models (LLMs), with a focus on developing strategies for efficient and\nsustainable training. Our primary emphasis is on continual domain-adaptive\npretraining, a process designed to equip LLMs with the ability to integrate new\ninformation from various domains while retaining previously learned knowledge\nand enhancing cross-domain knowledge transfer without relying on\ndomain-specific identification. Unlike previous studies, which mostly\nconcentrate on a limited selection of tasks or domains and primarily aim to\naddress the issue of forgetting, our research evaluates the adaptability and\ncapabilities of LLMs to changing data landscapes in practical scenarios. To\nthis end, we introduce a new benchmark designed to measure the adaptability of\nLLMs to these evolving data environments, offering a comprehensive framework\nfor evaluation. We examine the impact of model size on learning efficacy and\nforgetting, as well as how the progression and similarity of emerging domains\naffect the knowledge transfer within these models. Our findings uncover several\nkey insights: (i) when the sequence of domains shows semantic similarity,\ncontinual pretraining enables LLMs to better specialize in the current domain\ncompared to stand-alone fine-tuning, (ii) training across a diverse range of\ndomains enhances both backward and forward knowledge transfer, and (iii)\nsmaller models are particularly sensitive to continual pretraining, showing the\nmost significant rates of both forgetting and learning. We posit that our\nresearch marks a shift towards establishing a more realistic benchmark for\ninvestigating CL in LLMs, and has the potential to play a key role in guiding\nthe direction of future research in the field.",
        "score": 1.5929957628250122
      },
      {
        "title": "Large Language Model Can Continue Evolving From Mistakes,",
        "abstract": "As world knowledge evolves and new task paradigms emerge, Continual Learning\n(CL) is crucial for keeping Large Language Models (LLMs) up-to-date and\naddressing their shortcomings. In practical applications, LLMs often require\nboth continual instruction tuning (CIT) and continual pre-training (CPT) to\nadapt to new task paradigms and acquire necessary knowledge for task-solving.\nHowever, it remains challenging to collect CPT data that addresses the\nknowledge deficiencies in models while maintaining adequate volume, and\nimproving the efficiency of utilizing this data also presents significant\ndifficulties. Inspired by the 'summarizing mistakes' learning skill, we propose\nthe Continue Evolving from Mistakes (CEM) method, aiming to provide a\ndata-efficient approach for collecting CPT data and continually improving LLMs'\nperformance through iterative evaluation and supplementation with\nmistake-relevant knowledge. To efficiently utilize these CPT data and mitigate\nforgetting, we design a novel CL training set construction paradigm that\nintegrates parallel CIT and CPT data. Extensive experiments demonstrate the\nefficacy of the CEM method, achieving up to a 17% improvement in accuracy in\nthe best case. Furthermore, additional experiments confirm the potential of\ncombining CEM with catastrophic forgetting mitigation methods, enabling\niterative and continual model evolution.",
        "score": 1.4918029308319092
      },
      {
        "title": "MoE-CT: A Novel Approach For Large Language Models Training With\n  Resistance To Catastrophic Forgetting,",
        "abstract": "The advent of large language models (LLMs) has predominantly catered to\nhigh-resource languages, leaving a disparity in performance for low-resource\nlanguages. Conventional Continual Training (CT) approaches to bridge this gap\noften undermine a model's original linguistic proficiency when expanding to\nmultilingual contexts. Addressing this issue, we introduce a novel MoE-CT\narchitecture, a paradigm that innovatively separates the base model's learning\nfrom the multilingual expansion process. Our design freezes the original LLM\nparameters, thus safeguarding its performance in high-resource languages, while\nan appended MoE module, trained on diverse language datasets, augments\nlow-resource language proficiency. Our approach significantly outperforms\nconventional CT methods, as evidenced by our experiments, which show marked\nimprovements in multilingual benchmarks without sacrificing the model's\noriginal language performance. Moreover, our MoE-CT framework demonstrates\nenhanced resistance to forgetting and superior transfer learning capabilities.\nBy preserving the base model's integrity and focusing on strategic parameter\nexpansion, our methodology advances multilingual language modeling and\nrepresents a significant step forward for low-resource language inclusion in\nLLMs, indicating a fruitful direction for future research in language\ntechnologies.",
        "score": 1.4912298917770386
      },
      {
        "title": "Continual Learning of Large Language Models: A Comprehensive Survey,",
        "abstract": "The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.",
        "score": 1.4756337404251099
      },
      {
        "title": "HFT: Half Fine-Tuning for Large Language Models,",
        "abstract": "Large language models (LLMs) with one or more fine-tuning phases have become\na necessary step to unlock various capabilities, enabling LLMs to follow\nnatural language instructions or align with human preferences. However, it\ncarries the risk of catastrophic forgetting during sequential training, the\nparametric knowledge or the ability learned in previous stages may be\noverwhelmed by incoming training data. In this paper, we find that by regularly\nresetting partial parameters, LLMs can restore some of the original knowledge.\nInspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute\nfor full fine-tuning (FFT), to mitigate the forgetting issues, where half of\nthe parameters are selected to learn new tasks while the other half are frozen\nto remain previous knowledge. We provide a feasibility analysis from the\nperspective of optimization and interpret the parameter selection operation as\na regularization term. Without changing the model architecture, HFT could be\nseamlessly integrated into existing fine-tuning frameworks. Extensive\nexperiments and analysis on supervised fine-tuning, direct preference\noptimization, and continual learning consistently demonstrate the\neffectiveness, robustness, and efficiency of HFT. Compared with FFT, HFT not\nonly significantly alleviates the forgetting problem, but also achieves the\nbest performance in a series of downstream benchmarks, with an approximately\n30% reduction in training time.",
        "score": 1.4121880531311035
      },
      {
        "title": "Class-Incremental Learning with CLIP: Adaptive Representation Adjustment\n  and Parameter Fusion,",
        "abstract": "Class-incremental learning is a challenging problem, where the goal is to\ntrain a model that can classify data from an increasing number of classes over\ntime. With the advancement of vision-language pre-trained models such as CLIP,\nthey demonstrate good generalization ability that allows them to excel in\nclass-incremental learning with completely frozen parameters. However, further\nadaptation to downstream tasks by simply fine-tuning the model leads to severe\nforgetting. Most existing works with pre-trained models assume that the\nforgetting of old classes is uniform when the model acquires new knowledge. In\nthis paper, we propose a method named Adaptive Representation Adjustment and\nParameter Fusion (RAPF). During training for new data, we measure the influence\nof new classes on old ones and adjust the representations, using textual\nfeatures. After training, we employ a decomposed parameter fusion to further\nmitigate forgetting during adapter module fine-tuning. Experiments on several\nconventional benchmarks show that our method achieves state-of-the-art results.\nOur code is available at \\url{https://github.com/linlany/RAPF}.",
        "score": 1.3329110145568848
      },
      {
        "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language\n  Models,",
        "abstract": "Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.",
        "score": 1.2682117223739624
      },
      {
        "title": "Continual Learning for Large Language Models: A Survey,",
        "abstract": "Large language models (LLMs) are not amenable to frequent re-training, due to\nhigh training costs arising from their massive scale. However, updates are\nnecessary to endow LLMs with new skills and keep them up-to-date with rapidly\nevolving human knowledge. This paper surveys recent works on continual learning\nfor LLMs. Due to the unique nature of LLMs, we catalog continue learning\ntechniques in a novel multi-staged categorization scheme, involving continual\npretraining, instruction tuning, and alignment. We contrast continual learning\nfor LLMs with simpler adaptation methods used in smaller models, as well as\nwith other enhancement strategies like retrieval-augmented generation and model\nediting. Moreover, informed by a discussion of benchmarks and evaluation, we\nidentify several challenges and future work directions for this crucial task.",
        "score": 1.132926344871521
      },
      {
        "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning,",
        "abstract": "Catastrophic Forgetting (CF) means models forgetting previously acquired\nknowledge when learning new data. It compromises the effectiveness of large\nlanguage models (LLMs) during fine-tuning, yet the underlying causes have not\nbeen thoroughly investigated. This paper takes the first step to reveal the\ndirect link between the flatness of the model loss landscape and the extent of\nCF in the field of LLMs. Based on this, we introduce the sharpness-aware\nminimization to mitigate CF by flattening the loss landscape. Experiments on\nthree widely-used fine-tuning datasets, spanning different model scales,\ndemonstrate the effectiveness of our method in alleviating CF. Analyses show\nthat we nicely complement the existing anti-forgetting strategies, further\nenhancing the resistance of LLMs to CF.",
        "score": 1.1224392652511597
      },
      {
        "title": "Knowledge Editing for Large Language Models: A Survey,",
        "abstract": "Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.",
        "score": 1.1040565967559814
      },
      {
        "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?,",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success across various\ntasks, yet their ability to learn incrementally without forgetting remains\nunderexplored. Incremental learning (IL) is crucial as it enables models to\nacquire new knowledge while retaining previously learned information, akin to\nhuman learning. Existing benchmarks for IL are insufficient due to data leakage\nissues and the overqualification of LLMs. To address these challenges, we\nintroduce Concept-1K, a novel dataset comprising 1,023 recently emerged\nconcepts across diverse domains. The concepts in Concept-1K are discrete,\ninterpretable units of knowledge that allow for fine-grained analysis of\nlearning and forgetting processes. Using Concept-1K as a testbed, we aim to\nanswer the question: ``Can LLMs learn new concepts incrementally without\nforgetting like humans?'' Our investigation reveals that LLMs still suffer from\ncatastrophic forgetting and that LoRA, despite fine-tuning fewer parameters,\nmay lead to more forgetting on training data. Additionally, we explore the\nroles of in-context learning, model scale, buffer size, and pretraining in IL\nperformance. These findings highlight the strengths and limitations of LLMs in\nIL scenarios and provide a robust benchmark for future research.",
        "score": 1.082468867301941
      },
      {
        "title": "Towards Continual Knowledge Learning of Language Models,",
        "abstract": "Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs. The benchmark datasets, evaluation script, and\nbaseline code to reproduce our results are available at\nhttps://github.com/joeljang/continual-knowledge-learning.",
        "score": 0.7689773440361023
      },
      {
        "title": "From Static to Dynamic: A Continual Learning Framework for Large\n  Language Models,",
        "abstract": "The vast number of parameters in large language models (LLMs) endows them\nwith remarkable capabilities, allowing them to excel in a variety of natural\nlanguage processing tasks. However, this complexity also presents challenges,\nmaking LLMs difficult to train and inhibiting their ability to continuously\nassimilate new knowledge, which may lead to inaccuracies in their outputs. To\nmitigate these issues, this paper presents DynaMind, a novel continual learning\nframework designed for LLMs. DynaMind incorporates memory mechanisms to\nassimilate new knowledge and modular operators to enhance the model inference\nprocess with the newly assimilated knowledge, consequently improving the\naccuracies of LLMs' outputs. Benchmark experiments demonstrate DynaMind's\neffectiveness in overcoming these challenges. The code and demo of DynaMind are\navailable on GitHub: https://github.com/Elfsong/DynaMind.",
        "score": 0.5759478807449341
      },
      {
        "title": "Towards Practical Tool Usage for Continually Learning LLMs,",
        "abstract": "Large language models (LLMs) show an innate skill for solving language based\ntasks. But insights have suggested an inability to adjust for information or\ntask-solving skills becoming outdated, as their knowledge, stored directly\nwithin their parameters, remains static in time. Tool use helps by offloading\nwork to systems that the LLM can access through an interface, but LLMs that use\nthem still must adapt to nonstationary environments for prolonged use, as new\ntools can emerge and existing tools can change. Nevertheless, tools require\nless specialized knowledge, therefore we hypothesize they are better suited for\ncontinual learning (CL) as they rely less on parametric memory for solving\ntasks and instead focus on learning when to apply pre-defined tools. To verify\nthis, we develop a synthetic benchmark and follow this by aggregating existing\nNLP tasks to form a more realistic testing scenario. While we demonstrate\nscaling model size is not a solution, regardless of tool usage, continual\nlearning techniques can enable tool LLMs to both adapt faster while forgetting\nless, highlighting their potential as continual learners.",
        "score": 0.5373005270957947
      },
      {
        "title": "Improving Language Plasticity via Pretraining with Active Forgetting,",
        "abstract": "Pretrained language models (PLMs) are today the primary model for natural\nlanguage processing. Despite their impressive downstream performance, it can be\ndifficult to apply PLMs to new languages, a barrier to making their\ncapabilities universally accessible. While prior work has shown it possible to\naddress this issue by learning a new embedding layer for the new language,\ndoing so is both data and compute inefficient. We propose to use an active\nforgetting mechanism during pretraining, as a simple way of creating PLMs that\ncan quickly adapt to new languages. Concretely, by resetting the embedding\nlayer every K updates during pretraining, we encourage the PLM to improve its\nability of learning new embeddings within a limited number of updates, similar\nto a meta-learning effect. Experiments with RoBERTa show that models pretrained\nwith our forgetting mechanism not only demonstrate faster convergence during\nlanguage adaptation but also outperform standard ones in a low-data regime,\nparticularly for languages that are distant from English.",
        "score": 0.4922623634338379
      },
      {
        "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts,",
        "abstract": "Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. Due\nto this crucial need to keep models updated, online learning has emerged as a\ncritical necessity when utilizing LLMs for real-world applications. However,\ngiven the ever-expanding corpus of unseen documents and the large parameter\nspace of modern LLMs, efficient adaptation is essential. To address these\nchallenges, we propose Memory of Amortized Contexts (MAC), an efficient and\neffective online adaptation framework for LLMs with strong knowledge retention.\nWe propose an amortized feature extraction and memory-augmentation approach to\ncompress and extract information from new documents into compact modulations\nstored in a memory bank. When answering questions, our model attends to and\nextracts relevant knowledge from this memory bank. To learn informative\nmodulations in an efficient manner, we utilize amortization-based\nmeta-learning, which substitutes the optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. Code is available at: https://github.com/jihoontack/MAC.",
        "score": 0.4915294349193573
      },
      {
        "title": "Semiparametric Language Models Are Scalable Continual Learners,",
        "abstract": "Semiparametric language models (LMs) have shown promise in continuously\nlearning from new text data by combining a parameterized neural LM with a\ngrowable non-parametric memory for memorizing new content. However,\nconventional semiparametric LMs will finally become prohibitive for computing\nand storing if they are applied to continual learning over streaming data,\nbecause the non-parametric memory grows linearly with the amount of data they\nlearn from over time. To address the issue of scalability, we present a simple\nand intuitive approach called Selective Memorization (SeMem), which only\nmemorizes difficult samples that the model is likely to struggle with. We\ndemonstrate that SeMem improves the scalability of semiparametric LMs for\ncontinual learning over streaming data in two ways: (1) data-wise scalability:\nas the model becomes stronger through continual learning, it will encounter\nfewer difficult cases that need to be memorized, causing the growth of the\nnon-parametric memory to slow down over time rather than growing at a linear\nrate with the size of training data; (2) model-wise scalability: SeMem allows a\nlarger model to memorize fewer samples than its smaller counterpart because it\nis rarer for a larger model to encounter incomprehensible cases, resulting in a\nnon-parametric memory that does not scale linearly with model size. We conduct\nextensive experiments in language modeling and downstream tasks to test SeMem's\nresults, showing SeMem enables a semiparametric LM to be a scalable continual\nlearner with little forgetting.",
        "score": 0.3847928047180176
      },
      {
        "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models,",
        "abstract": "We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting",
        "score": 0.15950389206409454
      },
      {
        "title": "Ziya2: Data-centric Learning is All LLMs Need,",
        "abstract": "Various large language models (LLMs) have been proposed in recent years,\nincluding closed- and open-source ones, continually setting new records on\nmultiple benchmarks. However, the development of LLMs still faces several\nissues, such as high cost of training models from scratch, and continual\npre-training leading to catastrophic forgetting, etc. Although many such issues\nare addressed along the line of research on LLMs, an important yet practical\nlimitation is that many studies overly pursue enlarging model sizes without\ncomprehensively analyzing and optimizing the use of pre-training data in their\nlearning process, as well as appropriate organization and leveraging of such\ndata in training LLMs under cost-effective settings. In this work, we propose\nZiya2, a model with 13 billion parameters adopting LLaMA2 as the foundation\nmodel, and further pre-trained on 700 billion tokens, where we focus on\npre-training techniques and use data-centric optimization to enhance the\nlearning process of Ziya2 on different stages. We define three data attributes\nand firstly establish data-centric scaling laws to illustrate how different\ndata impacts LLMs. Experiments show that Ziya2 significantly outperforms other\nmodels in multiple benchmarks especially with promising results compared to\nrepresentative open-source ones. Ziya2 (Base) is released at\nhttps://huggingface.co/IDEA-CCNL/Ziya2-13B-Base and\nhttps://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.",
        "score": -0.11409962177276611
      },
      {
        "title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning,",
        "abstract": "Low-rank adaptation is a popular parameter-efficient fine-tuning method for\nlarge language models. In this paper, we analyze the impact of low-rank\nupdating, as implemented in LoRA. Our findings suggest that the low-rank\nupdating mechanism may limit the ability of LLMs to effectively learn and\nmemorize new knowledge. Inspired by this observation, we propose a new method\ncalled MoRA, which employs a square matrix to achieve high-rank updating while\nmaintaining the same number of trainable parameters. To achieve it, we\nintroduce the corresponding non-parameter operators to reduce the input\ndimension and increase the output dimension for the square matrix. Furthermore,\nthese operators ensure that the weight can be merged back into LLMs, which\nmakes our method can be deployed like LoRA. We perform a comprehensive\nevaluation of our method across five tasks: instruction tuning, mathematical\nreasoning, continual pretraining, memory and pretraining. Our method\noutperforms LoRA on memory-intensive tasks and achieves comparable performance\non other tasks.",
        "score": -0.5940161347389221
      },
      {
        "title": "Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced\n  Training for Neural Machine Translation,",
        "abstract": "Neural networks tend to gradually forget the previously learned knowledge\nwhen learning multiple tasks sequentially from dynamic data distributions. This\nproblem is called \\textit{catastrophic forgetting}, which is a fundamental\nchallenge in the continual learning of neural networks. In this work, we\nobserve that catastrophic forgetting not only occurs in continual learning but\nalso affects the traditional static training. Neural networks, especially\nneural machine translation models, suffer from catastrophic forgetting even if\nthey learn from a static training set. To be specific, the final model pays\nimbalanced attention to training samples, where recently exposed samples\nattract more attention than earlier samples. The underlying cause is that\ntraining samples do not get balanced training in each model update, so we name\nthis problem \\textit{imbalanced training}. To alleviate this problem, we\npropose Complementary Online Knowledge Distillation (COKD), which uses\ndynamically updated teacher models trained on specific data orders to\niteratively provide complementary knowledge to the student model. Experimental\nresults on multiple machine translation tasks show that our method successfully\nalleviates the problem of imbalanced training and achieves substantial\nimprovements over strong baseline systems.",
        "score": -0.7339663505554199
      },
      {
        "title": "Composable Sparse Fine-Tuning for Cross-Lingual Transfer,",
        "abstract": "Fine-tuning the entire set of parameters of a large pretrained model has\nbecome the mainstream approach for transfer learning. To increase its\nefficiency and prevent catastrophic forgetting and interference, techniques\nlike adapters and sparse fine-tuning have been developed. Adapters are modular,\nas they can be combined to adapt a model towards different facets of knowledge\n(e.g., dedicated language and/or task adapters). Sparse fine-tuning is\nexpressive, as it controls the behavior of all model components. In this work,\nwe introduce a new fine-tuning method with both these desirable properties. In\nparticular, we learn sparse, real-valued masks based on a simple variant of the\nLottery Ticket Hypothesis. Task-specific masks are obtained from annotated data\nin a source language, and language-specific masks from masked language modeling\nin a target language. Both these masks can then be composed with the pretrained\nmodel. Unlike adapter-based fine-tuning, this method neither increases the\nnumber of parameters at inference time nor alters the original model\narchitecture. Most importantly, it outperforms adapters in zero-shot\ncross-lingual transfer by a large margin in a series of multilingual\nbenchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI.\nBased on an in-depth analysis, we additionally find that sparsity is crucial to\nprevent both 1) interference between the fine-tunings to be composed and 2)\noverfitting. We release the code and models at\nhttps://github.com/cambridgeltl/composable-sft.",
        "score": -0.8750895857810974
      },
      {
        "title": "SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained\n  Large Language Models,",
        "abstract": "Full fine-tuning is a popular approach to adapt Transformer-based pre-trained\nlarge language models to a specific downstream task. However, the substantial\nrequirements for computational power and storage have discouraged its\nwidespread use. Moreover, increasing evidence of catastrophic forgetting and\noverparameterization in the Transformer architecture has motivated researchers\nto seek more efficient fine-tuning (PEFT) methods. Commonly known\nparameter-efficient fine-tuning methods like LoRA and BitFit are typically\napplied across all layers of the model. We propose a PEFT method, called\nStratified Progressive Adaptation Fine-tuning (SPAFIT), based on the\nlocalization of different types of linguistic knowledge to specific layers of\nthe model. Our experiments, conducted on nine tasks from the GLUE benchmark,\nshow that our proposed SPAFIT method outperforms other PEFT methods while\nfine-tuning only a fraction of the parameters adjusted by other methods.",
        "score": -1.4519257545471191
      },
      {
        "title": "Two-stage LLM Fine-tuning with Less Specialization and More\n  Generalization,",
        "abstract": "Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.",
        "score": -1.5007855892181396
      },
      {
        "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs,",
        "abstract": "Large language models (LLMs) have achieved significant progress from\npre-training on and memorizing a wide range of textual data, however, this\nprocess might suffer from privacy issues and violations of data protection\nregulations. As a result, the ability to easily remove data related to\nindividual users from such models while not deteriorating their predictive\nquality after the removal becomes increasingly important. To address these\nissues, in this work, we propose an efficient unlearning framework that could\nefficiently update LLMs without having to retrain the whole model after data\nremovals, by introducing lightweight unlearning layers learned with a selective\nteacher-student objective into the transformers. In addition, we introduce a\nfusion mechanism to effectively combine different unlearning layers that learns\nto forget different sets of data to handle a sequence of forgetting operations.\nExperiments on classification and generation tasks demonstrate the\neffectiveness of our proposed methods compared to the state-of-the-art\nbaselines.",
        "score": -1.9224125146865845
      },
      {
        "title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning,",
        "abstract": "Adapting Large Language Models (LLMs) to new tasks through fine-tuning has\nbeen made more efficient by the introduction of Parameter-Efficient Fine-Tuning\n(PEFT) techniques, such as LoRA. However, these methods often underperform\ncompared to full fine-tuning, particularly in scenarios involving complex\ndatasets. This issue becomes even more pronounced in complex domains,\nhighlighting the need for improved PEFT approaches that can achieve better\nperformance. Through a series of experiments, we have uncovered two critical\ninsights that shed light on the training and parameter inefficiency of LoRA.\nBuilding on these insights, we have developed HydraLoRA, a LoRA framework with\nan asymmetric structure that eliminates the need for domain expertise. Our\nexperiments demonstrate that HydraLoRA outperforms other PEFT approaches, even\nthose that rely on domain knowledge during the training and inference phases.",
        "score": -3.8850817680358887
      },
      {
        "title": "Effectiveness of Data Augmentation for Parameter Efficient Tuning with\n  Limited Data,",
        "abstract": "Recent work has demonstrated that using parameter efficient tuning techniques\nsuch as prefix tuning (or P-tuning) on pretrained language models can yield\nperformance that is comparable or superior to fine-tuning while dramatically\nreducing trainable parameters. Nevertheless, the effectiveness of such methods\nunder the context of data augmentation, a common strategy to improve learning\nunder low data regimes, has not been fully explored. In this paper, we examine\nthe effectiveness of several popular task-agnostic data augmentation\ntechniques, i.e., EDA, Back Translation, and Mixup, when using two general\nparameter efficient tuning methods, P-tuning v2 and LoRA, under data scarcity.\nWe show that data augmentation can be used to boost the performance of P-tuning\nand LoRA models, but the effectiveness of each technique varies and certain\nmethods can lead to a notable degradation in performance, particularly when\nusing larger models and on harder tasks. We further analyze the sentence\nrepresentations of P-tuning compared to fine-tuning to help understand the\nabove behaviour, and reveal how P-tuning generally presents a more limited\nability to separate the sentence embeddings from different classes of augmented\ndata. In addition, it displays poorer performance on heavily altered data.\nHowever, we demonstrate that by adding a simple contrastive loss function it\ncan help mitigate such issues for prefix tuning, resulting in sizable\nimprovements to augmented data performance.",
        "score": -3.8922717571258545
      }
    ]
  },
  {
    "title": "Defending Large Language Models Against Jailbreaking Attacks Through\n  Goal Prioritization",
    "abstract": "  While significant attention has been dedicated to exploiting weaknesses in\nLLMs through jailbreaking attacks, there remains a paucity of effort in\ndefending against these attacks. We point out a pivotal factor contributing to\nthe success of jailbreaks: the intrinsic conflict between the goals of being\nhelpful and ensuring safety. Accordingly, we propose to integrate goal\nprioritization at both training and inference stages to counteract.\nImplementing goal prioritization during inference substantially diminishes the\nAttack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And\nintegrating goal prioritization into model training reduces the ASR from 71.0%\nto 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking\nsamples are included during training, our approach slashes the ASR by half.\nAdditionally, our findings reveal that while stronger LLMs face greater safety\nrisks, they also possess a greater capacity to be steered towards defending\nagainst such attacks, both because of their stronger ability in instruction\nfollowing. Our work thus contributes to the comprehension of jailbreaking\nattacks and defenses, and sheds light on the relationship between LLMs'\ncapability and safety. Our code is available at\n\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.\n",
    "related_paper_titles": [
      "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
      "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
      "RAIN: Your Language Models Can Align Themselves without Finetuning"
    ],
    "related_paper_abstract": [
      "  Despite efforts to align large language models (LLMs) with human intentions,\nwidely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking\nattacks, wherein an adversary fools a targeted LLM into generating\nobjectionable content. To address this vulnerability, we propose SmoothLLM, the\nfirst algorithm designed to mitigate jailbreaking attacks. Based on our finding\nthat adversarially-generated prompts are brittle to character-level changes,\nour defense randomly perturbs multiple copies of a given input prompt, and then\naggregates the corresponding predictions to detect adversarial inputs. Across a\nrange of popular LLMs, SmoothLLM sets the state-of-the-art for robustness\nagainst the GCG, PAIR, RandomSearch, and AmpleGCG jailbreaks. SmoothLLM is also\nresistant against adaptive GCG attacks, exhibits a small, though non-negligible\ntrade-off between robustness and nominal performance, and is compatible with\nany LLM. Our code is publicly available at\n\\url{https://github.com/arobey1/smooth-llm}.\n",
      "  Safety lies at the core of the development of Large Language Models (LLMs).\nThere is ample work on aligning LLMs with human ethics and preferences,\nincluding data filtering in pretraining, supervised fine-tuning, reinforcement\nlearning from human feedback, and red teaming, etc. In this study, we discover\nthat chat in cipher can bypass the safety alignment techniques of LLMs, which\nare mainly conducted in natural languages. We propose a novel framework\nCipherChat to systematically examine the generalizability of safety alignment\nto non-natural languages -- ciphers. CipherChat enables humans to chat with\nLLMs through cipher prompts topped with system role descriptions and few-shot\nenciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\nincluding ChatGPT and GPT-4 for different representative human ciphers across\n11 safety domains in both English and Chinese. Experimental results show that\ncertain ciphers succeed almost 100% of the time to bypass the safety alignment\nof GPT-4 in several safety domains, demonstrating the necessity of developing\nsafety alignment for non-natural languages. Notably, we identify that LLMs seem\nto have a ''secret cipher'', and propose a novel SelfCipher that uses only role\nplay and several demonstrations in natural language to evoke this capability.\nSelfCipher surprisingly outperforms existing human ciphers in almost all cases.\nOur code and data will be released at https://github.com/RobustNLP/CipherChat.\n",
      "  Large language models (LLMs) often demonstrate inconsistencies with human\npreferences. Previous research typically gathered human preference data and\nthen aligned the pre-trained models using reinforcement learning or instruction\ntuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without\nrequiring alignment data is more appealing. This work explores the potential of\nthe latter setting. We discover that by integrating self-evaluation and rewind\nmechanisms, unaligned LLMs can directly produce responses consistent with human\npreferences via self-boosting. We introduce a novel inference method,\nRewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to\nevaluate their own generation and use the evaluation results to guide rewind\nand generation for AI safety. Notably, RAIN operates without the need of extra\ndata for model alignment and abstains from any training, gradient computation,\nor parameter updates. Experimental results evaluated by GPT-4 and humans\ndemonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the\nharmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while\nmaintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the\ntruthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.\n"
    ],
    "entities": [
      "ChatGPT",
      "SFT",
      "RLHF",
      "KV",
      "Human Feedback",
      "NLG",
      "MATH",
      "ToM",
      "MMLU",
      "KG",
      "GSM8K",
      "Direct Preference",
      "Wikipedia",
      "IFT",
      "Mind",
      "MoE",
      "Thought",
      "PTQ",
      "HumanEval",
      "Knowledge",
      "Gemini",
      "RAG",
      "Korean",
      "GPT",
      "Llama",
      "Automated",
      "Experts",
      "Mistral",
      "Large Language Models Large",
      "Evaluation"
    ],
    "retrieved_papers": [
      {
        "title": "Defending Large Language Models Against Jailbreaking Attacks Through\n  Goal Prioritization,",
        "abstract": "While significant attention has been dedicated to exploiting weaknesses in\nLLMs through jailbreaking attacks, there remains a paucity of effort in\ndefending against these attacks. We point out a pivotal factor contributing to\nthe success of jailbreaks: the intrinsic conflict between the goals of being\nhelpful and ensuring safety. Accordingly, we propose to integrate goal\nprioritization at both training and inference stages to counteract.\nImplementing goal prioritization during inference substantially diminishes the\nAttack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And\nintegrating goal prioritization into model training reduces the ASR from 71.0%\nto 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking\nsamples are included during training, our approach slashes the ASR by half.\nAdditionally, our findings reveal that while stronger LLMs face greater safety\nrisks, they also possess a greater capacity to be steered towards defending\nagainst such attacks, both because of their stronger ability in instruction\nfollowing. Our work thus contributes to the comprehension of jailbreaking\nattacks and defenses, and sheds light on the relationship between LLMs'\ncapability and safety. Our code is available at\n\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.",
        "score": 6.971518516540527
      },
      {
        "title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models,",
        "abstract": "The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these models still have numerous security vulnerabilities,\nparticularly when faced with jailbreak attacks. Therefore, by investigating\njailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us in\ndeveloping more robust defense mechanisms to fortify their security. In this\npaper, we further explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analysis-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse.",
        "score": 4.975236415863037
      },
      {
        "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey,",
        "abstract": "Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.",
        "score": 4.098611354827881
      },
      {
        "title": "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\n  Against Large Language Models,",
        "abstract": "Jailbreak attacks aim to induce Large Language Models (LLMs) to generate\nharmful responses for forbidden instructions, presenting severe misuse threats\nto LLMs. Up to now, research into jailbreak attacks and defenses is emerging,\nhowever, there is (surprisingly) no consensus on how to evaluate whether a\njailbreak attempt is successful. In other words, the methods to assess the\nharmfulness of an LLM's response are varied, such as manual annotation or\nprompting GPT-4 in specific ways. Each approach has its own set of strengths\nand weaknesses, impacting their alignment with human values, as well as the\ntime and financial cost. This diversity in evaluation presents challenges for\nresearchers in choosing suitable evaluation methods and conducting fair\ncomparisons across different jailbreak attacks and defenses. In this paper, we\nconduct a comprehensive analysis of jailbreak evaluation methodologies, drawing\nfrom nearly ninety jailbreak research released between May 2023 and April 2024.\nOur study introduces a systematic taxonomy of jailbreak evaluators, offering\nin-depth insights into their strengths and weaknesses, along with the current\nstatus of their adaptation. Moreover, to facilitate subsequent research, we\npropose JailbreakEval, a user-friendly toolkit focusing on the evaluation of\njailbreak attempts. It includes various well-known evaluators out-of-the-box,\nso that users can obtain evaluation results with only a single command.\nJailbreakEval also allows users to customize their own evaluation workflow in a\nunified framework with the ease of development and comparison. In summary, we\nregard JailbreakEval to be a catalyst that simplifies the evaluation process in\njailbreak research and fosters an inclusive standard for jailbreak evaluation\nwithin the community.",
        "score": 3.9254798889160156
      },
      {
        "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily,",
        "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.",
        "score": 3.6620352268218994
      },
      {
        "title": "Robust Prompt Optimization for Defending Language Models Against\n  Jailbreaking Attacks,",
        "abstract": "Despite advances in AI alignment, large language models (LLMs) remain\nvulnerable to adversarial attacks or jailbreaking, in which adversaries can\nmodify prompts to induce unwanted behavior. While some defenses have been\nproposed, they have not been adapted to newly proposed attacks and more\nchallenging threat models. To address this, we propose an optimization-based\nobjective for defending LLMs against jailbreaking attacks and an algorithm,\nRobust Prompt Optimization (RPO) to create robust system-level defenses. Our\napproach directly incorporates the adversary into the defensive objective and\noptimizes a lightweight and transferable suffix, enabling RPO to adapt to\nworst-case adaptive attacks. Our theoretical and experimental results show\nimproved robustness to both jailbreaks seen during optimization and unknown\njailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2\nto 0% on JailbreakBench, setting the state-of-the-art. Code can be found at\nhttps://github.com/lapisrocks/rpo",
        "score": 3.657717704772949
      },
      {
        "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs,",
        "abstract": "Misuse of the Large Language Models (LLMs) has raised widespread concern. To\naddress this issue, safeguards have been taken to ensure that LLMs align with\nsocial ethics. However, recent findings have revealed an unsettling\nvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By\napplying techniques, such as employing role-playing scenarios, adversarial\nexamples, or subtle subversion of safety objectives as a prompt, LLMs can\nproduce an inappropriate or even harmful response. While researchers have\nstudied several categories of jailbreak attacks, they have done so in\nisolation. To fill this gap, we present the first large-scale measurement of\nvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak\nmethods from four categories, 160 questions from 16 violation categories, and\nsix popular LLMs. Our extensive experimental results demonstrate that the\noptimized jailbreak prompts consistently achieve the highest attack success\nrates, as well as exhibit robustness across different LLMs. Some jailbreak\nprompt datasets, available from the Internet, can also achieve high attack\nsuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the\nclaims from many organizations regarding the coverage of violation categories\nin their policies, the attack success rates from these categories remain high,\nindicating the challenges of effectively aligning LLM policies and the ability\nto counter jailbreak attacks. We also discuss the trade-off between the attack\nperformance and efficiency, as well as show that the transferability of the\njailbreak prompts is still viable, becoming an option for black-box models.\nOverall, our research highlights the necessity of evaluating different\njailbreak methods. We hope our study can provide insights for future research\non jailbreak attacks and serve as a benchmark tool for evaluating them for\npractitioners.",
        "score": 3.5858519077301025
      },
      {
        "title": "SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance,",
        "abstract": "As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality.",
        "score": 3.429175615310669
      },
      {
        "title": "ObscurePrompt: Jailbreaking Large Language Models via Obscure Input,",
        "abstract": "Recently, Large Language Models (LLMs) have garnered significant attention\nfor their exceptional natural language processing capabilities. However,\nconcerns about their trustworthiness remain unresolved, particularly in\naddressing \"jailbreaking\" attacks on aligned LLMs. Previous research\npredominantly relies on scenarios with white-box LLMs or specific and fixed\nprompt templates, which are often impractical and lack broad applicability. In\nthis paper, we introduce a straightforward and novel method, named\nObscurePrompt, for jailbreaking LLMs, inspired by the observed fragile\nalignments in Out-of-Distribution (OOD) data. Specifically, we first formulate\nthe decision boundary in the jailbreaking process and then explore how obscure\ntext affects LLM's ethical decision boundary. ObscurePrompt starts with\nconstructing a base prompt that integrates well-known jailbreaking techniques.\nPowerful LLMs are then utilized to obscure the original prompt through\niterative transformations, aiming to bolster the attack's robustness.\nComprehensive experiments show that our approach substantially improves upon\nprevious methods in terms of attack effectiveness, maintaining efficacy against\ntwo prevalent defense mechanisms. We believe that our work can offer fresh\ninsights for future research on enhancing LLM alignment.",
        "score": 3.395284414291382
      },
      {
        "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware\n  Decoding,",
        "abstract": "As large language models (LLMs) become increasingly integrated into\nreal-world applications such as code generation and chatbot assistance,\nextensive efforts have been made to align LLM behavior with human values,\nincluding safety. Jailbreak attacks, aiming to provoke unintended and unsafe\nbehaviors from LLMs, remain a significant/leading LLM safety threat. In this\npaper, we aim to defend LLMs against jailbreak attacks by introducing\nSafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and\nharmless responses to user queries. Our insight in developing SafeDecoding is\nbased on the observation that, even though probabilities of tokens representing\nharmful contents outweigh those representing harmless responses, safety\ndisclaimers still appear among the top tokens after sorting tokens by\nprobability in descending order. This allows us to mitigate jailbreak attacks\nby identifying safety disclaimers and amplifying their token probabilities,\nwhile simultaneously attenuating the probabilities of token sequences that are\naligned with the objectives of jailbreak attacks. We perform extensive\nexperiments on five LLMs using six state-of-the-art jailbreak attacks and four\nbenchmark datasets. Our results show that SafeDecoding significantly reduces\nthe attack success rate and harmfulness of jailbreak attacks without\ncompromising the helpfulness of responses to benign user queries. SafeDecoding\noutperforms six defense methods.",
        "score": 3.3844518661499023
      },
      {
        "title": "Weak-to-Strong Jailbreaking on Large Language Models,",
        "abstract": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMs\nto produce harmful text. Our key intuition is based on the observation that\njailbroken and aligned models only differ in their initial decoding\ndistributions. The weak-to-strong attack's key technical insight is using two\nsmaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results show\nour method can increase the misalignment rate to over 99% on two datasets with\njust one forward pass per example. Our study exposes an urgent safety issue\nthat needs to be addressed when aligning LLMs. As an initial attempt, we\npropose a defense strategy to protect against such attacks, but creating more\nadvanced defenses remains challenging. The code for replicating the method is\navailable at https://github.com/XuandongZhao/weak-to-strong",
        "score": 3.3688430786132812
      },
      {
        "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender,",
        "abstract": "Aligning large language models (LLMs) with human values, particularly in the\nface of complex and stealthy jailbreak attacks, presents a formidable\nchallenge. In this study, we present a simple yet highly effective defense\nstrategy, i.e., Intention Analysis ($\\mathbb{IA}$). The principle behind this\nis to trigger LLMs' inherent self-correct and improve ability through a\ntwo-stage process: 1) essential intention analysis, and 2) policy-aligned\nresponse. Notably, $\\mathbb{IA}$ is an inference-only method, thus could\nenhance the safety of LLMs without compromising their helpfulness. Extensive\nexperiments on varying jailbreak benchmarks across ChatGLM, LLaMA2, Vicuna,\nMPT, DeepSeek, and GPT-3.5 show that $\\mathbb{IA}$ could consistently and\nsignificantly reduce the harmfulness in responses (averagely -53.1% attack\nsuccess rate) and maintain the general helpfulness. Encouragingly, with the\nhelp of our $\\mathbb{IA}$, Vicuna-7B even outperforms GPT-3.5 in terms of\nattack success rate. Further analyses present some insights into how our method\nworks. To facilitate reproducibility, we release our code and scripts at:\nhttps://github.com/alphadl/SafeLLM_with_IntentionAnalysis.",
        "score": 3.231773853302002
      },
      {
        "title": "Defending LLMs against Jailbreaking Attacks via Backtranslation,",
        "abstract": "Although many large language models (LLMs) have been trained to refuse\nharmful requests, they are still vulnerable to jailbreaking attacks which\nrewrite the original prompt to conceal its harmful intent. In this paper, we\npropose a new method for defending LLMs against jailbreaking attacks by\n``backtranslation''. Specifically, given an initial response generated by the\ntarget LLM from an input prompt, our backtranslation prompts a language model\nto infer an input prompt that can lead to the response. The inferred prompt is\ncalled the backtranslated prompt which tends to reveal the actual intent of the\noriginal prompt, since it is generated based on the LLM's response and not\ndirectly manipulated by the attacker. We then run the target LLM again on the\nbacktranslated prompt, and we refuse the original prompt if the model refuses\nthe backtranslated prompt. We explain that the proposed defense provides\nseveral benefits on its effectiveness and efficiency. We empirically\ndemonstrate that our defense significantly outperforms the baselines, in the\ncases that are hard for the baselines, and our defense also has little impact\non the generation quality for benign input prompts. Our implementation is based\non our library for LLM jailbreaking defense algorithms at\n\\url{https://github.com/YihanWang617/llm-jailbreaking-defense}, and the code\nfor reproducing our experiments is available at\n\\url{https://github.com/YihanWang617/LLM-Jailbreaking-Defense-Backtranslation}.",
        "score": 3.2197914123535156
      },
      {
        "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded\n  Logical Thinking,",
        "abstract": "While large language models (LLMs) have demonstrated increasing power, they\nhave also given rise to a wide range of harmful behaviors. As representatives,\njailbreak attacks can provoke harmful or unethical responses from LLMs, even\nafter safety alignment. In this paper, we investigate a novel category of\njailbreak attacks specifically designed to target the cognitive structure and\nprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs in\nthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)\neffect-to-cause reasoning. Different from previous jailbreak attacks, our\nproposed cognitive overload is a black-box attack with no need for knowledge of\nmodel architecture or access to model weights. Experiments conducted on\nAdvBench and MasterKey reveal that various LLMs, including both popular\nopen-source model Llama 2 and the proprietary model ChatGPT, can be compromised\nthrough cognitive overload. Motivated by cognitive psychology work on managing\ncognitive load, we further investigate defending cognitive overload attack from\ntwo perspectives. Empirical studies show that our cognitive overload from three\nperspectives can jailbreak all studied LLMs successfully, while existing\ndefense strategies can hardly mitigate the caused malicious uses effectively.",
        "score": 3.163435459136963
      },
      {
        "title": "Improved Techniques for Optimization-Based Jailbreaking on Large\n  Language Models,",
        "abstract": "Large language models (LLMs) are being rapidly developed, and a key component\nof their widespread deployment is their safety-related alignment. Many\nred-teaming efforts aim to jailbreak LLMs, where among these efforts, the\nGreedy Coordinate Gradient (GCG) attack's success has led to a growing interest\nin the study of optimization-based jailbreaking techniques. Although GCG is a\nsignificant milestone, its attacking efficiency remains unsatisfactory. In this\npaper, we present several improved (empirical) techniques for\noptimization-based jailbreaks like GCG. We first observe that the single target\ntemplate of \"Sure\" largely limits the attacking performance of GCG; given this,\nwe propose to apply diverse target templates containing harmful self-suggestion\nand/or guidance to mislead LLMs. Besides, from the optimization aspects, we\npropose an automatic multi-coordinate updating strategy in GCG (i.e.,\nadaptively deciding how many tokens to replace in each step) to accelerate\nconvergence, as well as tricks like easy-to-hard initialisation. Then, we\ncombine these improved technologies to develop an efficient jailbreak method,\ndubbed I-GCG. In our experiments, we evaluate on a series of benchmarks (such\nas NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved\ntechniques can help GCG outperform state-of-the-art jailbreaking attacks and\nachieve nearly 100% attack success rate. The code is released at\nhttps://github.com/jiaxiaojunQAQ/I-GCG.",
        "score": 3.0827701091766357
      },
      {
        "title": "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation,",
        "abstract": "Research on jailbreaking has been valuable for testing and understanding the\nsafety and security issues of large language models (LLMs). In this paper, we\nintroduce Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach\nthat leverages the reflective capabilities of LLMs for jailbreaking with only\nblack-box access. Unlike previous methods, IRIS simplifies the jailbreaking\nprocess by using a single model as both the attacker and target. This method\nfirst iteratively refines adversarial prompts through self-explanation, which\nis crucial for ensuring that even well-aligned LLMs obey adversarial\ninstructions. IRIS then rates and enhances the output given the refined prompt\nto increase its harmfulness. We find IRIS achieves jailbreak success rates of\n98% on GPT-4 and 92% on GPT-4 Turbo in under 7 queries. It significantly\noutperforms prior approaches in automatic, black-box and interpretable\njailbreaking, while requiring substantially fewer queries, thereby establishing\na new standard for interpretable jailbreaking methods.",
        "score": 3.0786283016204834
      },
      {
        "title": "Subtoxic Questions: Dive Into Attitude Change of LLM's Response in\n  Jailbreak Attempts,",
        "abstract": "As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and\nmore attention, it is of great significance to raise a generalized research\nparadigm to evaluate attack strengths and a basic model to conduct subtler\nexperiments. In this paper, we propose a novel approach by focusing on a set of\ntarget questions that are inherently more sensitive to jailbreak prompts,\naiming to circumvent the limitations posed by enhanced LLM security. Through\ndesigning and analyzing these sensitive questions, this paper reveals a more\neffective method of identifying vulnerabilities in LLMs, thereby contributing\nto the advancement of LLM security. This research not only challenges existing\njailbreaking methodologies but also fortifies LLMs against potential exploits.",
        "score": 2.9784538745880127
      },
      {
        "title": "Defending Large Language Models against Jailbreak Attacks via Semantic\n  Smoothing,",
        "abstract": "Aligned large language models (LLMs) are vulnerable to jailbreaking attacks,\nwhich bypass the safeguards of targeted LLMs and fool them into generating\nobjectionable content. While initial defenses show promise against token-based\nthreat models, there do not exist defenses that provide robustness against\nsemantic attacks and avoid unfavorable trade-offs between robustness and\nnominal performance. To meet this need, we propose SEMANTICSMOOTH, a\nsmoothing-based defense that aggregates the predictions of multiple\nsemantically transformed copies of a given input prompt. Experimental results\ndemonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against\nGCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on\ninstruction following benchmarks such as InstructionFollowing and AlpacaEval.\nThe codes will be publicly available at\nhttps://github.com/UCSB-NLP-Chang/SemanticSmooth.",
        "score": 2.9775655269622803
      },
      {
        "title": "All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks,",
        "abstract": "Large Language Models (LLMs), such as ChatGPT, encounter `jailbreak'\nchallenges, wherein safeguards are circumvented to generate ethically harmful\nprompts. This study introduces a straightforward black-box method for\nefficiently crafting jailbreak prompts, addressing the significant complexity\nand computational costs associated with conventional methods. Our technique\niteratively transforms harmful prompts into benign expressions directly\nutilizing the target LLM, predicated on the hypothesis that LLMs can\nautonomously generate expressions that evade safeguards. Through experiments\nconducted with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, our method\nconsistently achieved an attack success rate exceeding 80% within an average of\nfive iterations for forbidden questions and proved robust against model\nupdates. The jailbreak prompts generated were not only naturally-worded and\nsuccinct but also challenging to defend against. These findings suggest that\nthe creation of effective jailbreak prompts is less complex than previously\nbelieved, underscoring the heightened risk posed by black-box jailbreak\nattacks.",
        "score": 2.9743316173553467
      },
      {
        "title": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs,",
        "abstract": "Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we evaluate the impact of\nvarious attack settings on LLM performance and provide a baseline benchmark for\njailbreak attacks, encouraging the adoption of a standardized evaluation\nframework. Specifically, we evaluate the eight key factors of implementing\njailbreak attacks on LLMs from both target-level and attack-level perspectives.\nWe further conduct seven representative jailbreak attacks on six defense\nmethods across two widely used datasets, encompassing approximately 320\nexperiments with about 50,000 GPU hours on A800-80G. Our experimental results\nhighlight the need for standardized benchmarking to evaluate these attacks on\ndefense-enhanced LLMs. Our code is available at\nhttps://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking.",
        "score": 2.9614431858062744
      },
      {
        "title": "Protecting Your LLMs with Information Bottleneck,",
        "abstract": "The advent of large language models (LLMs) has revolutionized the field of\nnatural language processing, yet they might be attacked to produce harmful\ncontent. Despite efforts to ethically align LLMs, these are often fragile and\ncan be circumvented by jailbreaking attacks through optimized or manual\nadversarial prompts. To address this, we introduce the Information Bottleneck\nProtector (IBProtector), a defense mechanism grounded in the information\nbottleneck principle, and we modify the objective to avoid trivial solutions.\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a\nlightweight and trainable extractor, preserving only essential information for\nthe target LLMs to respond with the expected answer. Moreover, we further\nconsider a situation where the gradient is not visible to be compatible with\nany LLM. Our empirical evaluations show that IBProtector outperforms current\ndefense methods in mitigating jailbreak attempts, without overly affecting\nresponse quality or inference speed. Its effectiveness and adaptability across\nvarious attack methods and target LLMs underscore the potential of IBProtector\nas a novel, transferable defense that bolsters the security of LLMs without\nrequiring modifications to the underlying models.",
        "score": 2.9400107860565186
      },
      {
        "title": "AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on\n  Large Language Models,",
        "abstract": "In our research, we pioneer a novel approach to evaluate the effectiveness of\njailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2,\ndiverging from traditional robustness-focused binary evaluations. Our study\nintroduces two distinct evaluation frameworks: a coarse-grained evaluation and\na fine-grained evaluation. Each framework, using a scoring range from 0 to 1,\noffers a unique perspective, enabling a more comprehensive and nuanced\nevaluation of attack effectiveness and empowering attackers to refine their\nattack prompts with greater understanding. Furthermore, we have developed a\ncomprehensive ground truth dataset specifically tailored for jailbreak tasks.\nThis dataset not only serves as a crucial benchmark for our current study but\nalso establishes a foundational resource for future research, enabling\nconsistent and comparative analyses in this evolving field. Upon meticulous\ncomparison with traditional evaluation methods, we discovered that our\nevaluation aligns with the baseline's trend while offering a more profound and\ndetailed assessment. We believe that by accurately evaluating the effectiveness\nof attack prompts in the Jailbreak task, our work lays a solid foundation for\nassessing a wider array of similar or even more complex tasks in the realm of\nprompt injection, potentially revolutionizing this field.",
        "score": 2.8734426498413086
      },
      {
        "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking\n  Attacks,",
        "abstract": "Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As\ndefense mechanisms evolve, directly obtaining harmful information becomes\nincreasingly challenging for Jailbreaking attacks. In this work, inspired by\nhuman practices of indirect context to elicit harmful information, we focus on\na new attack form called Contextual Interaction Attack. The idea relies on the\nautoregressive nature of the generation process in LLMs. We contend that the\nprior context--the information preceding the attack query--plays a pivotal role\nin enabling potent Jailbreaking attacks. Specifically, we propose an approach\nthat leverages preliminary question-answer pairs to interact with the LLM. By\ndoing so, we guide the responses of the model toward revealing the 'desired'\nharmful information. We conduct experiments on four different LLMs and\ndemonstrate the efficacy of this attack, which is black-box and can also\ntransfer across LLMs. We believe this can lead to further developments and\nunderstanding of the context vector in LLMs.",
        "score": 2.7329213619232178
      },
      {
        "title": "AutoBreach: Universal and Adaptive Jailbreaking with Efficient\n  Wordplay-Guided Optimization,",
        "abstract": "Despite the widespread application of large language models (LLMs) across\nvarious tasks, recent studies indicate that they are susceptible to jailbreak\nattacks, which can render their defense mechanisms ineffective. However,\nprevious jailbreak research has frequently been constrained by limited\nuniversality, suboptimal efficiency, and a reliance on manual crafting. In\nresponse, we rethink the approach to jailbreaking LLMs and formally define\nthree essential properties from the attacker' s perspective, which contributes\nto guiding the design of jailbreak methods. We further introduce AutoBreach, a\nnovel method for jailbreaking LLMs that requires only black-box access.\nInspired by the versatility of wordplay, AutoBreach employs a wordplay-guided\nmapping rule sampling strategy to generate a variety of universal mapping rules\nfor creating adversarial prompts. This generation process leverages LLMs'\nautomatic summarization and reasoning capabilities, thus alleviating the manual\nburden. To boost jailbreak success rates, we further suggest sentence\ncompression and chain-of-thought-based mapping rules to correct errors and\nwordplay misinterpretations in target LLMs. Additionally, we propose a\ntwo-stage mapping rule optimization strategy that initially optimizes mapping\nrules before querying target LLMs to enhance the efficiency of AutoBreach.\nAutoBreach can efficiently identify security vulnerabilities across various\nLLMs, including three proprietary models: Claude-3, GPT-3.5, GPT-4 Turbo, and\ntwo LLMs' web platforms: Bingchat, GPT-4 Web, achieving an average success rate\nof over 80% with fewer than 10 queries",
        "score": 2.7229840755462646
      },
      {
        "title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs\n  Without Fine-Tuning,",
        "abstract": "Large Language Models (LLMs) are susceptible to `jailbreaking' prompts, which\ncan induce the generation of harmful content. This paper demonstrates that\nmoderate WANDA pruning (Sun et al., 2023) can increase their resistance to such\nattacks without the need for fine-tuning, while maintaining performance on\nstandard benchmarks. Our findings suggest that the benefits of pruning\ncorrelate with the initial safety levels of the model, indicating a\nregularizing effect of WANDA pruning. We introduce a dataset of 225 harmful\ntasks across five categories to systematically evaluate this safety\nenhancement. We argue that safety improvements can be understood through a\nregularization perspective. First, we show that pruning helps LLMs focus more\neffectively on task-relevant tokens within jailbreaking prompts. Then, we\nanalyze the effects of pruning on the perplexity of malicious prompts before\nand after their integration into jailbreak templates. Finally, we demonstrate\nstatistically significant performance improvements under domain shifts when\napplying WANDA to linear models.",
        "score": 2.7188398838043213
      },
      {
        "title": "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space\n  Analysis,",
        "abstract": "Large language models (LLMs) are susceptible to a type of attack known as\njailbreaking, which misleads LLMs to output harmful contents. Although there\nare diverse jailbreak attack strategies, there is no unified understanding on\nwhy some methods succeed and others fail. This paper explores the behavior of\nharmful and harmless prompts in the LLM's representation space to investigate\nthe intrinsic properties of successful jailbreak attacks. We hypothesize that\nsuccessful attacks share some similar properties: They are effective in moving\nthe representation of the harmful prompt towards the direction to the harmless\nprompts. We leverage hidden representations into the objective of existing\njailbreak attacks to move the attacks along the acceptance direction, and\nconduct experiments to validate the above hypothesis using the proposed\nobjective. We hope this study provides new insights into understanding how LLMs\nunderstand harmfulness information.",
        "score": 2.5444250106811523
      },
      {
        "title": "Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks\n  with Self-Refinement,",
        "abstract": "Caution: This paper includes offensive words that could potentially cause\nunpleasantness. Language models (LMs) are vulnerable to exploitation for\nadversarial misuse. Training LMs for safety alignment is extensive and makes it\nhard to respond to fast-developing attacks immediately, such as jailbreaks. We\npropose self-refine with formatting that achieves outstanding safety even in\nnon-safety-aligned LMs and evaluate our method alongside several defense\nbaselines, demonstrating that it is the safest training-free method against\njailbreak attacks. Additionally, we proposed a formatting method that improves\nthe efficiency of the self-refine process while reducing attack success rates\nin fewer iterations. We've also observed that non-safety-aligned LMs outperform\nsafety-aligned LMs in safety tasks by giving more helpful and safe responses.\nIn conclusion, our findings can achieve less safety risk with fewer\ncomputational costs, allowing non-safety LM to be easily utilized in real-world\nservice.",
        "score": 1.7983490228652954
      },
      {
        "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically,",
        "abstract": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thought reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries.\nInterestingly, TAP is also capable of jailbreaking LLMs protected by\nstate-of-the-art guardrails, e.g., LlamaGuard. This significantly improves upon\nthe previous state-of-the-art black-box method for generating jailbreaks.",
        "score": 1.5973787307739258
      },
      {
        "title": "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)\n  Safer Language Models,",
        "abstract": "We introduce WildTeaming, an automatic LLM safety red-teaming framework that\nmines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of\nnovel jailbreak tactics, and then composes multiple tactics for systematic\nexploration of novel jailbreaks. Compared to prior work that performed\nred-teaming via recruited human workers, gradient-based optimization, or\niterative revision with LLMs, our work investigates jailbreaks from chatbot\nusers who were not specifically instructed to break the system. WildTeaming\nreveals previously unidentified vulnerabilities of frontier LLMs, resulting in\nup to 4.6x more diverse and successful adversarial attacks compared to\nstate-of-the-art jailbreak methods.\n  While many datasets exist for jailbreak evaluation, very few open-source\ndatasets exist for jailbreak training, as safety training data has been closed\neven when model weights are open. With WildTeaming we create WildJailbreak, a\nlarge-scale open-source synthetic safety dataset with 262K vanilla (direct\nrequest) and adversarial (complex jailbreak) prompt-response pairs. To mitigate\nexaggerated safety behaviors, WildJailbreak provides two contrastive types of\nqueries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that\nresemble harmful queries in form but contain no harm. As WildJailbreak\nconsiderably upgrades the quality and scale of existing safety resources, it\nuniquely enables us to examine the scaling effects of data and the interplay of\ndata properties and model capabilities during safety training. Through\nextensive experiments, we identify the training properties that enable an ideal\nbalance of safety behaviors: appropriate safeguarding without over-refusal,\neffective handling of vanilla and adversarial queries, and minimal, if any,\ndecrease in general capabilities. All components of WildJailbeak contribute to\nachieving balanced safety behaviors of models.",
        "score": 1.4732356071472168
      },
      {
        "title": "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to\n  Defend Against Jailbreak Attacks,",
        "abstract": "LLMs are known to be vulnerable to jailbreak attacks, even after safety\nalignment. An important observation is that, while different types of jailbreak\nattacks can generate significantly different queries, they mostly result in\nsimilar responses that are rooted in the same harmful knowledge (e.g., detailed\nsteps to make a bomb). Therefore, we conjecture that directly unlearn the\nharmful knowledge in the LLM can be a more effective way to defend against\njailbreak attacks than the mainstream supervised fine-tuning (SFT) based\napproaches. Our extensive experiments confirmed our insight and suggested\nsurprising generalizability of our unlearning-based approach: using only 20 raw\nharmful questions \\emph{without} any jailbreak prompt during training, our\nsolution reduced the Attack Success Rate (ASR) in Vicuna-7B on\n\\emph{out-of-distribution} (OOD) harmful questions wrapped with various complex\njailbreak prompts from 82.6\\% to 7.7\\%. This significantly outperforms\nLlama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment samples but\nstill has an ASR of 21.9\\% even under the help of an additional safety system\nprompt. Further analysis reveals that the generalization ability of our\nsolution stems from the intrinsic relatedness among harmful responses across\nharmful questions (e.g., response patterns, shared steps and actions, and\nsimilarity among their learned representations in the LLM). Our code is\navailable at \\url{https://github.com/thu-coai/SafeUnlearning}.",
        "score": 1.273938536643982
      }
    ]
  },
  {
    "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large\n  Language Models",
    "abstract": "  Although Large Language Models (LLMs) demonstrate remarkable ability in\nprocessing and generating human-like text, they do have limitations when it\ncomes to comprehending and expressing world knowledge that extends beyond the\nboundaries of natural language(e.g., chemical molecular formula). Injecting a\ncollection of symbolic data directly into the training of LLMs can be\nproblematic, as it disregards the synergies among different symbolic families\nand overlooks the need for a balanced mixture of natural and symbolic data. In\nthis work, we tackle these challenges from both a data and framework\nperspective and introduce Symbol-LLM series models. First, we curated a data\ncollection consisting of 34 tasks and incorporating approximately 20 distinct\nsymbolic families, intending to capture the interrelations and foster synergies\nbetween symbols. Then, a two-stage tuning framework succeeds in injecting\nsymbolic knowledge without loss of the generality ability. Extensive\nexperiments on both symbol- and NL-centric tasks demonstrate the balanced and\nsuperior performances of Symbol-LLM series models. The project page is\nhttps://xufangzhi.github.io/symbol-llm-page/.\n",
    "related_paper_titles": [],
    "related_paper_abstract": [],
    "entities": [
      "LLMs",
      "Large Language Models",
      "LLM",
      "RAG",
      "Large Language Models Large Language Models",
      "Large",
      "ChatGPT",
      "CoT",
      "Language Models",
      "Mistral",
      "Large Language",
      "ICL",
      "DPO",
      "NLP",
      "GPT",
      "KGs",
      "SFT",
      "LLaMA",
      "OpenAI",
      "IFT",
      "MLLMs",
      "Retrieval Augmented",
      "Knowledge Graphs",
      "RLHF",
      "Human Feedback",
      "Llama",
      "Chinese",
      "SLMs",
      "Gemini",
      "Direct Preference"
    ],
    "retrieved_papers": [
      {
        "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large\n  Language Models,",
        "abstract": "Although Large Language Models (LLMs) demonstrate remarkable ability in\nprocessing and generating human-like text, they do have limitations when it\ncomes to comprehending and expressing world knowledge that extends beyond the\nboundaries of natural language(e.g., chemical molecular formula). Injecting a\ncollection of symbolic data directly into the training of LLMs can be\nproblematic, as it disregards the synergies among different symbolic families\nand overlooks the need for a balanced mixture of natural and symbolic data. In\nthis work, we tackle these challenges from both a data and framework\nperspective and introduce Symbol-LLM series models. First, we curated a data\ncollection consisting of 34 tasks and incorporating approximately 20 distinct\nsymbolic families, intending to capture the interrelations and foster synergies\nbetween symbols. Then, a two-stage tuning framework succeeds in injecting\nsymbolic knowledge without loss of the generality ability. Extensive\nexperiments on both symbol- and NL-centric tasks demonstrate the balanced and\nsuperior performances of Symbol-LLM series models. The project page is\nhttps://xufangzhi.github.io/symbol-llm-page/.",
        "score": 5.761084079742432
      },
      {
        "title": "Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language\n  Conversion for Language Models,",
        "abstract": "Symbols (or more broadly, non-natural language textual representations) such\nas numerical sequences, molecular formulas, and table delimiters widely exist,\nplaying important roles in various tasks such as abstract reasoning, chemical\nproperty prediction, and table question answering. Despite the impressive\nnatural language comprehension capabilities of large language models (LLMs),\ntheir reasoning abilities for symbols remain inadequate, which could attributed\nto the difference between symbol representations and general natural languages.\nWe propose symbol-to-language (S2L), a tuning-free method that enables large\nlanguage models to solve symbol-related problems with information expressed in\nnatural language. Specifically, S2L first converts the symbols involved to\nlanguage-based representations, which can be implemented by prompting LLMs or\nleveraging external tools, then these language-based representations are\nintegrated into the original problem via direct substitution or concatenation,\nserving as useful input information for LLMs. We evaluate the S2L method using\nboth API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight\nsymbol-related tasks, ranging from symbol-only abstract reasoning to sentiment\nanalysis in social media. Experimental results show that S2L consistently leads\nto superior performance. For example, by employing S2L for GPT-4, there can be\naverage significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and\nDyck language, respectively. Codes and data are available at\nhttps://github.com/THUNLP-MT/symbol2language.",
        "score": 2.998669147491455
      },
      {
        "title": "Scientific Large Language Models: A Survey on Biological & Chemical\n  Domains,",
        "abstract": "Large Language Models (LLMs) have emerged as a transformative power in\nenhancing natural language comprehension, representing a significant stride\ntoward artificial general intelligence. The application of LLMs extends beyond\nconventional linguistic boundaries, encompassing specialized linguistic systems\ndeveloped within various scientific disciplines. This growing interest has led\nto the advent of scientific LLMs, a novel subclass specifically engineered for\nfacilitating scientific discovery. As a burgeoning area in the community of AI\nfor Science, scientific LLMs warrant comprehensive exploration. However, a\nsystematic and up-to-date survey introducing them is currently lacking. In this\npaper, we endeavor to methodically delineate the concept of \"scientific\nlanguage\", whilst providing a thorough review of the latest advancements in\nscientific LLMs. Given the expansive realm of scientific disciplines, our\nanalysis adopts a focused lens, concentrating on the biological and chemical\ndomains. This includes an in-depth examination of LLMs for textual knowledge,\nsmall molecules, macromolecular proteins, genomic sequences, and their\ncombinations, analyzing them in terms of model architectures, capabilities,\ndatasets, and evaluation. Finally, we critically examine the prevailing\nchallenges and point out promising research directions along with the advances\nof LLMs. By offering a comprehensive overview of technical developments in this\nfield, this survey aspires to be an invaluable resource for researchers\nnavigating the intricate landscape of scientific LLMs.",
        "score": 2.7121598720550537
      },
      {
        "title": "Efficient Large Language Models: A Survey,",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nimportant tasks such as natural language understanding and language generation,\nand thus have the potential to make a substantial impact on our society. Such\ncapabilities, however, come with the considerable resources they demand,\nhighlighting the strong need to develop effective techniques for addressing\ntheir efficiency challenges. In this survey, we provide a systematic and\ncomprehensive review of efficient LLMs research. We organize the literature in\na taxonomy consisting of three main categories, covering distinct yet\ninterconnected efficient LLMs topics from model-centric, data-centric, and\nframework-centric perspective, respectively. We have also created a GitHub\nrepository where we organize the papers featured in this survey at\nhttps://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively\nmaintain the repository and incorporate new research as it emerges. We hope our\nsurvey can serve as a valuable resource to help researchers and practitioners\ngain a systematic understanding of efficient LLMs research and inspire them to\ncontribute to this important and exciting field.",
        "score": 2.570204496383667
      },
      {
        "title": "Feedback-aligned Mixed LLMs for Machine Language-Molecule Translation,",
        "abstract": "The intersection of chemistry and Artificial Intelligence (AI) is an active\narea of research focused on accelerating scientific discovery. While using\nlarge language models (LLMs) with scientific modalities has shown potential,\nthere are significant challenges to address, such as improving training\nefficiency and dealing with the out-of-distribution problem. Focussing on the\ntask of automated language-molecule translation, we are the first to use\nstate-of-the art (SOTA) human-centric optimisation algorithms in the\ncross-modal setting, successfully aligning cross-language-molecule modals. We\nempirically show that we can augment the capabilities of scientific LLMs\nwithout the need for extensive data or large models. We conduct experiments\nusing only 10% of the available data to mitigate memorisation effects\nassociated with training large models on extensive datasets. We achieve\nsignificant performance gains, surpassing the best benchmark model trained on\nextensive in-distribution data by a large margin and reach new SOTA levels.\nAdditionally we are the first to propose employing non-linear fusion for mixing\ncross-modal LLMs which further boosts performance gains without increasing\ntraining costs or data needs. Finally, we introduce a fine-grained,\ndomain-agnostic evaluation method to assess hallucination in LLMs and promote\nresponsible use.",
        "score": 2.4454588890075684
      },
      {
        "title": "Large Knowledge Model: Perspectives and Challenges,",
        "abstract": "Humankind's understanding of the world is fundamentally linked to our\nperception and cognition, with \\emph{human languages} serving as one of the\nmajor carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language\nModels} (LLMs) like ChatGPT epitomize the pre-training of extensive,\nsequence-based world knowledge into neural networks, facilitating the\nprocessing and manipulation of this knowledge in a parametric space. This\narticle explores large models through the lens of \"knowledge\". We initially\ninvestigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in\nenhancing LLMs, covering aspects like knowledge-augmented language model,\nstructure-inducing pre-training, knowledgeable prompts, structured CoT,\nknowledge editing, semantic tools for LLM and knowledgeable AI agents.\nSubsequently, we examine how LLMs can boost traditional symbolic knowledge\nbases, encompassing aspects like using LLM as KG builder and controller,\nstructured knowledge pretraining, and LLM-enhanced symbolic reasoning.\nConsidering the intricate nature of human knowledge, we advocate for the\ncreation of \\emph{Large Knowledge Models} (LKM), specifically engineered to\nmanage diversified spectrum of knowledge structures. This promising undertaking\nwould entail several key challenges, such as disentangling knowledge base from\nlanguage models, cognitive alignment with human knowledge, integration of\nperception and cognition, and building large commonsense models for interacting\nwith physical world, among others. We finally propose a five-\"A\" principle to\ndistinguish the concept of LKM.",
        "score": 2.3170080184936523
      },
      {
        "title": "Large Language Models are Interpretable Learners,",
        "abstract": "The trade-off between expressiveness and interpretability remains a core\nchallenge when building human-centric predictive models for classification and\ndecision-making. While symbolic rules offer interpretability, they often lack\nexpressiveness, whereas neural networks excel in performance but are known for\nbeing black boxes. In this paper, we show a combination of Large Language\nModels (LLMs) and symbolic programs can bridge this gap. In the proposed\nLLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language\nprompts provides a massive set of interpretable modules that can transform raw\ninput into natural language concepts. Symbolic programs then integrate these\nmodules into an interpretable decision rule. To train LSPs, we develop a\ndivide-and-conquer approach to incrementally build the program from scratch,\nwhere the learning process of each step is guided by LLMs. To evaluate the\neffectiveness of LSPs in extracting interpretable and accurate knowledge from\ndata, we introduce IL-Bench, a collection of diverse tasks, including both\nsynthetic and real-world scenarios across different modalities. Empirical\nresults demonstrate LSP's superior performance compared to traditional\nneurosymbolic programs and vanilla automatic prompt tuning methods. Moreover,\nas the knowledge learned by LSP is a combination of natural language\ndescriptions and symbolic rules, it is easily transferable to humans\n(interpretable), and other LLMs, and generalizes well to out-of-distribution\nsamples.",
        "score": 2.2593443393707275
      },
      {
        "title": "MolX: Enhancing Large Language Models for Molecular Learning with A\n  Multi-Modal Extension,",
        "abstract": "Recently, Large Language Models (LLMs) with their strong task-handling\ncapabilities have shown remarkable advancements across a spectrum of fields,\nmoving beyond natural language understanding. However, their proficiency within\nthe chemistry domain remains restricted, especially in solving professional\nmolecule-related tasks. This challenge is attributed to their inherent\nlimitations in comprehending molecules using only common textual\nrepresentations, i.e., SMILES strings. In this study, we seek to enhance the\nability of LLMs to comprehend molecules by designing and equipping them with a\nmulti-modal external module, namely MolX. In particular, instead of directly\nusing a SMILES string to represent a molecule, we utilize specific encoders to\nextract fine-grained features from both SMILES string and 2D molecular graph\nrepresentations for feeding into an LLM. Moreover, a human-defined molecular\nfingerprint is incorporated to leverage its embedded domain knowledge. Then, to\nestablish an alignment between MolX and the LLM's textual input space, the\nwhole model in which the LLM is frozen, is pre-trained with a versatile\nstrategy including a diverse set of tasks. Extensive experimental evaluations\ndemonstrate that our proposed method only introduces a small number of\ntrainable parameters while outperforming baselines on various downstream\nmolecule-related tasks ranging from molecule-to-text translation to\nretrosynthesis, with and without fine-tuning the LLM.",
        "score": 2.157468557357788
      },
      {
        "title": "Interactive Evolution: A Neural-Symbolic Self-Training Framework For\n  Large Language Models,",
        "abstract": "One of the primary driving forces contributing to the superior performance of\nLarge Language Models (LLMs) is the extensive availability of human-annotated\nnatural language data, which is used for alignment fine-tuning. This inspired\nresearchers to investigate self-training methods to mitigate the extensive\nreliance on human annotations. However, the current success of self-training\nhas been primarily observed in natural language scenarios, rather than in the\nincreasingly important neural-symbolic scenarios. To this end, we propose an\nenvironment-guided neural-symbolic self-training framework named ENVISIONS. It\naims to overcome two main challenges: (1) the scarcity of symbolic data, and\n(2) the limited proficiency of LLMs in processing symbolic language. Extensive\nevaluations conducted on three distinct domains demonstrate the effectiveness\nof our approach. Additionally, we have conducted a comprehensive analysis to\nuncover the factors contributing to ENVISIONS's success, thereby offering\nvaluable insights for future research in this area. Code will be available at\n\\url{https://github.com/xufangzhi/ENVISIONS}.",
        "score": 2.059689521789551
      },
      {
        "title": "Large Language Models: A Survey,",
        "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs' ability of general-purpose language\nunderstanding and generation is acquired by training billions of model's\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.",
        "score": 2.0146026611328125
      },
      {
        "title": "VANER: Leveraging Large Language Model for Versatile and Adaptive\n  Biomedical Named Entity Recognition,",
        "abstract": "Prevalent solution for BioNER involves using representation learning\ntechniques coupled with sequence labeling. However, such methods are inherently\ntask-specific, demonstrate poor generalizability, and often require dedicated\nmodel for each dataset. To leverage the versatile capabilities of recently\nremarkable large language models (LLMs), several endeavors have explored\ngenerative approaches to entity extraction. Yet, these approaches often fall\nshort of the effectiveness of previouly sequence labeling approaches. In this\npaper, we utilize the open-sourced LLM LLaMA2 as the backbone model, and design\nspecific instructions to distinguish between different types of entities and\ndatasets. By combining the LLM's understanding of instructions with sequence\nlabeling techniques, we use mix of datasets to train a model capable of\nextracting various types of entities. Given that the backbone LLMs lacks\nspecialized medical knowledge, we also integrate external entity knowledge\nbases and employ instruction tuning to compel the model to densely recognize\ncarefully curated entities. Our model VANER, trained with a small partition of\nparameters, significantly outperforms previous LLMs-based models and, for the\nfirst time, as a model based on LLM, surpasses the majority of conventional\nstate-of-the-art BioNER systems, achieving the highest F1 scores across three\ndatasets.",
        "score": 1.9397075176239014
      },
      {
        "title": "ChemDFM: Dialogue Foundation Model for Chemistry,",
        "abstract": "Large language models (LLMs) have established great success in the general\ndomain of natural language processing. Their emerging task generalization and\nfree-form dialogue capabilities can greatly help to design Chemical General\nIntelligence (CGI) to assist real-world research in chemistry. However, the\nexistence of specialized language and knowledge in the field of chemistry, such\nas the highly informative SMILES notation, hinders the performance of\ngeneral-domain LLMs in chemistry. To this end, we develop ChemDFM, the first\nLLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature,\ntextbooks, and instructions as well as various data from the general domain.\nTherefore, it can store, understand, and reason over chemical knowledge and\nlanguages while still possessing advanced free-form language comprehension\ncapabilities. Extensive quantitative evaluation shows that ChemDFM can\nsignificantly outperform the representative open-sourced LLMs. Moreover,\nChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite\nthe significant size difference. Further qualitative evaluations demonstrate\nthe efficiency and effectiveness of ChemDFM in real-world research scenarios.\nWe will open-source the ChemDFM model soon.",
        "score": 1.8527648448944092
      },
      {
        "title": "Limitations of Language Models in Arithmetic and Symbolic Induction,",
        "abstract": "Recent work has shown that large pretrained Language Models (LMs) can not\nonly perform remarkably well on a range of Natural Language Processing (NLP)\ntasks but also start improving on reasoning tasks such as arithmetic induction,\nsymbolic manipulation, and commonsense reasoning with increasing size of\nmodels. However, it is still unclear what the underlying capabilities of these\nLMs are. Surprisingly, we find that these models have limitations on certain\nbasic symbolic manipulation tasks such as copy, reverse, and addition. When the\ntotal number of symbols or repeating symbols increases, the model performance\ndrops quickly. We investigate the potential causes behind this phenomenon and\nexamine a set of possible methods, including explicit positional markers,\nfine-grained computation steps, and LMs with callable programs. Experimental\nresults show that none of these techniques can solve the simplest addition\ninduction problem completely. In the end, we introduce LMs with tutor, which\ndemonstrates every single step of teaching. LMs with tutor is able to deliver\n100% accuracy in situations of OOD and repeating symbols, shedding new insights\non the boundary of large LMs in induction.",
        "score": 1.5402990579605103
      },
      {
        "title": "Generating Data for Symbolic Language with Large Language Models,",
        "abstract": "While large language models (LLMs) bring not only performance but also\ncomplexity, recent work has started to turn LLMs into data generators rather\nthan task inferencers, where another affordable task model is trained for\nefficient deployment and inference. However, such an approach has primarily\nbeen applied to natural language tasks and has not yet been explored for\nsymbolic language tasks with complex structured outputs (e.g., semantic parsing\nand code generation). In this paper, we propose SymGen which utilizes LLMs for\ngenerating various annotation-expensive symbolic language data. SymGen consists\nof an informative prompt to steer generation and an agreement-based verifier to\nimprove data correctness. We conduct extensive experiments on six symbolic\nlanguage tasks across various settings. Compared with the LLMs, we demonstrate\nthe 1\\%-sized task model can achieve comparable or better performance, largely\ncutting inference and deployment costs. We also show that generated data with\nonly a few human demonstrations can be as effective as over 10 times the amount\nof human-annotated data when training the task model, saving a considerable\namount of annotation effort. SymGen sheds new light on data generation for\ncomplex tasks, and we release the code at\n\\href{https://github.com/HKUNLP/SymGen}{https://github.com/HKUNLP/SymGen}.",
        "score": 1.4720816612243652
      },
      {
        "title": "What can Large Language Models do in chemistry? A comprehensive\n  benchmark on eight tasks,",
        "abstract": "Large Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been applied in various kinds of areas\nsuch as science, finance and software engineering. However, the capability of\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\nLLMs in a wide range of tasks across the chemistry domain. We identify three\nkey chemistry-related capabilities including understanding, reasoning and\nexplaining to explore in LLMs and establish a benchmark containing eight\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\na broad exploration of the capacities of LLMs within the context of practical\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\nsettings with carefully selected demonstration examples and specially crafted\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\nexhibit different competitive levels in eight chemistry tasks. In addition to\nthe key findings from the comprehensive benchmark analysis, our work provides\ninsights into the limitation of current LLMs and the impact of in-context\nlearning settings on LLMs' performance across various chemistry tasks. The code\nand datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.",
        "score": 1.3418560028076172
      },
      {
        "title": "Are LLMs Ready for Real-World Materials Discovery?,",
        "abstract": "Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.",
        "score": 1.3319767713546753
      },
      {
        "title": "Stochastic LLMs do not Understand Language: Towards Symbolic,\n  Explainable and Ontologically Based LLMs,",
        "abstract": "In our opinion the exuberance surrounding the relative success of data-driven\nlarge language models (LLMs) is slightly misguided and for several reasons (i)\nLLMs cannot be relied upon for factual information since for LLMs all ingested\ntext (factual or non-factual) was created equal; (ii) due to their subsymbolic\nna-ture, whatever 'knowledge' these models acquire about language will always\nbe buried in billions of microfeatures (weights), none of which is meaningful\non its own; and (iii) LLMs will often fail to make the correct inferences in\nseveral linguistic contexts (e.g., nominal compounds, copredication, quantifier\nscope ambi-guities, intensional contexts. Since we believe the relative success\nof data-driven large language models (LLMs) is not a reflection on the symbolic\nvs. subsymbol-ic debate but a reflection on applying the successful strategy of\na bottom-up reverse engineering of language at scale, we suggest in this paper\napplying the effective bottom-up strategy in a symbolic setting resulting in\nsymbolic, explainable, and ontologically grounded language models.",
        "score": 1.1717791557312012
      },
      {
        "title": "In-Context Symbolic Regression: Leveraging Large Language Models for\n  Function Discovery,",
        "abstract": "State of the art Symbolic Regression (SR) methods currently build specialized\nmodels, while the application of Large Language Models (LLMs) remains largely\nunexplored. In this work, we introduce the first comprehensive framework that\nutilizes LLMs for the task of SR. We propose In-Context Symbolic Regression\n(ICSR), an SR method which iteratively refines a functional form with an LLM\nand determines its coefficients with an external optimizer. ICSR leverages\nLLMs' strong mathematical prior both to propose an initial set of possible\nfunctions given the observations and to refine them based on their errors. Our\nfindings reveal that LLMs are able to successfully find symbolic equations that\nfit the given data, matching or outperforming the overall performance of the\nbest SR baselines on four popular benchmarks, while yielding simpler equations\nwith better out of distribution generalization.",
        "score": 0.9816840887069702
      },
      {
        "title": "nach0: Multimodal Natural and Chemical Languages Foundation Model,",
        "abstract": "Large Language Models (LLMs) have substantially driven scientific progress in\nvarious domains, and many papers have demonstrated their ability to tackle\ncomplex problems with creative solutions. Our paper introduces a new foundation\nmodel, nach0, capable of solving various chemical and biological tasks:\nbiomedical question answering, named entity recognition, molecular generation,\nmolecular synthesis, attributes prediction, and others. nach0 is a multi-domain\nand multi-task encoder-decoder LLM pre-trained on unlabeled text from\nscientific literature, patents, and molecule strings to incorporate a range of\nchemical and linguistic knowledge. We employed instruction tuning, where\nspecific task-related instructions are utilized to fine-tune nach0 for the\nfinal set of tasks. To train nach0 effectively, we leverage the NeMo framework,\nenabling efficient parallel optimization of both base and large model versions.\nExtensive experiments demonstrate that our model outperforms state-of-the-art\nbaselines on single-domain and cross-domain tasks. Furthermore, it can generate\nhigh-quality outputs in molecular and textual formats, showcasing its\neffectiveness in multi-domain setups.",
        "score": 0.923234224319458
      },
      {
        "title": "A Comprehensive Survey of Scientific Large Language Models and Their\n  Applications in Scientific Discovery,",
        "abstract": "In many scientific fields, large language models (LLMs) have revolutionized\nthe way with which text and other modalities of data (e.g., molecules and\nproteins) are dealt, achieving superior performance in various applications and\naugmenting the scientific discovery process. Nevertheless, previous surveys on\nscientific LLMs often concentrate on one to two fields or a single modality. In\nthis paper, we aim to provide a more holistic view of the research landscape by\nunveiling cross-field and cross-modal connections between scientific LLMs\nregarding their architectures and pre-training techniques. To this end, we\ncomprehensively survey over 250 scientific LLMs, discuss their commonalities\nand differences, as well as summarize pre-training datasets and evaluation\ntasks for each field and modality. Moreover, we investigate how LLMs have been\ndeployed to benefit scientific discovery. Resources related to this survey are\navailable at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.",
        "score": 0.8322535157203674
      },
      {
        "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains,",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating natural language. However, their capabilities wane\nin highly specialized domains underrepresented in the pretraining corpus, such\nas physical and biomedical sciences. This work explores how to repurpose\ngeneral LLMs into effective task solvers for specialized domains. We introduce\na novel, model-agnostic framework for learning custom input tags, which are\nparameterized as continuous vectors appended to the LLM's embedding layer, to\ncondition the LLM. We design two types of input tags: domain tags are used to\ndelimit specialized representations (e.g., chemical formulas) and provide\ndomain-relevant context; function tags are used to represent specific functions\n(e.g., predicting molecular properties) and compress function-solving\ninstructions. We develop a three-stage protocol to learn these tags using\nauxiliary data and domain knowledge. By explicitly disentangling task domains\nfrom task functions, our method enables zero-shot generalization to unseen\nproblems through diverse combinations of the input tags. It also boosts LLM's\nperformance in various specialized domains, such as predicting protein or\nchemical properties and modeling drug-target interactions, outperforming expert\nmodels tailored to these tasks.",
        "score": 0.6287692189216614
      },
      {
        "title": "Symbolic and Language Agnostic Large Language Models,",
        "abstract": "We argue that the relative success of large language models (LLMs) is not a\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\nan appropriate strategy of bottom-up reverse engineering of language at scale.\nHowever, due to the subsymbolic nature of these models whatever knowledge these\nsystems acquire about language will always be buried in millions of\nmicrofeatures (weights) none of which is meaningful on its own. Moreover, and\ndue to their stochastic nature, these models will often fail in capturing\nvarious inferential aspects that are prevalent in natural language. What we\nsuggest here is employing the successful bottom-up strategy in a symbolic\nsetting, producing symbolic, language agnostic and ontologically grounded large\nlanguage models.",
        "score": 0.3771114647388458
      },
      {
        "title": "Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?,",
        "abstract": "A hallmark of intelligence is the ability to use a familiar domain to make\ninferences about a less familiar domain, known as analogical reasoning. In this\narticle, we delve into the performance of Large Language Models (LLMs) in\ndealing with progressively complex analogies expressed in unstructured text. We\ndiscuss analogies at four distinct levels of complexity: lexical analogies,\nsyntactic analogies, semantic analogies, and pragmatic analogies. As the\nanalogies become more complex, they require increasingly extensive, diverse\nknowledge beyond the textual content, unlikely to be found in the lexical\nco-occurrence statistics that power LLMs. To address this, we discuss the\nnecessity of employing Neuro-symbolic AI techniques that combine statistical\nand symbolic AI, informing the representation of unstructured text to highlight\nand augment relevant content, provide abstraction and guide the mapping\nprocess. Our knowledge-informed approach maintains the efficiency of LLMs while\npreserving the ability to explain analogies for pedagogical applications.",
        "score": 0.1371537446975708
      },
      {
        "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from\n  Large Language Models for Commonsense Reasoning,",
        "abstract": "The sequential process of conceptualization and instantiation is essential to\ngeneralizable commonsense reasoning as it allows the application of existing\nknowledge to unfamiliar scenarios. However, existing works tend to undervalue\nthe step of instantiation and heavily rely on pre-built concept taxonomies and\nhuman annotations to collect both types of knowledge, resulting in a lack of\ninstantiated knowledge to complete reasoning, high cost, and limited\nscalability. To tackle these challenges, we introduce CANDLE, a distillation\nframework that iteratively performs contextualized conceptualization and\ninstantiation over commonsense knowledge bases by instructing large language\nmodels to generate both types of knowledge with critic filtering. By applying\nCANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six\nmillion conceptualizations and instantiated commonsense knowledge triples. Both\ntypes of knowledge are firmly rooted in the original ATOMIC dataset, and\nintrinsic evaluations demonstrate their exceptional quality and diversity.\nEmpirical results indicate that distilling CANDLE on student models provides\nbenefits across four downstream tasks. Our code, data, and models are publicly\navailable at https://github.com/HKUST-KnowComp/CANDLE.",
        "score": -0.2132474184036255
      },
      {
        "title": "MLLM-SR: Conversational Symbolic Regression base Multi-Modal Large\n  Language Models,",
        "abstract": "Formulas are the language of communication between humans and nature. It is\nan important research topic of artificial intelligence to find expressions from\nobserved data to reflect the relationship between each variable in the data,\nwhich is called a symbolic regression problem. The existing symbolic regression\nmethods directly generate expressions according to the given observation data,\nand we cannot require the algorithm to generate expressions that meet specific\nrequirements according to the known prior knowledge. For example, the\nexpression needs to contain $\\sin$ or be symmetric, and so on. Even if it can,\nit often requires very complex operations, which is very inconvenient. In this\npaper, based on multi-modal large language models, we propose MLLM-SR, a\nconversational symbolic regression method that can generate expressions that\nmeet the requirements simply by describing the requirements with natural\nlanguage instructions. By experimenting on the Nguyen dataset, we can\ndemonstrate that MLLM-SR leads the state-of-the-art baselines in fitting\nperformance. More notably, we experimentally demonstrate that MLLM-SR can well\nunderstand the prior knowledge we add to the natural language instructions.\nMoreover, the addition of prior knowledge can effectively guide MLLM-SR to\ngenerate correct expressions.",
        "score": -0.6284472346305847
      },
      {
        "title": "Unifying Molecular and Textual Representations via Multi-task Language\n  Modelling,",
        "abstract": "The recent advances in neural language models have also been successfully\napplied to the field of chemistry, offering generative solutions for classical\nproblems in molecular design and synthesis planning. These new methods have the\npotential to fuel a new era of data-driven automation in scientific discovery.\nHowever, specialized models are still typically required for each task, leading\nto the need for problem-specific fine-tuning and neglecting task\ninterrelations. The main obstacle in this field is the lack of a unified\nrepresentation between natural language and chemical representations,\ncomplicating and limiting human-machine interaction. Here, we propose the first\nmulti-domain, multi-task language model that can solve a wide range of tasks in\nboth the chemical and natural language domains. Our model can handle chemical\nand natural language concurrently, without requiring expensive pre-training on\nsingle domains or task-specific models. Interestingly, sharing weights across\ndomains remarkably improves our model when benchmarked against state-of-the-art\nbaselines on single-domain and cross-domain tasks. In particular, sharing\ninformation across domains and tasks gives rise to large improvements in\ncross-domain tasks, the magnitude of which increase with scale, as measured by\nmore than a dozen of relevant metrics. Our work suggests that such models can\nrobustly and efficiently accelerate discovery in physical sciences by\nsuperseding problem-specific fine-tuning and enhancing human-model\ninteractions.",
        "score": -0.9521463513374329
      },
      {
        "title": "Translation between Molecules and Natural Language,",
        "abstract": "We present $\\textbf{MolT5}$ $-$ a self-supervised learning framework for\npretraining models on a vast amount of unlabeled natural language text and\nmolecule strings. $\\textbf{MolT5}$ allows for new, useful, and challenging\nanalogs of traditional vision-language tasks, such as molecule captioning and\ntext-based de novo molecule generation (altogether: translation between\nmolecules and language), which we explore for the first time. Since\n$\\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome the\nchemistry domain shortcoming of data scarcity. Furthermore, we consider several\nmetrics, including a new cross-modal embedding-based metric, to evaluate the\ntasks of molecule captioning and text-based molecule generation. Our results\nshow that $\\textbf{MolT5}$-based models are able to generate outputs, both\nmolecules and captions, which in many cases are high quality.",
        "score": -1.5816842317581177
      },
      {
        "title": "L+M-24: Building a Dataset for Language + Molecules @ ACL 2024,",
        "abstract": "Language-molecule models have emerged as an exciting direction for molecular\ndiscovery and understanding. However, training these models is challenging due\nto the scarcity of molecule-language pair datasets. At this point, datasets\nhave been released which are 1) small and scraped from existing databases, 2)\nlarge but noisy and constructed by performing entity linking on the scientific\nliterature, and 3) built by converting property prediction datasets to natural\nlanguage using templates. In this document, we detail the $\\textit{L+M-24}$\ndataset, which has been created for the Language + Molecules Workshop shared\ntask at ACL 2024. In particular, $\\textit{L+M-24}$ is designed to focus on\nthree key benefits of natural language in molecule design: compositionality,\nfunctionality, and abstraction.",
        "score": -1.7469278573989868
      },
      {
        "title": "Neural-Symbolic Recursive Machine for Systematic Generalization,",
        "abstract": "Current learning models often struggle with human-like systematic\ngeneralization, particularly in learning compositional rules from limited data\nand extrapolating them to novel combinations. We introduce the Neural-Symbolic\nRecursive Machine (NSR), whose core is a Grounded Symbol System (GSS), allowing\nfor the emergence of combinatorial syntax and semantics directly from training\ndata. The NSR employs a modular design that integrates neural perception,\nsyntactic parsing, and semantic reasoning. These components are synergistically\ntrained through a novel deduction-abduction algorithm. Our findings demonstrate\nthat NSR's design, imbued with the inductive biases of equivariance and\ncompositionality, grants it the expressiveness to adeptly handle diverse\nsequence-to-sequence tasks and achieve unparalleled systematic generalization.\nWe evaluate NSR's efficacy across four challenging benchmarks designed to probe\nsystematic generalization capabilities: SCAN for semantic parsing, PCFG for\nstring manipulation, HINT for arithmetic reasoning, and a compositional machine\ntranslation task. The results affirm NSR's superiority over contemporary neural\nand hybrid models in terms of generalization and transferability.",
        "score": -2.4898548126220703
      },
      {
        "title": "COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,",
        "abstract": "Conceptual knowledge is fundamental to human cognition and knowledge bases.\nHowever, existing knowledge probing works only focus on evaluating factual\nknowledge of pre-trained language models (PLMs) and ignore conceptual\nknowledge. Since conceptual knowledge often appears as implicit commonsense\nbehind texts, designing probes for conceptual knowledge is hard. Inspired by\nknowledge representation schemata, we comprehensively evaluate conceptual\nknowledge of PLMs by designing three tasks to probe whether PLMs organize\nentities by conceptual similarities, learn conceptual properties, and\nconceptualize entities in contexts, respectively. For the tasks, we collect and\nannotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual\nknowledge Probing bENchmark. Extensive experiments on different sizes and types\nof PLMs show that existing PLMs systematically lack conceptual knowledge and\nsuffer from various spurious correlations. We believe this is a critical\nbottleneck for realizing human-like cognition in PLMs. COPEN and our codes are\npublicly released at https://github.com/THU-KEG/COPEN.",
        "score": -3.5060226917266846
      }
    ]
  },
  {
    "title": "Think Twice: Perspective-Taking Improves Large Language Models'\n  Theory-of-Mind Capabilities",
    "abstract": "  Human interactions are deeply rooted in the interplay of thoughts, beliefs,\nand desires made possible by Theory of Mind (ToM): our cognitive ability to\nunderstand the mental states of ourselves and others. Although ToM may come\nnaturally to us, emulating it presents a challenge to even the most advanced\nLarge Language Models (LLMs). Recent improvements to LLMs' reasoning\ncapabilities from simple yet effective prompting techniques such as\nChain-of-Thought have seen limited applicability to ToM. In this paper, we turn\nto the prominent cognitive science theory \"Simulation Theory\" to bridge this\ngap. We introduce SimToM, a novel two-stage prompting framework inspired by\nSimulation Theory's notion of perspective-taking. To implement this idea on\ncurrent ToM benchmarks, SimToM first filters context based on what the\ncharacter in question knows before answering a question about their mental\nstate. Our approach, which requires no additional training and minimal\nprompt-tuning, shows substantial improvement over existing methods, and our\nanalysis reveals the importance of perspective-taking to Theory-of-Mind\ncapabilities. Our findings suggest perspective-taking as a promising direction\nfor future research into improving LLMs' ToM capabilities.\n",
    "related_paper_titles": [
      "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
      "Understanding Social Reasoning in Language Models with Language Models",
      "Large Language Models Fail on Trivial Alterations to Theory-of-Mind\n  Tasks"
    ],
    "related_paper_abstract": [
      "  Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial. In this\nwork, we examine the open question of social intelligence and Theory of Mind in\nmodern NLP systems from an empirical and theory-based perspective. We show that\none of today's largest language models (GPT-3; Brown et al., 2020) lacks this\nkind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et\nal., 2019), which measures models' ability to understand intents and reactions\nof participants of social interactions, and ToMi (Le et al., 2019), which\nmeasures whether models can infer mental states and realities of participants\nof situations. Our results show that models struggle substantially at these\nTheory of Mind tasks, with well-below-human accuracies of 55% and 60% on\nSocialIQa and ToMi, respectively. To conclude, we draw on theories from\npragmatics to contextualize this shortcoming of large language models, by\nexamining the limitations stemming from their data, neural architecture, and\ntraining paradigms. Challenging the prevalent narrative that only scale is\nneeded, we posit that person-centric NLP approaches might be more effective\ntowards neural Theory of Mind.\n  In our updated version, we also analyze newer instruction tuned and RLFH\nmodels for neural ToM. We find that even ChatGPT and GPT-4 do not display\nemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on\nthe ToMi questions related to mental states and realities.\n",
      "  As Large Language Models (LLMs) become increasingly integrated into our\neveryday lives, understanding their ability to comprehend human mental states\nbecomes critical for ensuring effective interactions. However, despite the\nrecent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of\nLLMs, the degree to which these models can align with human ToM remains a\nnuanced topic of exploration. This is primarily due to two distinct challenges:\n(1) the presence of inconsistent results from previous evaluations, and (2)\nconcerns surrounding the validity of existing evaluation methodologies. To\naddress these challenges, we present a novel framework for procedurally\ngenerating evaluations with LLMs by populating causal templates. Using our\nframework, we create a new social reasoning benchmark (BigToM) for LLMs which\nconsists of 25 controls and 5,000 model-written evaluations. We find that human\nparticipants rate the quality of our benchmark higher than previous\ncrowd-sourced evaluations and comparable to expert-written evaluations. Using\nBigToM, we evaluate the social reasoning capabilities of a variety of LLMs and\ncompare model performances with human performance. Our results suggest that\nGPT4 has ToM capabilities that mirror human inference patterns, though less\nreliable, while other LLMs struggle.\n",
      "  Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.\n"
    ],
    "entities": [
      "RAG",
      "ChatGPT",
      "DPO",
      "LLaMA",
      "ESG",
      "Llama",
      "Retrieval Augmented",
      "Human Feedback",
      "Gemini",
      "Persian",
      "AIoT",
      "MATH",
      "KV",
      "KGE",
      "IFT",
      "ToM",
      "SLMs",
      "Direct Preference",
      "MLLMs",
      "Retrieval",
      "APIs",
      "LoRA",
      "KG",
      "PDE",
      "Python",
      "DNN",
      "API",
      "SQL",
      "GSM8K",
      "PDEs"
    ],
    "retrieved_papers": [
      {
        "title": "Think Twice: Perspective-Taking Improves Large Language Models'\n  Theory-of-Mind Capabilities,",
        "abstract": "Human interactions are deeply rooted in the interplay of thoughts, beliefs,\nand desires made possible by Theory of Mind (ToM): our cognitive ability to\nunderstand the mental states of ourselves and others. Although ToM may come\nnaturally to us, emulating it presents a challenge to even the most advanced\nLarge Language Models (LLMs). Recent improvements to LLMs' reasoning\ncapabilities from simple yet effective prompting techniques such as\nChain-of-Thought have seen limited applicability to ToM. In this paper, we turn\nto the prominent cognitive science theory \"Simulation Theory\" to bridge this\ngap. We introduce SimToM, a novel two-stage prompting framework inspired by\nSimulation Theory's notion of perspective-taking. To implement this idea on\ncurrent ToM benchmarks, SimToM first filters context based on what the\ncharacter in question knows before answering a question about their mental\nstate. Our approach, which requires no additional training and minimal\nprompt-tuning, shows substantial improvement over existing methods, and our\nanalysis reveals the importance of perspective-taking to Theory-of-Mind\ncapabilities. Our findings suggest perspective-taking as a promising direction\nfor future research into improving LLMs' ToM capabilities.",
        "score": 5.913578510284424
      },
      {
        "title": "Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs\n  for Open-Ended Responses,",
        "abstract": "Theory of Mind (ToM) reasoning entails recognizing that other individuals\npossess their own intentions, emotions, and thoughts, which is vital for\nguiding one's own thought processes. Although large language models (LLMs)\nexcel in tasks such as summarization, question answering, and translation, they\nstill face challenges with ToM reasoning, especially in open-ended questions.\nDespite advancements, the extent to which LLMs truly understand ToM reasoning\nand how closely it aligns with human ToM reasoning remains inadequately\nexplored in open-ended scenarios. Motivated by this gap, we assess the\nabilities of LLMs to perceive and integrate human intentions and emotions into\ntheir ToM reasoning processes within open-ended questions. Our study utilizes\nposts from Reddit's ChangeMyView platform, which demands nuanced social\nreasoning to craft persuasive responses. Our analysis, comparing semantic\nsimilarity and lexical overlap metrics between responses generated by humans\nand LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended\nquestions, with even the most advanced models showing notable limitations. To\nenhance LLM capabilities, we implement a prompt tuning method that incorporates\nhuman intentions and emotions, resulting in improvements in ToM reasoning\nperformance. However, despite these improvements, the enhancement still falls\nshort of fully achieving human-like reasoning. This research highlights the\ndeficiencies in LLMs' social reasoning and demonstrates how integrating human\nintentions and emotions can boost their effectiveness.",
        "score": 0.4493551254272461
      },
      {
        "title": "Re-Reading Improves Reasoning in Large Language Models,",
        "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models\n(LLMs), we introduce a simple, yet general and effective prompting method, Re2,\ni.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most\nthought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim\nto elicit the reasoning process in the output, Re2 shifts the focus to the\ninput by processing questions twice, thereby enhancing the understanding\nprocess. Consequently, Re2 demonstrates strong generality and compatibility\nwith most thought-eliciting prompting methods, including CoT. Crucially, Re2\nfacilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs\nbecause the first pass could provide global information for the second pass. We\nbegin with a preliminary empirical study as the foundation of Re2, illustrating\nits potential to enable \"bidirectional\" attention mechanisms. We then evaluate\nRe2 on extensive reasoning benchmarks across 14 datasets, spanning 112\nexperiments, to validate its effectiveness and generality. Our findings\nindicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2\nconsistently enhances the reasoning performance of LLMs through a simple\nre-reading strategy. Further analyses reveal Re2's adaptability, showing how it\ncan be effectively integrated with different LLMs, thought-eliciting prompting,\nand ensemble strategies. Our code is available at\n\\url{https://github.com/Tebmer/Rereading-LLM-Reasoning/}",
        "score": 0.2996387779712677
      },
      {
        "title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?,",
        "abstract": "\"Thinking is for Doing.\" Humans can infer other people's mental states from\nobservations--an ability called Theory-of-Mind (ToM)--and subsequently act\npragmatically on those inferences. Existing question answering benchmarks such\nas ToMi ask models questions to make inferences about beliefs of characters in\na story, but do not test whether models can then use these inferences to guide\ntheir actions. We propose a new evaluation paradigm for large language models\n(LLMs): Thinking for Doing (T4D), which requires models to connect inferences\nabout others' mental states to actions in social scenarios. Experiments on T4D\ndemonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking\ncharacters' beliefs in stories, but they struggle to translate this capability\ninto strategic action. Our analysis reveals the core challenge for LLMs lies in\nidentifying the implicit inferences about mental states without being\nexplicitly asked about as in ToMi, that lead to choosing the correct action in\nT4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee\nand Reflect (FaR), which provides a reasoning structure that encourages LLMs to\nanticipate future challenges and reason about potential actions. FaR boosts\nGPT-4's performance from 50% to 71% on T4D, outperforming other prompting\nmethods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to\ndiverse out-of-distribution story structures and scenarios that also require\nToM inferences to choose an action, consistently outperforming other methods\nincluding few-shot in-context learning.",
        "score": 0.069236621260643
      },
      {
        "title": "UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large\n  Language Model Capabilities,",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive inferential\ncapabilities, with numerous research endeavors devoted to enhancing this\ncapacity through prompting. Despite these efforts, a unified epistemological\nfoundation is still conspicuously absent. Drawing inspiration from Kant's a\npriori philosophy, we propose the UPAR prompting framework, designed to emulate\nthe structure of human cognition within LLMs. The UPAR framework is delineated\ninto four phases: \"Understand\", \"Plan\", \"Act\", and \"Reflect\", enabling the\nextraction of structured information from complex contexts, prior planning of\nsolutions, execution according to plan, and self-reflection. This structure\nsignificantly augments the explainability and accuracy of LLM inference,\nproducing a human-understandable and inspectable inferential trajectory.\nFurthermore, our work offers an epistemological foundation for existing\nprompting techniques, allowing for a possible systematic integration of these\nmethods. With GPT-4, our approach elevates the accuracy from COT baseline of\n22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in\nthe causal judgment task. Without using few-shot examples or external tools,\nUPAR significantly outperforms existing prompting methods on SCIBENCH, a\nchallenging dataset containing collegiate-level mathematics, chemistry, and\nphysics scientific problems.",
        "score": -0.2235078662633896
      },
      {
        "title": "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of\n  Mind in Large Language Models,",
        "abstract": "While humans naturally develop theory of mind (ToM), the capability to\nunderstand other people's mental states and beliefs, state-of-the-art large\nlanguage models (LLMs) underperform on simple ToM benchmarks. We posit that we\ncan extend our understanding of LLMs' ToM abilities by evaluating key human ToM\nprecursors -- perception inference and perception-to-belief inference -- in\nLLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate\nthese precursory inferences for ToM in LLMs by annotating characters'\nperceptions on ToMi and FANToM, respectively. Our evaluation of eight\nstate-of-the-art LLMs reveals that the models generally perform well in\nperception inference while exhibiting limited capability in\nperception-to-belief inference (e.g., lack of inhibitory control). Based on\nthese results, we present PercepToM, a novel ToM method leveraging LLMs' strong\nperception inference capability while supplementing their limited\nperception-to-belief inference. Experimental results demonstrate that PercepToM\nsignificantly enhances LLM's performance, especially in false belief scenarios.",
        "score": -0.3233167827129364
      },
      {
        "title": "Improving Interpersonal Communication by Simulating Audiences with\n  Language Models,",
        "abstract": "How do we communicate with others to achieve our goals? We use our prior\nexperience or advice from others, or construct a candidate utterance by\npredicting how it will be received. However, our experiences are limited and\nbiased, and reasoning about potential outcomes can be difficult and cognitively\nchallenging. In this paper, we explore how we can leverage Large Language Model\n(LLM) simulations to help us communicate better. We propose the\nExplore-Generate-Simulate (EGS) framework, which takes as input any scenario\nwhere an individual is communicating to an audience with a goal they want to\nachieve. EGS (1) explores the solution space by producing a diverse set of\nadvice relevant to the scenario, (2) generates communication candidates\nconditioned on subsets of the advice, and (3) simulates the reactions from\nvarious audiences to determine both the best candidate and advice to use. We\nevaluate the framework on eight scenarios spanning the ten fundamental\nprocesses of interpersonal communication. For each scenario, we collect a\ndataset of human evaluations across candidates and baselines, and showcase that\nour framework's chosen candidate is preferred over popular generation\nmechanisms including Chain-of-Thought. We also find that audience simulations\nachieve reasonably high agreement with human raters across 5 of the 8\nscenarios. Finally, we demonstrate the generality of our framework by applying\nit to real-world scenarios described by users on web forums. Through\nevaluations and demonstrations, we show that EGS enhances the effectiveness and\noutcomes of goal-oriented communication across a variety of situations, thus\nopening up new possibilities for the application of large language models in\nrevolutionizing communication and decision-making processes.",
        "score": -0.6592251658439636
      },
      {
        "title": "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play\n  Multi-Character Belief Tracker,",
        "abstract": "Theory of Mind (ToM)$\\unicode{x2014}$the ability to reason about the mental\nstates of other people$\\unicode{x2014}$is a key element of our social\nintelligence. Yet, despite their ever more impressive performance, large-scale\nneural language models still lack basic theory of mind capabilities\nout-of-the-box. We posit that simply scaling up models will not imbue them with\ntheory of mind due to the inherently symbolic and implicit nature of the\nphenomenon, and instead investigate an alternative: can we design a\ndecoding-time algorithm that enhances theory of mind of off-the-shelf neural\nlanguage models without explicit supervision? We present SymbolicToM, a\nplug-and-play approach to reason about the belief states of multiple characters\nin reading comprehension tasks via explicit symbolic representation. More\nconcretely, our approach tracks each entity's beliefs, their estimation of\nother entities' beliefs, and higher-order levels of reasoning, all through\ngraphical representations, allowing for more precise and interpretable\nreasoning than previous approaches. Empirical results on the well-known ToMi\nbenchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances\noff-the-shelf neural networks' theory of mind in a zero-shot setting while\nshowing robust out-of-distribution performance compared to supervised\nbaselines. Our work also reveals spurious patterns in existing theory of mind\nbenchmarks, emphasizing the importance of out-of-distribution evaluation and\nmethods that do not overfit a particular dataset.",
        "score": -0.6977506279945374
      },
      {
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in\n  Interactions,",
        "abstract": "Theory of mind (ToM) evaluations currently focus on testing models using\npassive narratives that inherently lack interactivity. We introduce FANToM, a\nnew benchmark designed to stress-test ToM within information-asymmetric\nconversational contexts via question answering. Our benchmark draws upon\nimportant theoretical requisites from psychology and necessary empirical\nconsiderations when evaluating large language models (LLMs). In particular, we\nformulate multiple types of questions that demand the same underlying reasoning\nto identify illusory or false sense of ToM capabilities in LLMs. We show that\nFANToM is challenging for state-of-the-art LLMs, which perform significantly\nworse than humans even with chain-of-thought reasoning or fine-tuning.",
        "score": -0.7975428104400635
      },
      {
        "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models,",
        "abstract": "We introduce Graph of Thoughts (GoT): a framework that advances prompting\ncapabilities in large language models (LLMs) beyond those offered by paradigms\nsuch as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information generated by an LLM as\nan arbitrary graph, where units of information (\"LLM thoughts\") are vertices,\nand edges correspond to dependencies between these vertices. This approach\nenables combining arbitrary LLM thoughts into synergistic outcomes, distilling\nthe essence of whole networks of thoughts, or enhancing thoughts using feedback\nloops. We illustrate that GoT offers advantages over state of the art on\ndifferent tasks, for example increasing the quality of sorting by 62% over ToT,\nwhile simultaneously reducing costs by >31%. We ensure that GoT is extensible\nwith new thought transformations and thus can be used to spearhead new\nprompting schemes. This work brings the LLM reasoning closer to human thinking\nor brain mechanisms such as recurrence, both of which form complex networks.",
        "score": -0.7989192008972168
      },
      {
        "title": "Views Are My Own, but Also Yours: Benchmarking Theory of Mind Using\n  Common Ground,",
        "abstract": "Evaluating the theory of mind (ToM) capabilities of language models (LMs) has\nrecently received a great deal of attention. However, many existing benchmarks\nrely on synthetic data, which risks misaligning the resulting experiments with\nhuman behavior. We introduce the first ToM dataset based on naturally occurring\nspoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We\nthen show that integrating a simple, explicit representation of beliefs\nimproves LM performance on Common-ToM.",
        "score": -0.9696218371391296
      },
      {
        "title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language\n  Models,",
        "abstract": "Large language models (LLMs) have shown impressive emergent abilities in a\nwide range of tasks, but still face challenges in handling complex reasoning\nproblems. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT)\nhave predominately focused on enhancing accuracy, but overlook the rapidly\nincreasing token cost, which could be particularly problematic for open-ended\nreal-world tasks with huge solution spaces. Motivated by the dual process\ntheory of human cognition, we propose \"Synergy of Thoughts\" (SoT) to unleash\nthe synergistic potential of hybrid LLMs for efficient reasoning. By default,\nSoT uses smaller-scale language models to generate multiple low-cost reasoning\nthoughts, which resembles the parallel intuitions produced by System 1. If\nthese intuitions exhibit conflicts, SoT will invoke the reflective reasoning of\nscaled-up language models to emulate the intervention of System 2, which will\noverride the intuitive thoughts and rectify the reasoning process. This\nframework is model-agnostic and training-free, which can be flexibly\nimplemented with various off-the-shelf LLMs. Experiments on six representative\nreasoning tasks show that SoT substantially reduces the token cost by\n38.3%-75.1%, and simultaneously achieves state-of-the-art reasoning accuracy\nand solution diversity. Notably, the average token cost reduction on open-ended\ntasks reaches up to 69.1%. Code repo with all prompts will be released upon\npublication.",
        "score": -1.0409982204437256
      },
      {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models,",
        "abstract": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.",
        "score": -1.2704460620880127
      },
      {
        "title": "Violation of Expectation via Metacognitive Prompting Reduces Theory of\n  Mind Prediction Error in Large Language Models,",
        "abstract": "Recent research shows that Large Language Models (LLMs) exhibit a compelling\nlevel of proficiency in Theory of Mind (ToM) tasks. This ability to impute\nunobservable mental states to others is vital to human social cognition and may\nprove equally important in principal-agent relations between individual humans\nand Artificial Intelligences (AIs). In this paper, we explore how a mechanism\nstudied in developmental psychology known as Violation of Expectation (VoE) can\nbe implemented to reduce errors in LLM prediction about users by leveraging\nemergent ToM affordances. And we introduce a \\textit{metacognitive prompting}\nframework to apply VoE in the context of an AI tutor. By storing and retrieving\nfacts derived in cases where LLM expectation about the user was violated, we\nfind that LLMs are able to learn about users in ways that echo theories of\nhuman learning. Finally, we discuss latent hazards and augmentative\nopportunities associated with modeling user psychology and propose ways to\nmitigate risk along with possible directions for future inquiry.",
        "score": -1.2942821979522705
      },
      {
        "title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning\n  in Large Language Models,",
        "abstract": "Theory of Mind (ToM) is the ability to reason about one's own and others'\nmental states. ToM plays a critical role in the development of intelligence,\nlanguage understanding, and cognitive processes. While previous work has\nprimarily focused on first and second-order ToM, we explore higher-order ToM,\nwhich involves recursive reasoning on others' beliefs. We introduce HI-TOM, a\nHigher Order Theory of Mind benchmark. Our experimental evaluation using\nvarious Large Language Models (LLMs) indicates a decline in performance on\nhigher-order ToM tasks, demonstrating the limitations of current LLMs. We\nconduct a thorough analysis of different failure cases of LLMs, and share our\nthoughts on the implications of our findings on the future of NLP.",
        "score": -1.2953004837036133
      },
      {
        "title": "Boosting Theory-of-Mind Performance in Large Language Models via\n  Prompting,",
        "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face\nchallenges in complex reasoning. Theory-of-mind (ToM) tasks, which require\nunderstanding agents' beliefs, goals, and mental states, are essential for\ncommon-sense reasoning involving humans, making it crucial to enhance LLM\nperformance in this area. This study measures the ToM performance of GPT-4 and\nthree GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates\nthe effectiveness of in-context learning in improving their ToM comprehension.\nWe evaluated prompts featuring two-shot chain of thought reasoning and\nstep-by-step thinking instructions. We found that LLMs trained with\nReinforcement Learning from Human Feedback (RLHF) (all models excluding\nDavinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed\nbest in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell\nshort of the 87% human accuracy on the test set. However, when supplied with\nprompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM\naccuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate\nprompting enhances LLM ToM reasoning, and they underscore the context-dependent\nnature of LLM cognitive capacities.",
        "score": -1.3163121938705444
      },
      {
        "title": "Eliminating Reasoning via Inferring with Planning: A New Framework to\n  Guide LLMs' Non-linear Thinking,",
        "abstract": "Chain-of-Thought(CoT) prompting and its variants explore equipping large\nlanguage models (LLMs) with high-level reasoning abilities by emulating\nhuman-like linear cognition and logic. However, the human mind is complicated\nand mixed with both linear and nonlinear thinking. In this work, we propose\n\\textbf{I}nferential \\textbf{E}xclusion \\textbf{P}rompting (IEP), a novel\nprompting that combines the principles of elimination and inference in order to\nguide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize\nNatural Language Inference (NLI) to deduce each possible solution's entailment\nrelation with context, commonsense, or facts, therefore yielding a broader\nperspective by thinking back for inferring. This forward planning and backward\neliminating process allows IEP to better simulate the complex human thinking\nprocesses compared to other CoT-based methods, which only reflect linear\ncognitive processes. We conducted a series of empirical studies and have\ncorroborated that IEP consistently outperforms CoT across various tasks.\nAdditionally, we observe that integrating IEP and CoT further improves the\nLLMs' performance on certain tasks, highlighting the necessity of equipping\nLLMs with mixed logic processes. Moreover, to better evaluate comprehensive\nfeatures inherent in human logic, we introduce \\textbf{M}ental-\\textbf{A}bility\n\\textbf{R}easoning \\textbf{B}enchmark (MARB). The benchmark comprises six novel\nsubtasks with a total of 9,115 questions, among which 1,685 are developed with\nhand-crafted rationale references. We believe both \\textsc{IEP} and\n\\textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and\nverbal reasoning abilities and drive further advancements. \\textsc{MARB} will\nbe available at ~\\texttt{anonymity link} soon.",
        "score": -1.3755422830581665
      },
      {
        "title": "Encouraging Divergent Thinking in Large Language Models through\n  Multi-Agent Debate,",
        "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n\"tit for tat\" and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of \"tit for tat\" state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Code is available at\nhttps://github.com/Skytliang/Multi-Agents-Debate.",
        "score": -1.377755045890808
      },
      {
        "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve,",
        "abstract": "Large Language Models (LLMs) have shown impressive abilities in various\ntasks. However, fundamentally improving them depends on high-quality datasets\nor computationally expensive fine-tuning. On the contrary, humans can easily\nimprove themselves by self-thinking and memory, without external resources. In\nthis paper, we propose a framework, MoT, to let the LLM self-improve through\nMemory-of-Thought, without annotated datasets and parameter updates.\nSpecifically, MoT is divided into two stages: 1. before the test stage, the LLM\npre-thinks on the unlabeled dataset and saves the high-confidence thoughts as\nexternal memory; 2. During the test stage, given a test question, the LLM\nrecalls relevant memory to help itself reason and answer it. Experimental\nresults show that MoT can help ChatGPT significantly improve its abilities in\narithmetic reasoning, commonsense reasoning, factual reasoning, and natural\nlanguage inference. Further analyses show that each component contributes\ncritically to the improvements and MoT can lead to consistent improvements\nacross various CoT methods and LLMs.",
        "score": -1.4016417264938354
      },
      {
        "title": "Why Can Large Language Models Generate Correct Chain-of-Thoughts?,",
        "abstract": "This paper delves into the capabilities of large language models (LLMs),\nspecifically focusing on advancing the theoretical comprehension of\nchain-of-thought prompting. We investigate how LLMs can be effectively induced\nto generate a coherent chain of thoughts. To achieve this, we introduce a\ntwo-level hierarchical graphical model tailored for natural language\ngeneration. Within this framework, we establish a compelling geometrical\nconvergence rate that gauges the likelihood of an LLM-generated chain of\nthoughts compared to those originating from the true language. Our findings\nprovide a theoretical justification for the ability of LLMs to produce the\ncorrect sequence of thoughts (potentially) explaining performance gains in\ntasks demanding reasoning skills.",
        "score": -1.4621456861495972
      },
      {
        "title": "ToM-LM: Delegating Theory of Mind Reasoning to External Symbolic\n  Executors in Large Language Models,",
        "abstract": "Theory of Mind (ToM) refers to the ability of individuals to attribute mental\nstates to others. While Large Language Models (LLMs) have shown some promise\nwith ToM ability, they still struggle with complex ToM reasoning. Our approach\nleverages an external symbolic executor, specifically the SMCDEL model checker,\nand fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,\nan LLM is first fine-tuned through pairs of natural language and symbolic\nformulation representation of ToM problems and is then instructed to generate\nthe symbolic formulation with a one-shot in-context example. The generated\nsymbolic formulation is then executed by the SMCDEL model checker to perform\ntransparent and verifiable ToM reasoning and give the final result. We\ndemonstrate that our approach, ToM-LM, shows a significant improvement over all\nthe constructed baselines. Our study proposes a novel view about externalizing\na particular component of ToM reasoning, mainly reasoning about beliefs, and\nsuggests generalizing it to other aspects of ToM reasoning.",
        "score": -1.6375916004180908
      },
      {
        "title": "ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks\n  for Exploring Theory of Mind,",
        "abstract": "Theory of Mind (ToM), the capacity to comprehend the mental states of\ndistinct individuals, is essential for numerous practical applications. With\nthe development of large language models (LLMs), there is a heated debate about\nwhether they are able to perform ToM tasks. Previous studies have used\ndifferent tasks and prompts to test the ToM on LLMs and the results are\ninconsistent: some studies asserted these models are capable of exhibiting ToM,\nwhile others suggest the opposite. In this study, We present ToMChallenges, a\ndataset for comprehensively evaluating the Theory of Mind based on the\nSally-Anne and Smarties tests with a diverse set of tasks. In addition, we also\npropose an auto-grader to streamline the answer evaluation process. We tested\nthree models: davinci, turbo, and gpt-4. Our evaluation results and error\nanalyses show that LLMs have inconsistent behaviors across prompts and tasks.\nPerforming the ToM tasks robustly remains a challenge for the LLMs. In\naddition, our paper wants to raise awareness in evaluating the ToM in LLMs and\nwe want to invite more discussion on how to design the prompts and tasks for\nToM tasks that can better assess the LLMs' ability.",
        "score": -1.8597224950790405
      },
      {
        "title": "Chain-of-Thought Reasoning Without Prompting,",
        "abstract": "In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding effectively\nelicits reasoning capabilities from language models, which were previously\nobscured by standard greedy decoding.",
        "score": -1.8743172883987427
      },
      {
        "title": "R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought\n  Reasoning in Large Language Models under Noisy Context,",
        "abstract": "With the help of Chain-of-Thought (CoT) prompting, Large Language Models\n(LLMs) have achieved remarkable performance on various reasoning tasks.\nHowever, most of them have been evaluated under noise-free context and the\ndilemma for LLMs to produce inaccurate results under the noisy context has not\nbeen fully investigated. Existing studies utilize trigger sentences to\nencourage LLMs to concentrate on the relevant information but the trigger has\nlimited effect on final answer prediction. Inspired by interactive CoT method,\nwhere intermediate reasoning steps are promoted by multiple rounds of\ninteraction between users and LLMs, we propose a novel prompting method, namely\nR$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$\nprompting interacts with LLMs to perform key sentence extraction, variable\ndeclaration and answer prediction, which corresponds to a thought process of\nreviewing, rephrasing and resolving. The responses generated at the last\ninteraction will perform as hints to guide toward the responses of the next\ninteraction. Our experiments show that R$^3$ prompting significantly\noutperforms existing CoT prompting methods on five reasoning tasks under noisy\ncontext. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on\nthe reasoning tasks under noisy context compared to the most competitive\nprompting baseline. More analyses and ablation studies show the robustness and\ngeneralization of R$^3$ prompting method in solving reasoning tasks in LLMs\nunder noisy context.",
        "score": -2.056325674057007
      },
      {
        "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought,",
        "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world\nknowledge, they have been shown incapable of recalling these knowledge to solve\ntasks requiring complex & multi-step reasoning. Similar to how humans develop a\n\"chain of thought\" for these tasks, how can we equip PLMs with such abilities?\nIn this work, we explore an iterative prompting framework, a new prompting\nparadigm which progressively elicits relevant knowledge from PLMs for\nmulti-step inference. We identify key limitations of existing prompting\nmethods, namely they are either restricted to queries with a single\nidentifiable relation/predicate, or being agnostic to input contexts, which\nmakes it difficult to capture variabilities across different inference steps.\nWe propose an iterative context-aware prompter, which addresses these\nlimitations by learning to dynamically synthesize prompts conditioned on the\ncurrent step's contexts. Experiments on three datasets involving multi-step\nreasoning show the effectiveness of the iterative scheme and the context-aware\nprompter design.",
        "score": -2.1674599647521973
      },
      {
        "title": "Thread of Thought Unraveling Chaotic Contexts,",
        "abstract": "Large Language Models (LLMs) have ushered in a transformative era in the\nfield of natural language processing, excelling in tasks related to text\ncomprehension and generation. Nevertheless, they encounter difficulties when\nconfronted with chaotic contexts (e.g., distractors rather than long irrelevant\ncontext), leading to the inadvertent omission of certain details within the\nchaotic context. In response to these challenges, we introduce the \"Thread of\nThought\" (ThoT) strategy, which draws inspiration from human cognitive\nprocesses. ThoT systematically segments and analyzes extended contexts while\nadeptly selecting pertinent information. This strategy serves as a versatile\n\"plug-and-play\" module, seamlessly integrating with various LLMs and prompting\ntechniques. In the experiments, we utilize the PopQA and EntityQ datasets, as\nwell as a Multi-Turn Conversation Response dataset (MTCR) we collected, to\nillustrate that ThoT significantly improves reasoning performance compared to\nother prompting techniques.",
        "score": -2.180062770843506
      },
      {
        "title": "Benchmarking Mental State Representations in Language Models,",
        "abstract": "While numerous works have assessed the generative performance of language\nmodels (LMs) on tasks requiring Theory of Mind reasoning, research into the\nmodels' internal representation of mental states remains limited. Recent work\nhas used probing to demonstrate that LMs can represent beliefs of themselves\nand others. However, these claims are accompanied by limited evaluation, making\nit difficult to assess how mental state representations are affected by model\ndesign and training choices. We report an extensive benchmark with various LM\ntypes with different model sizes, fine-tuning approaches, and prompt designs to\nstudy the robustness of mental state representations and memorisation issues\nwithin the probes. Our results show that the quality of models' internal\nrepresentations of the beliefs of others increases with model size and, more\ncrucially, with fine-tuning. We are the first to study how prompt variations\nimpact probing performance on theory of mind tasks. We demonstrate that models'\nrepresentations are sensitive to prompt variations, even when such variations\nshould be beneficial. Finally, we complement previous activation editing\nexperiments on Theory of Mind tasks and show that it is possible to improve\nmodels' reasoning performance by steering their activations without the need to\ntrain any probe.",
        "score": -2.3864173889160156
      },
      {
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of\n  What Matters,",
        "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.",
        "score": -2.7311904430389404
      },
      {
        "title": "The Impact of Reasoning Step Length on Large Language Models,",
        "abstract": "Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models",
        "score": -3.2707905769348145
      },
      {
        "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking,",
        "abstract": "When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.",
        "score": -4.282558917999268
      }
    ]
  },
  {
    "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models\n  via Unconstrained Generation",
    "abstract": "  Large language models (LLMs) have emerged as pivotal contributors in\ncontemporary natural language processing and are increasingly being applied\nacross a diverse range of industries. However, these large-scale probabilistic\nstatistical models cannot currently ensure the requisite quality in\nprofessional content generation. These models often produce hallucinated text,\ncompromising their practical utility in professional contexts. To assess the\nauthentic reliability of LLMs in text generation, numerous initiatives have\ndeveloped benchmark evaluations for hallucination phenomena. Nevertheless,\nthese benchmarks frequently utilize constrained generation techniques due to\ncost and temporal constraints. These techniques encompass the use of directed\nhallucination induction and strategies that deliberately alter authentic text\nto produce hallucinations. These approaches are not congruent with the\nunrestricted text generation demanded by real-world applications. Furthermore,\na well-established Chinese-language dataset dedicated to the evaluation of\nhallucinations in text generation is presently lacking. Consequently, we have\ndeveloped an Unconstrained Hallucination Generation Evaluation (UHGEval)\nbenchmark, designed to compile outputs produced with minimal restrictions by\nLLMs. Concurrently, we have established a comprehensive benchmark evaluation\nframework to aid subsequent researchers in undertaking scalable and\nreproducible experiments. We have also executed extensive experiments,\nevaluating prominent Chinese language models and the GPT series models to\nderive professional performance insights regarding hallucination challenges.\n",
    "related_paper_titles": [
      "Siren's Song in the AI Ocean: A Survey on Hallucination in Large\n  Language Models",
      "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of\n  LLMs by Validating Low-Confidence Generation",
      "Evaluating Hallucinations in Chinese Large Language Models"
    ],
    "related_paper_abstract": [
      "  While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.\n",
      "  Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with GPT-3.5 (text-davinci-003) on the 'article\ngeneration task', we first demonstrate the individual efficacy of our detection\nand mitigation techniques. Specifically, the detection technique achieves a\nrecall of ~88% and the mitigation technique successfully mitigates 57.6% of the\ncorrectly detected hallucinations. Importantly, our mitigation technique does\nnot introduce new hallucinations even in the case of incorrectly detected\nhallucinations, i.e., false positives. Then, we show that the proposed active\ndetection and mitigation approach successfully reduces the hallucinations of\nthe GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the\neffectiveness and wide applicability of our approach through additional studies\nincluding performance on different types of questions (multi-hop and false\npremise questions) and with another LLM from a different model family (Vicuna).\nIn summary, our work contributes to improving the reliability and\ntrustworthiness of large language models, a crucial step en route to enabling\ntheir widespread adoption in real-world applications.\n",
      "  In this paper, we establish a benchmark named HalluQA (Chinese Hallucination\nQuestion-Answering) to measure the hallucination phenomenon in Chinese large\nlanguage models. HalluQA contains 450 meticulously designed adversarial\nquestions, spanning multiple domains, and takes into account Chinese historical\nculture, customs, and social phenomena. During the construction of HalluQA, we\nconsider two types of hallucinations: imitative falsehoods and factual errors,\nand we construct adversarial samples based on GLM-130B and ChatGPT. For\nevaluation, we design an automated evaluation method using GPT-4 to judge\nwhether a model output is hallucinated. We conduct extensive experiments on 24\nlarge language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk\nand etc. Out of the 24 models, 18 achieved non-hallucination rates lower than\n50%. This indicates that HalluQA is highly challenging. We analyze the primary\ntypes of hallucinations in different types of models and their causes.\nAdditionally, we discuss which types of hallucinations should be prioritized\nfor different types of models.\n"
    ],
    "entities": [
      "Large Language Models Large Language Models",
      "ICL",
      "CoT",
      "KGs",
      "ESG",
      "Persian",
      "MATH",
      "KV",
      "KGE",
      "IFT",
      "Knowledge Graphs",
      "SLMs",
      "Retrieval",
      "KG",
      "PDE",
      "PDEs",
      "NER",
      "SVM",
      "PTQ",
      "Verilog",
      "MoE",
      "OpenAI",
      "Thought",
      "LLaMA",
      "Llama",
      "Experts",
      "Indian",
      "Mistral",
      "Monte Carlo Tree Search",
      "Language Model"
    ],
    "retrieved_papers": [
      {
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models\n  via Unconstrained Generation,",
        "abstract": "Large language models (LLMs) have emerged as pivotal contributors in\ncontemporary natural language processing and are increasingly being applied\nacross a diverse range of industries. However, these large-scale probabilistic\nstatistical models cannot currently ensure the requisite quality in\nprofessional content generation. These models often produce hallucinated text,\ncompromising their practical utility in professional contexts. To assess the\nauthentic reliability of LLMs in text generation, numerous initiatives have\ndeveloped benchmark evaluations for hallucination phenomena. Nevertheless,\nthese benchmarks frequently utilize constrained generation techniques due to\ncost and temporal constraints. These techniques encompass the use of directed\nhallucination induction and strategies that deliberately alter authentic text\nto produce hallucinations. These approaches are not congruent with the\nunrestricted text generation demanded by real-world applications. Furthermore,\na well-established Chinese-language dataset dedicated to the evaluation of\nhallucinations in text generation is presently lacking. Consequently, we have\ndeveloped an Unconstrained Hallucination Generation Evaluation (UHGEval)\nbenchmark, designed to compile outputs produced with minimal restrictions by\nLLMs. Concurrently, we have established a comprehensive benchmark evaluation\nframework to aid subsequent researchers in undertaking scalable and\nreproducible experiments. We have also executed extensive experiments,\nevaluating prominent Chinese language models and the GPT series models to\nderive professional performance insights regarding hallucination challenges.",
        "score": 7.214004993438721
      },
      {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large\n  Language Models,",
        "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate\nhallucinations, i.e., content that conflicts with the source or cannot be\nverified by the factual knowledge. To understand what types of content and to\nwhich extent LLMs are apt to hallucinate, we introduce the Hallucination\nEvaluation benchmark for Large Language Models (HaluEval), a large collection\nof generated and human-annotated hallucinated samples for evaluating the\nperformance of LLMs in recognizing hallucination. To generate these samples, we\npropose a ChatGPT-based two-step framework, i.e., sampling-then-filtering.\nBesides, we also hire some human labelers to annotate the hallucinations in\nChatGPT responses. The empirical results suggest that ChatGPT is likely to\ngenerate hallucinated content in specific topics by fabricating unverifiable\ninformation (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face\ngreat challenges in recognizing the hallucinations in texts. However, our\nexperiments also prove that providing external knowledge or adding reasoning\nsteps can help LLMs recognize hallucinations. Our benchmark can be accessed at\nhttps://github.com/RUCAIBox/HaluEval.",
        "score": 2.70556902885437
      },
      {
        "title": "HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level\n  Hallucination Evaluation,",
        "abstract": "Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), achieving remarkable performance across diverse\ntasks and enabling widespread real-world applications. However, LLMs are prone\nto hallucination, generating content that either conflicts with established\nknowledge or is unfaithful to the original sources. Existing hallucination\nbenchmarks primarily focus on sentence- or passage-level hallucination\ndetection, neglecting dialogue-level evaluation, hallucination localization,\nand rationale provision. They also predominantly target factuality\nhallucinations while underestimating faithfulness hallucinations, often relying\non labor-intensive or non-specialized evaluators. To address these limitations,\nwe propose HalluDial, the first comprehensive large-scale benchmark for\nautomatic dialogue-level hallucination evaluation. HalluDial encompasses both\nspontaneous and induced hallucination scenarios, covering factuality and\nfaithfulness hallucinations. The benchmark includes 4,094 dialogues with a\ntotal of 146,856 samples. Leveraging HalluDial, we conduct a comprehensive\nmeta-evaluation of LLMs' hallucination evaluation capabilities in\ninformation-seeking dialogues and introduce a specialized judge language model,\nHalluJudge. The high data quality of HalluDial enables HalluJudge to achieve\nsuperior or competitive performance in hallucination evaluation, facilitating\nthe automatic assessment of dialogue-level hallucinations in LLMs and providing\nvaluable insights into this phenomenon. The dataset and the code are available\nat https://github.com/FlagOpen/HalluDial.",
        "score": 2.644937038421631
      },
      {
        "title": "HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination\n  Tendency of LLMs,",
        "abstract": "Hallucinations pose a significant challenge to the reliability and alignment\nof Large Language Models (LLMs), limiting their widespread acceptance beyond\nchatbot applications. Despite ongoing efforts, hallucinations remain a\nprevalent challenge in LLMs. The detection of hallucinations itself is also a\nformidable task, frequently requiring manual labeling or constrained\nevaluations. This paper introduces an automated scalable framework that\ncombines benchmarking LLMs' hallucination tendencies with efficient\nhallucination detection. We leverage LLMs to generate challenging tasks related\nto hypothetical phenomena, subsequently employing them as agents for efficient\nhallucination detection. The framework is domain-agnostic, allowing the use of\nany language model for benchmark creation or evaluation in any domain. We\nintroduce the publicly available HypoTermQA Benchmarking Dataset, on which\nstate-of-the-art models' performance ranged between 3% and 11%, and evaluator\nagents demonstrated a 6% error rate in hallucination prediction. The proposed\nframework provides opportunities to test and improve LLMs. Additionally, it has\nthe potential to generate benchmarking datasets tailored to specific domains,\nsuch as law, health, and finance.",
        "score": 2.5403714179992676
      },
      {
        "title": "A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions,",
        "abstract": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), leading to remarkable\nadvancements in text understanding and generation. Nevertheless, alongside\nthese strides, LLMs exhibit a critical tendency to produce hallucinations,\nresulting in content that is inconsistent with real-world facts or user inputs.\nThis phenomenon poses substantial challenges to their practical deployment and\nraises concerns over the reliability of LLMs in real-world scenarios, which\nattracts increasing attention to detect and mitigate these hallucinations. In\nthis survey, we aim to provide a thorough and in-depth overview of recent\nadvances in the field of LLM hallucinations. We begin with an innovative\ntaxonomy of LLM hallucinations, then delve into the factors contributing to\nhallucinations. Subsequently, we present a comprehensive overview of\nhallucination detection methods and benchmarks. Additionally, representative\napproaches designed to mitigate hallucinations are introduced accordingly.\nFinally, we analyze the challenges that highlight the current limitations and\nformulate open questions, aiming to delineate pathways for future research on\nhallucinations in LLMs.",
        "score": 2.367871046066284
      },
      {
        "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language\n  Models,",
        "abstract": "While Large language models (LLMs) have garnered widespread applications\nacross various domains due to their powerful language understanding and\ngeneration capabilities, the detection of non-factual or hallucinatory content\ngenerated by LLMs remains scarce. Currently, one significant challenge in\nhallucination detection is the laborious task of time-consuming and expensive\nmanual annotation of the hallucinatory generation. To address this issue, this\npaper first introduces a method for automatically constructing model-specific\nhallucination datasets based on existing fact-checking datasets called\nAutoHall. Furthermore, we propose a zero-resource and black-box hallucination\ndetection method based on self-contradiction. We conduct experiments towards\nprevalent open-/closed-source LLMs, achieving superior hallucination detection\nperformance compared to extant baselines. Moreover, our experiments reveal\nvariations in hallucination proportions and types among different models.",
        "score": 2.1838605403900146
      },
      {
        "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy\n  Retrieval-Augmented Language Models,",
        "abstract": "Retrieval-augmented generation (RAG) has become a main technique for\nalleviating hallucinations in large language models (LLMs). Despite the\nintegration of RAG, LLMs may still present unsupported or contradictory claims\nto the retrieved contents. In order to develop effective hallucination\nprevention strategies under RAG, it is important to create benchmark datasets\nthat can measure the extent of hallucination. This paper presents RAGTruth, a\ncorpus tailored for analyzing word-level hallucinations in various domains and\ntasks within the standard RAG frameworks for LLM applications. RAGTruth\ncomprises nearly 18,000 naturally generated responses from diverse LLMs using\nRAG. These responses have undergone meticulous manual annotations at both the\nindividual cases and word levels, incorporating evaluations of hallucination\nintensity. We not only benchmark hallucination frequencies across different\nLLMs, but also critically assess the effectiveness of several existing\nhallucination detection methodologies. Furthermore, we show that using a\nhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\nsmall LLM and achieve a competitive level of performance in hallucination\ndetection when compared to the existing prompt-based approaches using\nstate-of-the-art large language models such as GPT-4.",
        "score": 2.1639351844787598
      },
      {
        "title": "Language Models Hallucinate, but May Excel at Fact Verification,",
        "abstract": "Recent progress in natural language processing (NLP) owes much to remarkable\nadvances in large language models (LLMs). Nevertheless, LLMs frequently\n\"hallucinate,\" resulting in non-factual outputs. Our carefully-designed human\nevaluation substantiates the serious hallucination issue, revealing that even\nGPT-3.5 produces factual outputs less than 25% of the time. This underscores\nthe importance of fact verifiers in order to measure and incentivize progress.\nOur systematic investigation affirms that LLMs can be repurposed as effective\nfact verifiers with strong correlations with human judgments. Surprisingly,\nFLAN-T5-11B, the least factual generator in our study, performs the best as a\nfact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.\nDelving deeper, we analyze the reliance of these LLMs on high-quality evidence,\nas well as their deficiencies in robustness and generalization ability. Our\nstudy presents insights for developing trustworthy generation models.",
        "score": 2.1384596824645996
      },
      {
        "title": "Evaluating Hallucinations in Chinese Large Language Models,",
        "abstract": "In this paper, we establish a benchmark named HalluQA (Chinese Hallucination\nQuestion-Answering) to measure the hallucination phenomenon in Chinese large\nlanguage models. HalluQA contains 450 meticulously designed adversarial\nquestions, spanning multiple domains, and takes into account Chinese historical\nculture, customs, and social phenomena. During the construction of HalluQA, we\nconsider two types of hallucinations: imitative falsehoods and factual errors,\nand we construct adversarial samples based on GLM-130B and ChatGPT. For\nevaluation, we design an automated evaluation method using GPT-4 to judge\nwhether a model output is hallucinated. We conduct extensive experiments on 24\nlarge language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk\nand etc. Out of the 24 models, 18 achieved non-hallucination rates lower than\n50%. This indicates that HalluQA is highly challenging. We analyze the primary\ntypes of hallucinations in different types of models and their causes.\nAdditionally, we discuss which types of hallucinations should be prioritized\nfor different types of models.",
        "score": 1.9168286323547363
      },
      {
        "title": "Chain of Natural Language Inference for Reducing Large Language Model\n  Ungrounded Hallucinations,",
        "abstract": "Large language models (LLMs) can generate fluent natural language texts when\ngiven relevant documents as background context. This ability has attracted\nconsiderable interest in developing industry applications of LLMs. However,\nLLMs are prone to generate hallucinations that are not supported by the\nprovided sources. In this paper, we propose a hierarchical framework to detect\nand mitigate such ungrounded hallucination. Our framework uses Chain of Natural\nLanguage Inference (CoNLI) for hallucination detection and hallucination\nreduction via post-editing. Our approach achieves state-of-the-art performance\non hallucination detection and enhances text quality through rewrite, using\nLLMs without any fine-tuning or domain-specific prompt engineering. We show\nthat this simple plug-and-play framework can serve as an effective choice for\nhallucination detection and reduction, achieving competitive performance across\nvarious contexts.",
        "score": 1.8643884658813477
      },
      {
        "title": "DEE: Dual-stage Explainable Evaluation Method for Text Generation,",
        "abstract": "Automatic methods for evaluating machine-generated texts hold significant\nimportance due to the expanding applications of generative systems.\nConventional methods tend to grapple with a lack of explainability, issuing a\nsolitary numerical score to signify the assessment outcome. Recent advancements\nhave sought to mitigate this limitation by incorporating large language models\n(LLMs) to offer more detailed error analyses, yet their applicability remains\nconstrained, particularly in industrial contexts where comprehensive error\ncoverage and swift detection are paramount. To alleviate these challenges, we\nintroduce DEE, a Dual-stage Explainable Evaluation method for estimating the\nquality of text generation. Built upon Llama 2, DEE follows a dual-stage\nprinciple guided by stage-specific instructions to perform efficient\nidentification of errors in generated texts in the initial stage and\nsubsequently delves into providing comprehensive diagnostic reports in the\nsecond stage. DEE is fine-tuned on our elaborately assembled dataset AntEval,\nwhich encompasses 15K examples from 4 real-world applications of Alipay that\nemploy generative systems. The dataset concerns newly emerged issues like\nhallucination and toxicity, thereby broadening the scope of DEE's evaluation\ncriteria. Experimental results affirm that DEE's superiority over existing\nevaluation methods, achieving significant improvements in both human\ncorrelation as well as efficiency.",
        "score": 1.8348703384399414
      },
      {
        "title": "Alleviating Hallucinations of Large Language Models through Induced\n  Hallucinations,",
        "abstract": "Despite their impressive capabilities, large language models (LLMs) have been\nobserved to generate responses that include inaccurate or fabricated\ninformation, a phenomenon commonly known as ``hallucination''. In this work, we\npropose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to\nalleviate hallucinations. We first construct a factually weak LLM by inducing\nhallucinations from the original LLMs. Then, we penalize these induced\nhallucinations during decoding to enhance the factuality of the generated\ncontent. Concretely, we determine the final next-token predictions by\namplifying the predictions from the original model and downplaying the induced\nuntruthful predictions via contrastive decoding. Experimental results on both\ndiscrimination-based and generation-based hallucination evaluation benchmarks,\nsuch as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD\nmethods can effectively enhance the factuality of LLMs across various model\nsizes and families. For example, when equipped with ICD, Llama2-7B-Chat and\nMistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on\nTruthfulQA, respectively.",
        "score": 1.8291106224060059
      },
      {
        "title": "MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM\n  Hallucination Detection,",
        "abstract": "In Natural Language Generation (NLG), contemporary Large Language Models\n(LLMs) face several challenges, such as generating fluent yet inaccurate\noutputs and reliance on fluency-centric metrics. This often leads to neural\nnetworks exhibiting \"hallucinations\". The SHROOM challenge focuses on\nautomatically identifying these hallucinations in the generated text. To tackle\nthese issues, we introduce two key components, a data augmentation pipeline\nincorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a\nvoting ensemble from three models pre-trained on Natural Language Inference\n(NLI) tasks and fine-tuned on diverse datasets.",
        "score": 1.8157161474227905
      },
      {
        "title": "Detecting Hallucinations in Large Language Model Generation: A Token\n  Probability Approach,",
        "abstract": "Concerns regarding the propensity of Large Language Models (LLMs) to produce\ninaccurate outputs, also known as hallucinations, have escalated. Detecting\nthem is vital for ensuring the reliability of applications relying on\nLLM-generated content. Current methods often demand substantial resources and\nrely on extensive LLMs or employ supervised learning with multidimensional\nfeatures or intricate linguistic and semantic analyses difficult to reproduce\nand largely depend on using the same LLM that hallucinated. This paper\nintroduces a supervised learning approach employing two simple classifiers\nutilizing only four numerical features derived from tokens and vocabulary\nprobabilities obtained from other LLM evaluators, which are not necessarily the\nsame. The method yields promising results, surpassing state-of-the-art outcomes\nin multiple tasks across three different benchmarks. Additionally, we provide a\ncomprehensive examination of the strengths and weaknesses of our approach,\nhighlighting the significance of the features utilized and the LLM employed as\nan evaluator. We have released our code publicly at\nhttps://github.com/Baylor-AI/HalluDetect.",
        "score": 1.469542384147644
      },
      {
        "title": "Mitigating Large Language Model Hallucination with Faithful Finetuning,",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious natural language processing tasks. However, they are prone to\ngenerating fluent yet untruthful responses, known as \"hallucinations\".\nHallucinations can lead to the spread of misinformation and cause harm in\ncritical applications. Mitigating hallucinations is challenging as they arise\nfrom factors such as noisy data, model overconfidence, lack of knowledge, and\nthe generation process itself. Recent efforts have attempted to address this\nissue through representation editing and decoding algorithms, reducing\nhallucinations without major structural changes or retraining. However, these\napproaches either implicitly edit LLMs' behavior in latent space or suppress\nthe tendency to output unfaithful results during decoding instead of explicitly\nmodeling on hallucination. In this work, we introduce Faithful Finetuning (F2),\na novel method that explicitly models the process of faithful question\nanswering through carefully designed loss functions during fine-tuning. We\nconduct extensive experiments on popular datasets and demonstrate that F2\nachieves significant improvements over vanilla models and baselines.",
        "score": 1.3543751239776611
      },
      {
        "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination\n  in Large Language Models,",
        "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
        "score": 1.3492799997329712
      },
      {
        "title": "Ever: Mitigating Hallucination in Large Language Models through\n  Real-Time Verification and Rectification,",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating fluent text. However, they often encounter the challenge of\ngenerating inaccurate or hallucinated content. This issue is common in both\nnon-retrieval-based generation and retrieval-augmented generation approaches,\nand existing post-hoc rectification methods may not address the accumulated\nhallucination errors that may be caused by the \"snowballing\" issue, especially\nin reasoning tasks. To tackle these challenges, we introduce a novel approach\ncalled Real-time Verification and Rectification (Ever). Instead of waiting\nuntil the end of the generation process to rectify hallucinations, Ever employs\na real-time, step-wise generation and hallucination rectification strategy. The\nprimary objective is to detect and rectify hallucinations as they occur during\nthe text generation process. When compared to both retrieval-based and\nnon-retrieval-based baselines, Ever demonstrates a significant improvement in\ngenerating trustworthy and factually accurate text across a diverse range of\ntasks, including short-form QA, biography generation, and multi-hop reasoning.",
        "score": 0.8382381796836853
      },
      {
        "title": "Zero-Shot Multi-task Hallucination Detection,",
        "abstract": "In recent studies, the extensive utilization of large language models has\nunderscored the importance of robust evaluation methodologies for assessing\ntext generation quality and relevance to specific tasks. This has revealed a\nprevalent issue known as hallucination, an emergent condition in the model\nwhere generated text lacks faithfulness to the source and deviates from the\nevaluation criteria. In this study, we formally define hallucination and\npropose a framework for its quantitative detection in a zero-shot setting,\nleveraging our definition and the assumption that model outputs entail task and\nsample specific inputs. In detecting hallucinations, our solution achieves an\naccuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting.\nNotably, our solution maintains computational efficiency, requiring far less\ncomputational resources than other SOTA approaches, aligning with the trend\ntowards lightweight and compressed models.",
        "score": 0.7781771421432495
      },
      {
        "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models,",
        "abstract": "As large language models continue to develop in the field of AI, text\ngeneration systems are susceptible to a worrisome phenomenon known as\nhallucination. In this study, we summarize recent compelling insights into\nhallucinations in LLMs. We present a novel taxonomy of hallucinations from\nvarious text generation tasks, thus provide theoretical insights, detection\nmethods and improvement approaches. Based on this, future research directions\nare proposed. Our contribution are threefold: (1) We provide a detailed and\ncomplete taxonomy for hallucinations appearing in text generation tasks; (2) We\nprovide theoretical analyses of hallucinations in LLMs and provide existing\ndetection and improvement methods; (3) We propose several research directions\nthat can be developed in the future. As hallucinations garner significant\nattention from the community, we will maintain updates on relevant research\nprogress.",
        "score": 0.765810489654541
      },
      {
        "title": "Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text\n  Generation,",
        "abstract": "Hallucination of text ungrounded in the input is a well-known problem in\nneural data-to-text generation. Many methods have been proposed to mitigate it,\nbut they typically require altering model architecture or collecting additional\ndata, and thus cannot be easily applied to an existing model. In this paper, we\nexplore a new way to mitigate hallucinations by combining the probabilistic\noutput of a generator language model (LM) with the output of a special \"text\ncritic\" classifier, which guides the generation by assessing the match between\nthe input data and the text generated so far. Our method does not need any\nchanges to the underlying LM's architecture or training procedure and can thus\nbe combined with any model and decoding operating on word probabilities. The\ncritic does not need any additional training data, using the base LM's training\ndata and synthetic negative examples. Our experimental results show that our\nmethod improves over the baseline on the WebNLG and OpenDialKG benchmarks.",
        "score": 0.7196200489997864
      },
      {
        "title": "Fine-grained Hallucination Detection and Editing for Language Models,",
        "abstract": "Large language models (LMs) are prone to generate factual errors, which are\noften called hallucinations. In this paper, we introduce a comprehensive\ntaxonomy of hallucinations and argue that hallucinations manifest in diverse\nforms, each requiring varying degrees of careful assessments to verify\nfactuality. We propose a novel task of automatic fine-grained hallucination\ndetection and construct a new evaluation benchmark, FavaBench, that includes\nabout one thousand fine-grained human judgments on three LM outputs across\nvarious domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B)\nexhibit diverse types of hallucinations in the majority of their outputs in\ninformation-seeking scenarios. We train FAVA, a retrieval-augmented LM by\ncarefully creating synthetic data to detect and correct fine-grained\nhallucinations. On our benchmark, our automatic and human evaluations show that\nFAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination\ndetection, and edits suggested by FAVA improve the factuality of LM-generated\ntext.",
        "score": 0.6124264597892761
      },
      {
        "title": "A Token-level Reference-free Hallucination Detection Benchmark for\n  Free-form Text Generation,",
        "abstract": "Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.",
        "score": 0.6061998605728149
      },
      {
        "title": "Can We Catch the Elephant? A Survey of the Evolvement of Hallucination\n  Evaluation on Natural Language Generation,",
        "abstract": "Hallucination in Natural Language Generation (NLG) is like the elephant in\nthe room, obvious but often overlooked until recent achievements significantly\nimproved the fluency and grammaticality of generated text. As the capabilities\nof text generation models have improved, researchers have begun to pay more\nattention to the phenomenon of hallucination. Despite significant progress in\nthis field in recent years, the evaluation system for hallucination is complex\nand diverse, lacking clear organization. We are the first to comprehensively\nsurvey how various evaluation methods have evolved with the development of text\ngeneration models from three dimensions, including hallucinated fact\ngranularity, evaluator design principles, and assessment facets. This survey\naims to help researchers identify current limitations in hallucination\nevaluation and highlight future research directions.",
        "score": 0.4531940519809723
      },
      {
        "title": "A Systematic Review of Data-to-Text NLG,",
        "abstract": "This systematic review undertakes a comprehensive analysis of current\nresearch on data-to-text generation, identifying gaps, challenges, and future\ndirections within the field. Relevant literature in this field on datasets,\nevaluation metrics, application areas, multilingualism, language models, and\nhallucination mitigation methods is reviewed. Various methods for producing\nhigh-quality text are explored, addressing the challenge of hallucinations in\ndata-to-text generation. These methods include re-ranking, traditional and\nneural pipeline architecture, planning architectures, data cleaning, controlled\ngeneration, and modification of models and training techniques. Their\neffectiveness and limitations are assessed, highlighting the need for\nuniversally applicable strategies to mitigate hallucinations. The review also\nexamines the usage, popularity, and impact of datasets, alongside evaluation\nmetrics, with an emphasis on both automatic and human assessment. Additionally,\nthe evolution of data-to-text models, particularly the widespread adoption of\ntransformer models, is discussed. Despite advancements in text quality, the\nreview emphasizes the importance of research in low-resourced languages and the\nengineering of datasets in these languages to promote inclusivity. Finally,\nseveral application domains of data-to-text are highlighted, emphasizing their\nrelevance in such domains. Overall, this review serves as a guiding framework\nfor fostering innovation and advancing data-to-text generation.",
        "score": 0.44670554995536804
      },
      {
        "title": "Survey of Hallucination in Natural Language Generation,",
        "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, machine translation, and visual-language generation;\nand (3) hallucinations in large language models (LLMs). This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.",
        "score": 0.15806551277637482
      },
      {
        "title": "SLPL SHROOM at SemEval2024 Task 06: A comprehensive study on models\n  ability to detect hallucination,",
        "abstract": "Language models, particularly generative models, are susceptible to\nhallucinations, generating outputs that contradict factual knowledge or the\nsource text. This study explores methods for detecting hallucinations in three\nSemEval-2024 Task 6 tasks: Machine Translation, Definition Modeling, and\nParaphrase Generation. We evaluate two methods: semantic similarity between the\ngenerated text and factual references, and an ensemble of language models that\njudge each other's outputs. Our results show that semantic similarity achieves\nmoderate accuracy and correlation scores in trial data, while the ensemble\nmethod offers insights into the complexities of hallucination detection but\nfalls short of expectations. This work highlights the challenges of\nhallucination detection and underscores the need for further research in this\ncritical area.",
        "score": -0.07979609072208405
      },
      {
        "title": "Understanding and Detecting Hallucinations in Neural Machine Translation\n  via Model Introspection,",
        "abstract": "Neural sequence generation models are known to \"hallucinate\", by producing\noutputs that are unrelated to the source text. These hallucinations are\npotentially harmful, yet it remains unclear in what conditions they arise and\nhow to mitigate their impact. In this work, we first identify internal model\nsymptoms of hallucinations by analyzing the relative token contributions to the\ngeneration in contrastive hallucinated vs. non-hallucinated outputs generated\nvia source perturbations. We then show that these symptoms are reliable\nindicators of natural hallucinations, by using them to design a lightweight\nhallucination detector which outperforms both model-free baselines and strong\nclassifiers based on quality estimation or large pre-trained models on manually\nannotated English-Chinese and German-English translation test beds.",
        "score": -0.25091251730918884
      },
      {
        "title": "Comparing Hallucination Detection Metrics for Multilingual Generation,",
        "abstract": "While many hallucination detection techniques have been evaluated on English\ntext, their effectiveness in multilingual contexts remains unknown. This paper\nassesses how well various factual hallucination detection metrics (lexical\nmetrics like ROUGE and Named Entity Overlap, and Natural Language Inference\n(NLI)-based metrics) identify hallucinations in generated biographical\nsummaries across languages. We compare how well automatic metrics correlate to\neach other and whether they agree with human judgments of factuality. Our\nanalysis reveals that while the lexical metrics are ineffective, NLI-based\nmetrics perform well, correlating with human annotations in many settings and\noften outperforming supervised models. However, NLI metrics are still limited,\nas they do not detect single-fact hallucinations well and fail for\nlower-resource languages. Therefore, our findings highlight the gaps in\nexisiting hallucination detection methods for non-English languages and\nmotivate future research to develop more robust multilingual detection methods\nfor LLM hallucinations.",
        "score": -0.7540023326873779
      },
      {
        "title": "A Data-Centric Approach To Generate Faithful and High Quality Patient\n  Summaries with Large Language Models,",
        "abstract": "Patients often face difficulties in understanding their hospitalizations,\nwhile healthcare workers have limited resources to provide explanations. In\nthis work, we investigate the potential of large language models to generate\npatient summaries based on doctors' notes and study the effect of training data\non the faithfulness and quality of the generated summaries. To this end, we\nrelease (i) a rigorous labeling protocol for errors in medical texts and (ii) a\npublicly available dataset of annotated hallucinations in 100 doctor-written\nand 100 generated summaries. We show that fine-tuning on hallucination-free\ndata effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama\n2, while preserving relevant information. We observe a similar effect on GPT-4\n(0.70 to 0.40), when the few-shot examples are hallucination-free. We also\nconduct a qualitative evaluation using hallucination-free and improved training\ndata. We find that common quantitative metrics do not correlate well with\nfaithfulness and quality. Finally, we test GPT-4 for automatic hallucination\ndetection, which clearly outperforms common baselines.",
        "score": -1.0744898319244385
      },
      {
        "title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation,",
        "abstract": "Hallucinations pose a significant challenge to the reliability of neural\nmodels for abstractive summarisation. While automatically generated summaries\nmay be fluent, they often lack faithfulness to the original document. This\nissue becomes even more pronounced in low-resource settings, such as\ncross-lingual transfer. With the existing faithful metrics focusing on English,\neven measuring the extent of this phenomenon in cross-lingual settings is hard.\nTo address this, we first develop a novel metric, mFACT, evaluating the\nfaithfulness of non-English summaries, leveraging translation-based transfer\nfrom multiple English faithfulness metrics. We then propose a simple but\neffective method to reduce hallucinations with a cross-lingual transfer, which\nweighs the loss of each training example by its faithfulness score. Through\nextensive experiments in multiple languages, we demonstrate that mFACT is the\nmetric that is most suited to detect hallucinations. Moreover, we find that our\nproposed loss weighting method drastically increases both performance and\nfaithfulness according to both automatic and human evaluation when compared to\nstrong baselines for cross-lingual transfer such as MAD-X. Our code and dataset\nare available at https://github.com/yfqiu-nlp/mfact-summ.",
        "score": -2.2936716079711914
      }
    ]
  },
  {
    "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models",
    "abstract": "  Alignment has become a critical step for instruction-tuned Large Language\nModels (LLMs) to become helpful assistants. However, the effective evaluation\nof alignment for emerging Chinese LLMs is still largely unexplored. To fill in\nthis gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark\nfor evaluating LLMs' alignment in Chinese. We design a human-in-the-loop data\ncuration pipeline, containing eight main categories, 683 real-scenario rooted\nqueries and corresponding human verified references. To ensure the correctness\nof references, each knowledge-intensive query is accompanied with evidences\ncollected from reliable web sources (including URLs and quotations) by our\nannotators. For automatic evaluation, our benchmark employs a rule-calibrated\nmulti-dimensional LLM-as-Judge~\\cite{zheng2023judging} approach with\nChain-of-Thought to generate explanations and final ratings, ensuring high\nreliability and interpretability. All evaluation code, data, and LLM\ngenerations are available at \\url{https://github.com/THUDM/AlignBench}. Since\nits release, AlignBench has been adopted by top (Chinese) LLMs for evaluating\ntheir alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi,\nBaichuan, and Abab.\n",
    "related_paper_titles": [
      "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for\n  Foundation Models",
      "A Survey on Evaluation of Large Language Models",
      "Principle-Driven Self-Alignment of Language Models from Scratch with\n  Minimal Human Supervision"
    ],
    "related_paper_abstract": [
      "  New NLP benchmarks are urgently needed to align with the rapid development of\nlarge language models (LLMs). We present C-Eval, the first comprehensive\nChinese evaluation suite designed to assess advanced knowledge and reasoning\nabilities of foundation models in a Chinese context. C-Eval comprises\nmultiple-choice questions across four difficulty levels: middle school, high\nschool, college, and professional. The questions span 52 diverse disciplines,\nranging from humanities to science and engineering. C-Eval is accompanied by\nC-Eval Hard, a subset of very challenging subjects in C-Eval that requires\nadvanced reasoning abilities to solve. We conduct a comprehensive evaluation of\nthe most advanced LLMs on C-Eval, including both English- and Chinese-oriented\nmodels. Results indicate that only GPT-4 could achieve an average accuracy of\nover 60%, suggesting that there is still significant room for improvement for\ncurrent LLMs. We anticipate C-Eval will help analyze important strengths and\nshortcomings of foundation models, and foster their development and growth for\nChinese users.\n",
      "  Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.\n",
      "  Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised\nfine-tuning (SFT) with human annotations and reinforcement learning from human\nfeedback (RLHF) to align the output of large language models (LLMs) with human\nintentions, ensuring they are helpful, ethical, and reliable. However, this\ndependence can significantly constrain the true potential of AI-assistant\nagents due to the high cost of obtaining human supervision and the related\nissues on quality, reliability, diversity, self-consistency, and undesirable\nbiases. To address these challenges, we propose a novel approach called\nSELF-ALIGN, which combines principle-driven reasoning and the generative power\nof LLMs for the self-alignment of AI agents with minimal human supervision. Our\napproach encompasses four stages: first, we use an LLM to generate synthetic\nprompts, and a topic-guided method to augment the prompt diversity; second, we\nuse a small set of human-written principles for AI models to follow, and guide\nthe LLM through in-context learning from demonstrations (of principles\napplication) to produce helpful, ethical, and reliable responses to user's\nqueries; third, we fine-tune the original LLM with the high-quality\nself-aligned responses so that the resulting model can generate desirable\nresponses for each query directly without the principle set and the\ndemonstrations anymore; and finally, we offer a refinement step to address the\nissues of overly-brief or indirect responses. Applying SELF-ALIGN to the\nLLaMA-65b base language model, we develop an AI assistant named Dromedary. With\nfewer than 300 lines of human annotations (including < 200 seed prompts, 16\ngeneric principles, and 5 exemplars for in-context learning). Dromedary\nsignificantly surpasses the performance of several state-of-the-art AI systems,\nincluding Text-Davinci-003 and Alpaca, on benchmark datasets with various\nsettings.\n"
    ],
    "entities": [
      "Large Language Models Large Language Models",
      "CAD",
      "Large Language",
      "Adam",
      "Persian",
      "DTs",
      "LLaMA",
      "SVM",
      "ICL",
      "ANN",
      "CoT",
      "WSIs",
      "LiDAR",
      "ESG",
      "OpenAI",
      "APIs",
      "SLMs",
      "Retrieval",
      "KGs",
      "KG",
      "Verilog",
      "KGE",
      "IFT",
      "Large Language Models Large",
      "Python",
      "Knowledge Graphs",
      "SQL",
      "Natural Language",
      "NER",
      "Thought"
    ],
    "retrieved_papers": [
      {
        "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models,",
        "abstract": "Alignment has become a critical step for instruction-tuned Large Language\nModels (LLMs) to become helpful assistants. However, effective evaluation of\nalignment for emerging Chinese LLMs is still significantly lacking, calling for\nreal-scenario grounded, open-ended, challenging and automatic evaluations\ntailored for alignment. To fill in this gap, we introduce AlignBench, a\ncomprehensive multi-dimensional benchmark for evaluating LLMs' alignment in\nChinese. Equipped with a human-in-the-loop data curation pipeline, our\nbenchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with\nChain-of-Thought to generate explanations and final ratings as evaluations,\nensuring high reliability and interpretability. Furthermore, we report\nAlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that\nrecovers 95% of GPT-4's evaluation ability. We will provide public APIs for\nevaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'\nChinese alignment. All evaluation codes, data, and LLM generations are\navailable at \\url{https://github.com/THUDM/AlignBench}.",
        "score": 4.8963303565979
      },
      {
        "title": "Towards Scalable Automated Alignment of LLMs: A Survey,",
        "abstract": "Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.",
        "score": 2.996164560317993
      },
      {
        "title": "OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and\n  Safety,",
        "abstract": "The rapid development of Chinese large language models (LLMs) poses big\nchallenges for efficient LLM evaluation. While current initiatives have\nintroduced new benchmarks or evaluation platforms for assessing Chinese LLMs,\nmany of these focus primarily on capabilities, usually overlooking potential\nalignment and safety issues. To address this gap, we introduce OpenEval, an\nevaluation testbed that benchmarks Chinese LLMs across capability, alignment\nand safety. For capability assessment, we include 12 benchmark datasets to\nevaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge,\ncommonsense reasoning and mathematical reasoning. For alignment assessment,\nOpenEval contains 7 datasets that examines the bias, offensiveness and\nillegalness in the outputs yielded by Chinese LLMs. To evaluate safety,\nespecially anticipated risks (e.g., power-seeking, self-awareness) of advanced\nLLMs, we include 6 datasets. In addition to these benchmarks, we have\nimplemented a phased public evaluation and benchmark update strategy to ensure\nthat OpenEval is in line with the development of Chinese LLMs or even able to\nprovide cutting-edge benchmark datasets to guide the development of Chinese\nLLMs. In our first public evaluation, we have tested a range of Chinese LLMs,\nspanning from 7B to 72B parameters, including both open-source and proprietary\nmodels. Evaluation results indicate that while Chinese LLMs have shown\nimpressive performance in certain tasks, more attention should be directed\ntowards broader aspects such as commonsense reasoning, alignment, and safety.",
        "score": 2.479985237121582
      },
      {
        "title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model,",
        "abstract": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.",
        "score": 2.319274663925171
      },
      {
        "title": "Getting More from Less: Large Language Models are Good Spontaneous\n  Multilingual Learners,",
        "abstract": "Recently, Large Language Models (LLMs) have shown impressive language\ncapabilities. While most of the existing LLMs have very unbalanced performance\nacross different languages, multilingual alignment based on translation\nparallel data is an effective method to enhance the LLMs' multilingual\ncapabilities. In this work, we discover and comprehensively investigate the\nspontaneous multilingual alignment improvement of LLMs. We find that LLMs\ninstruction-tuned on the question translation data (i.e. without annotated\nanswers) are able to encourage the alignment between English and a wide range\nof languages, even including those unseen during instruction-tuning.\nAdditionally, we utilize different settings and mechanistic interpretability\nmethods to analyze the LLM's performance in the multilingual scenario\ncomprehensively. Our work suggests that LLMs have enormous potential for\nimproving multilingual alignment efficiently with great language and task\ngeneralization.",
        "score": 2.2289183139801025
      },
      {
        "title": "Aligning Large Language Models with Human: A Survey,",
        "abstract": "Large Language Models (LLMs) trained on extensive textual corpora have\nemerged as leading solutions for a broad array of Natural Language Processing\n(NLP) tasks. Despite their notable performance, these models are prone to\ncertain limitations such as misunderstanding human instructions, generating\npotentially biased content, or factually incorrect (hallucinated) information.\nHence, aligning LLMs with human expectations has become an active area of\ninterest within the research community. This survey presents a comprehensive\noverview of these alignment technologies, including the following aspects. (1)\nData collection: the methods for effectively collecting high-quality\ninstructions for LLM alignment, including the use of NLP benchmarks, human\nannotations, and leveraging strong LLMs. (2) Training methodologies: a detailed\nreview of the prevailing training methods employed for LLM alignment. Our\nexploration encompasses Supervised Fine-tuning, both Online and Offline human\npreference training, along with parameter-efficient training mechanisms. (3)\nModel Evaluation: the methods for evaluating the effectiveness of these\nhuman-aligned LLMs, presenting a multifaceted approach towards their\nassessment. In conclusion, we collate and distill our findings, shedding light\non several promising future research avenues in the field. This survey,\ntherefore, serves as a valuable resource for anyone invested in understanding\nand advancing the alignment of LLMs to better suit human-oriented tasks and\nexpectations. An associated GitHub link collecting the latest papers is\navailable at https://github.com/GaryYufei/AlignLLMHumanSurvey.",
        "score": 2.213712215423584
      },
      {
        "title": "OpenAssistant Conversations -- Democratizing Large Language Model\n  Alignment,",
        "abstract": "Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.",
        "score": 2.1632723808288574
      },
      {
        "title": "CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the\n  Generalizability of Large Language Models,",
        "abstract": "The advancement of large language models (LLMs) has enhanced the ability to\ngeneralize across a wide range of unseen natural language processing (NLP)\ntasks through instruction-following. Yet, their effectiveness often diminishes\nin low-resource languages like Chinese, exacerbated by biased evaluations from\ndata leakage, casting doubt on their true generalizability to new linguistic\nterritories. In response, we introduce the Chinese Instruction-Following\nBenchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of\nLLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000\ninput-output pairs, developed by native speakers to test complex reasoning and\nChinese cultural nuances across 20 categories. To mitigate data contamination,\nwe release only half of the dataset publicly, with the remainder kept private,\nand introduce diversified instructions to minimize score variance, totaling\n45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable\nperformance gap, with the best model scoring only 52.9%, highlighting the\nlimitations of LLMs in less familiar language and task contexts. This work not\nonly uncovers the current limitations of LLMs in handling Chinese language\ntasks but also sets a new standard for future LLM generalizability research,\npushing towards the development of more adaptable, culturally informed, and\nlinguistically diverse models.",
        "score": 2.0784828662872314
      },
      {
        "title": "The Poison of Alignment,",
        "abstract": "From the perspective of content safety issues, alignment has shown to limit\nlarge language models' (LLMs) harmful content generation. This intentional\nmethod of reinforcing models to not respond to certain user inputs seem to be\npresent in many modern open-source instruction tuning datasets such as\nOpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned\nmodel's performance affected by the presence of alignment in supervised\nfine-tuning dataset. To be specific, we noticed that alignment acts as if it is\npoisoning the instruction dataset. Experimentally, we demonstrate that aligned\nanswers significantly worsen the performance of the resulting fine-tuned\nmodel's on various reasoning benchmarks such as Big Bench (BBH), Massive\nMultitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning\nOver Paragraphs (DROP), performing worse than the counterpart tuned without\nalignment by 4-33%.",
        "score": 2.003110647201538
      },
      {
        "title": "MedBench: A Comprehensive, Standardized, and Reliable Benchmarking\n  System for Evaluating Chinese Medical Large Language Models,",
        "abstract": "Ensuring the general efficacy and goodness for human beings from medical\nlarge language models (LLM) before real-world deployment is crucial. However, a\nwidely accepted and accessible evaluation process for medical LLM, especially\nin the Chinese context, remains to be established. In this work, we introduce\n\"MedBench\", a comprehensive, standardized, and reliable benchmarking system for\nChinese medical LLM. First, MedBench assembles the currently largest evaluation\ndataset (300,901 questions) to cover 43 clinical specialties and performs\nmulti-facet evaluation on medical LLM. Second, MedBench provides a standardized\nand fully automatic cloud-based evaluation infrastructure, with physical\nseparations for question and ground truth. Third, MedBench implements dynamic\nevaluation mechanisms to prevent shortcut learning and answer remembering.\nApplying MedBench to popular general and medical LLMs, we observe unbiased,\nreproducible evaluation results largely aligning with medical professionals'\nperspectives. This study establishes a significant foundation for preparing the\npractical applications of Chinese medical LLMs. MedBench is publicly accessible\nat https://medbench.opencompass.org.cn.",
        "score": 1.835595726966858
      },
      {
        "title": "Less is More for Improving Automatic Evaluation of Factual Consistency,",
        "abstract": "Assessing the factual consistency of automatically generated texts in\nrelation to source context is crucial for developing reliable natural language\ngeneration applications. Recent literature proposes AlignScore which uses a\nunified alignment model to evaluate factual consistency and substantially\noutperforms previous methods across many benchmark tasks. In this paper, we\ntake a closer look of datasets used in AlignScore and uncover an unexpected\nfinding: utilizing a smaller number of data points can actually improve\nperformance. We process the original AlignScore training dataset to remove\nnoise, augment with robustness-enhanced samples, and utilize a subset\ncomprising 10\\% of the data to train an improved factual consistency evaluation\nmodel, we call LIM-RA (Less Is More for Robust AlignScore). LIM-RA demonstrates\nsuperior performance, consistently outperforming AlignScore and other strong\nbaselines like ChatGPT across four benchmarks (two utilizing traditional\nnatural language generation datasets and two focused on large language model\noutputs). Our experiments show that LIM-RA achieves the highest score on 24 of\nthe 33 test datasets, while staying competitive on the rest, establishing the\nnew state-of-the-art benchmarks.",
        "score": 1.8034552335739136
      },
      {
        "title": "Aligner: Efficient Alignment by Learning to Correct,",
        "abstract": "With the rapid development of large language models (LLMs) and ever-evolving\npractical requirements, finding an efficient and effective alignment method has\nnever been more critical. However, the tension between the complexity of\ncurrent alignment methods and the need for rapid iteration in deployment\nscenarios necessitates the development of a model-agnostic alignment approach\nthat can operate under these constraints. In this paper, we introduce Aligner,\na novel and simple alignment paradigm that learns the correctional residuals\nbetween preferred and dispreferred answers using a small model. Designed as a\nmodel-agnostic, plug-and-play module, Aligner can be directly applied to\nvarious open-source and API-based models with only one-off training, making it\nsuitable for rapid iteration. Notably, Aligner can be applied to any powerful,\nlarge-scale upstream models. Moreover, it can even iteratively bootstrap the\nupstream models using corrected responses as synthetic human preference data,\nbreaking through the model's performance ceiling. Our experiments demonstrate\nperformance improvements by deploying the same Aligner model across 11\ndifferent LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and\nhonesty). Specifically, Aligner-7B has achieved an average improvement of 68.9%\nin helpfulness and 23.8% in harmlessness across the tested LLMs while also\neffectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking\nAligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%,\nsurpassing GPT-4 Omni's 57.5% Win Rate (community report).",
        "score": 1.7540267705917358
      },
      {
        "title": "Advancing the Evaluation of Traditional Chinese Language Models: Towards\n  a Comprehensive Benchmark Suite,",
        "abstract": "The evaluation of large language models is an essential task in the field of\nlanguage understanding and generation. As language models continue to advance,\nthe need for effective benchmarks to assess their performance has become\nimperative. In the context of Traditional Chinese, there is a scarcity of\ncomprehensive and diverse benchmarks to evaluate the capabilities of language\nmodels, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA,\nand FGC dataset. To address this gap, we propose a novel set of benchmarks that\nleverage existing English datasets and are tailored to evaluate language models\nin Traditional Chinese. These benchmarks encompass a wide range of tasks,\nincluding contextual question-answering, summarization, classification, and\ntable understanding. The proposed benchmarks offer a comprehensive evaluation\nframework, enabling the assessment of language models' capabilities across\ndifferent tasks. In this paper, we evaluate the performance of GPT-3.5,\nTaiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks.\nThe evaluation results highlight that our model, Model 7-C, achieves\nperformance comparable to GPT-3.5 with respect to a part of the evaluated\ncapabilities. In an effort to advance the evaluation of language models in\nTraditional Chinese and stimulate further research in this field, we have\nopen-sourced our benchmark and opened the model for trial.",
        "score": 1.7119296789169312
      },
      {
        "title": "Aligners: Decoupling LLMs and Alignment,",
        "abstract": "Large Language Models (LLMs) need to be aligned with human expectations to\nensure their safety and utility in most applications. Alignment is challenging,\ncostly, and needs to be repeated for every LLM and alignment criterion. We\npropose to decouple LLMs and alignment by training aligner models that can be\nused to align any LLM for a given criteria on an as-needed basis, thus also\nreducing the potential negative impacts of alignment on performance. Our recipe\nfor training the aligner models solely relies on synthetic data generated with\na (prompted) LLM and can be easily adjusted for a variety of alignment\ncriteria. We use the same synthetic data to train inspectors, binary\nmiss-alignment classification models to guide a \"squad\" of multiple aligners.\nOur empirical results demonstrate consistent improvements when applying aligner\nsquad to various LLMs, including chat-aligned models, across several\ninstruction-following and red-teaming datasets.",
        "score": 1.6862831115722656
      },
      {
        "title": "ArcMMLU: A Library and Information Science Benchmark for Large Language\n  Models,",
        "abstract": "In light of the rapidly evolving capabilities of large language models\n(LLMs), it becomes imperative to develop rigorous domain-specific evaluation\nbenchmarks to accurately assess their capabilities. In response to this need,\nthis paper introduces ArcMMLU, a specialized benchmark tailored for the Library\n& Information Science (LIS) domain in Chinese. This benchmark aims to measure\nthe knowledge and reasoning capability of LLMs within four key sub-domains:\nArchival Science, Data Science, Library Science, and Information Science.\nFollowing the format of MMLU/CMMLU, we collected over 6,000 high-quality\nquestions for the compilation of ArcMMLU. This extensive compilation can\nreflect the diverse nature of the LIS domain and offer a robust foundation for\nLLM evaluation. Our comprehensive evaluation reveals that while most mainstream\nLLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a\nnotable performance gap, suggesting substantial headroom for refinement in LLM\ncapabilities within the LIS domain. Further analysis explores the effectiveness\nof few-shot examples on model performance and highlights challenging questions\nwhere models consistently underperform, providing valuable insights for\ntargeted improvements. ArcMMLU fills a critical gap in LLM evaluations within\nthe Chinese LIS domain and paves the way for future development of LLMs\ntailored to this specialized area.",
        "score": 1.6198924779891968
      },
      {
        "title": "A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO,\n  DPO and More,",
        "abstract": "With advancements in self-supervised learning, the availability of trillions\ntokens in a pre-training corpus, instruction fine-tuning, and the development\nof large Transformers with billions of parameters, large language models (LLMs)\nare now capable of generating factual and coherent responses to human queries.\nHowever, the mixed quality of training data can lead to the generation of\nundesired responses, presenting a significant challenge. Over the past two\nyears, various methods have been proposed from different perspectives to\nenhance LLMs, particularly in aligning them with human expectation. Despite\nthese efforts, there has not been a comprehensive survey paper that categorizes\nand details these approaches. In this work, we aim to address this gap by\ncategorizing these papers into distinct topics and providing detailed\nexplanations of each alignment method, thereby helping readers gain a thorough\nunderstanding of the current state of the field.",
        "score": 1.6122605800628662
      },
      {
        "title": "C$^{3}$Bench: A Comprehensive Classical Chinese Understanding Benchmark\n  for Large Language Models,",
        "abstract": "Classical Chinese Understanding (CCU) holds significant value in preserving\nand exploration of the outstanding traditional Chinese culture. Recently,\nresearchers have attempted to leverage the potential of Large Language Models\n(LLMs) for CCU by capitalizing on their remarkable comprehension and semantic\ncapabilities. However, no comprehensive benchmark is available to assess the\nCCU capabilities of LLMs. To fill this gap, this paper introduces C$^{3}$bench,\na Comprehensive Classical Chinese understanding benchmark, which comprises\n50,000 text pairs for five primary CCU tasks, including classification,\nretrieval, named entity recognition, punctuation, and translation. Furthermore,\nthe data in C$^{3}$bench originates from ten different domains, covering most\nof the categories in classical Chinese. Leveraging the proposed C$^{3}$bench,\nwe extensively evaluate the quantitative performance of 15 representative LLMs\non all five CCU tasks. Our results not only establish a public leaderboard of\nLLMs' CCU capabilities but also gain some findings. Specifically, existing LLMs\nare struggle with CCU tasks and still inferior to supervised models.\nAdditionally, the results indicate that CCU is a task that requires special\nattention. We believe this study could provide a standard benchmark,\ncomprehensive baselines, and valuable insights for the future advancement of\nLLM-based CCU research. The evaluation pipeline and dataset are available at\n\\url{https://github.com/SCUT-DLVCLab/C3bench}.",
        "score": 1.5491491556167603
      },
      {
        "title": "CLEVA: Chinese Language Models EVAluation Platform,",
        "abstract": "With the continuous emergence of Chinese Large Language Models (LLMs), how to\nevaluate a model's capabilities has become an increasingly significant issue.\nThe absence of a comprehensive Chinese benchmark that thoroughly assesses a\nmodel's performance, the unstandardized and incomparable prompting procedure,\nand the prevalent risk of contamination pose major challenges in the current\nevaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted\nto holistically evaluate Chinese LLMs. Our platform employs a standardized\nworkflow to assess LLMs' performance across various dimensions, regularly\nupdating a competitive leaderboard. To alleviate contamination, CLEVA curates a\nsignificant proportion of new data and develops a sampling strategy that\nguarantees a unique subset for each leaderboard round. Empowered by an\neasy-to-use interface that requires just a few mouse clicks and a model API,\nusers can conduct a thorough evaluation with minimal coding. Large-scale\nexperiments featuring 23 Chinese LLMs have validated CLEVA's efficacy.",
        "score": 1.5021815299987793
      },
      {
        "title": "COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning,",
        "abstract": "Recently, there have been significant advancements in large language models\n(LLMs), particularly focused on the English language. These advancements have\nenabled these LLMs to understand and execute complex instructions with\nunprecedented accuracy and fluency. However, despite these advancements, there\nremains a noticeable gap in the development of Chinese instruction tuning. The\nunique linguistic features and cultural depth of the Chinese language pose\nchallenges for instruction tuning tasks. Existing datasets are either derived\nfrom English-centric LLMs or are ill-suited for aligning with the interaction\npatterns of real-world Chinese users. To bridge this gap, we introduce\nCOIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to\nbuild a diverse, wide-ranging instruction-tuning dataset to better align model\nbehavior with human interactions. To this end, we collect a high-quality\nhuman-written corpus from various sources on the Chinese Internet, including\nQ&A communities, Wikis, examinations, and existing NLP datasets. This corpus\nwas rigorously filtered and carefully processed to form the COIG-CQIA dataset.\nFurthermore, we train models of various scales on different subsets of CQIA,\nfollowing in-depth evaluation and analyses. The findings from our experiments\noffer valuable insights for selecting and developing Chinese instruction-tuning\ndatasets. We also find that models trained on CQIA-Subset achieve competitive\nresults in human assessment as well as knowledge and security benchmarks. Data\nare available at https://huggingface.co/datasets/m-a-p/COIG-CQIA",
        "score": 1.4246411323547363
      },
      {
        "title": "TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for\n  Human-Aligned LLMs,",
        "abstract": "Large language models (LLMs) have shown impressive capabilities across\nvarious natural language tasks. However, evaluating their alignment with human\npreferences remains a challenge. To this end, we propose a comprehensive human\nevaluation framework to assess LLMs' proficiency in following instructions on\ndiverse real-world tasks. We construct a hierarchical task tree encompassing 7\nmajor areas covering over 200 categories and over 800 tasks, which covers\ndiverse capabilities such as question answering, reasoning, multiturn dialogue,\nand text generation, to evaluate LLMs in a comprehensive and in-depth manner.\nWe also design detailed evaluation standards and processes to facilitate\nconsistent, unbiased judgments from human evaluators. A test set of over 3,000\ninstances is released, spanning different difficulty levels and knowledge\ndomains. Our work provides a standardized methodology to evaluate human\nalignment in LLMs for both English and Chinese. We also analyze the feasibility\nof automating parts of evaluation with a strong LLM (GPT-4). Our framework\nsupports a thorough assessment of LLMs as they are integrated into real-world\napplications. We have made publicly available the task tree, TencentLLMEval\ndataset, and evaluation methodology which have been demonstrated as effective\nin assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to\nfacilitate the benchmarking of advances in the development of safe and\nhuman-aligned LLMs.",
        "score": 1.4089479446411133
      },
      {
        "title": "Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question\n  Answering Benchmark,",
        "abstract": "How to better evaluate the capabilities of Large Language Models (LLMs) is\nthe focal point and hot topic in current LLMs research. Previous work has noted\nthat due to the extremely high cost of iterative updates of LLMs, they are\noften unable to answer the latest dynamic questions well. To promote the\nimprovement of Chinese LLMs' ability to answer dynamic questions, in this\npaper, we introduce CDQA, a Chinese Dynamic QA benchmark containing\nquestion-answer pairs related to the latest news on the Chinese Internet. We\nobtain high-quality data through a pipeline that combines humans and models,\nand carefully classify the samples according to the frequency of answer changes\nto facilitate a more fine-grained observation of LLMs' capabilities. We have\nalso evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA.\nExtensive experiments and valuable insights suggest that our proposed CDQA is\nchallenging and worthy of more further study. We believe that the benchmark we\nprovide will become one of the key data resources for improving LLMs' Chinese\nquestion-answering ability in the future.",
        "score": 1.3126708269119263
      },
      {
        "title": "CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large\n  Language Models,",
        "abstract": "Text-to-Table aims to generate structured tables to convey the key\ninformation from unstructured documents. Existing text-to-table datasets are\ntypically oriented English, limiting the research in non-English languages.\nMeanwhile, the emergence of large language models (LLMs) has shown great\nsuccess as general task solvers in multi-lingual settings (e.g., ChatGPT),\ntheoretically enabling text-to-table in other languages. In this paper, we\npropose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this\ntask. Our preliminary analysis of English text-to-table datasets highlights two\nkey factors for dataset construction: data diversity and data hallucination.\nInspired by this, the CT-Eval dataset selects a popular Chinese\nmultidisciplinary online encyclopedia as the source and covers 28 domains to\nensure data diversity. To minimize data hallucination, we first train an LLM to\njudge and filter out the task samples with hallucination, then employ human\nannotators to clean the hallucinations in the validation and testing sets.\nAfter this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we\nevaluate the performance of open-source and closed-source LLMs. Our results\nreveal that zero-shot LLMs (including GPT-4) still have a significant\nperformance gap compared with human judgment. Furthermore, after fine-tuning,\nopen-source LLMs can significantly improve their text-to-table ability,\noutperforming GPT-4 by a large margin. In short, CT-Eval not only helps\nresearchers evaluate and quickly understand the Chinese text-to-table ability\nof existing LLMs but also serves as a valuable resource to significantly\nimprove the text-to-table performance of LLMs.",
        "score": 1.0370737314224243
      },
      {
        "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation,",
        "abstract": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun",
        "score": 1.014062762260437
      },
      {
        "title": "ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large\n  Language Models,",
        "abstract": "The unprecedented performance of large language models (LLMs) requires\ncomprehensive and accurate evaluation. We argue that for LLMs evaluation,\nbenchmarks need to be comprehensive and systematic. To this end, we propose the\nZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional\nability coverage: We comprehensively evaluate LLMs across 7 ability dimensions\ncovering 51 tasks. Especially, we also propose a new benchmark that focuses on\nknowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration:\nWe use 3 different yet complementary evaluation methods to comprehensively\nevaluate LLMs, which can ensure the authority and accuracy of the evaluation\nresults. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering\nbenchmark that fully assesses LLMs in Chinese, while also providing equally\nrobust evaluation abilities in English. (4) Avoiding potential data leakage: To\navoid data leakage, we construct evaluation data specifically for 37 tasks. We\nevaluate 10 current mainstream LLMs and conduct an in-depth discussion and\nanalysis of their results. The ZhuJiu benchmark and open-participation\nleaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we\nalso provide a demo video at https://youtu.be/qypkJ89L1Ic.",
        "score": 0.46097806096076965
      },
      {
        "title": "Generative Judge for Evaluating Alignment,",
        "abstract": "The rapid development of Large Language Models (LLMs) has substantially\nexpanded the range of tasks they can address. In the field of Natural Language\nProcessing (NLP), researchers have shifted their focus from conventional NLP\ntasks (e.g., sequence tagging and parsing) towards tasks that revolve around\naligning with human needs (e.g., brainstorming and email writing). This shift\nin task distribution imposes new requirements on evaluating these aligned\nmodels regarding generality (i.e., assessing performance across diverse\nscenarios), flexibility (i.e., examining under different protocols), and\ninterpretability (i.e., scrutinizing models with explanations). In this paper,\nwe propose a generative judge with 13B parameters, Auto-J, designed to address\nthese challenges. Our model is trained on user queries and LLM-generated\nresponses under massive real-world scenarios and accommodates diverse\nevaluation protocols (e.g., pairwise response comparison and single-response\nevaluation) with well-structured natural language critiques. To demonstrate the\nefficacy of our approach, we construct a new testbed covering 58 different\nscenarios. Experimentally, Auto-J outperforms a series of strong competitors,\nincluding both open-source and closed-source models, by a large margin. We also\nprovide detailed analysis and case studies to further reveal the potential of\nour method and make a variety of resources public at\nhttps://github.com/GAIR-NLP/auto-j.",
        "score": 0.39976370334625244
      },
      {
        "title": "WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search\n  Results with Citations,",
        "abstract": "Enhancing the attribution in large language models (LLMs) is a crucial task.\nOne feasible approach is to enable LLMs to cite external sources that support\ntheir generations. However, existing datasets and evaluation methods in this\ndomain still exhibit notable limitations. In this work, we formulate the task\nof attributed query-focused summarization (AQFS) and present WebCiteS, a\nChinese dataset featuring 7k human-annotated summaries with citations. WebCiteS\nderives from real-world user queries and web search results, offering a\nvaluable resource for model training and evaluation. Prior works in attribution\nevaluation do not differentiate between groundedness errors and citation\nerrors. They also fall short in automatically verifying sentences that draw\npartial support from multiple sources. We tackle these issues by developing\ndetailed metrics and enabling the automatic evaluator to decompose the\nsentences into sub-claims for fine-grained verification. Our comprehensive\nevaluation of both open-source and proprietary models on WebCiteS highlights\nthe challenge LLMs face in correctly citing sources, underscoring the necessity\nfor further improvement. The dataset and code will be open-sourced to\nfacilitate further research in this crucial field.",
        "score": 0.22737160325050354
      },
      {
        "title": "Towards Better Instruction Following Language Models for Chinese:\n  Investigating the Impact of Training Data and Evaluation,",
        "abstract": "Recently, significant public efforts have been directed towards developing\nlow-cost models with capabilities akin to ChatGPT, thereby fostering the growth\nof open-source conversational models. However, there remains a scarcity of\ncomprehensive and in-depth evaluations of these models' performance. In this\nstudy, we examine the influence of training data factors, including quantity,\nquality, and linguistic distribution, on model performance. Our analysis is\ngrounded in several publicly accessible, high-quality instruction datasets, as\nwell as our own Chinese multi-turn conversations. We assess various models\nusing a evaluation set of 1,000 samples, encompassing nine real-world\nscenarios. Our goal is to supplement manual evaluations with quantitative\nanalyses, offering valuable insights for the continued advancement of\nopen-source chat models. Furthermore, to enhance the performance and training\nand inference efficiency of models in the Chinese domain, we extend the\nvocabulary of LLaMA - the model with the closest open-source performance to\nproprietary language models like GPT-3 - and conduct secondary pre-training on\n3.4B Chinese words. We make our model, data, as well as code publicly\navailable.",
        "score": 0.22216935455799103
      },
      {
        "title": "WYWEB: A NLP Evaluation Benchmark For Classical Chinese,",
        "abstract": "To fully evaluate the overall performance of different NLP models in a given\ndomain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and\nCLUE. The fi eld of natural language understanding has traditionally focused on\nbenchmarks for various tasks in languages such as Chinese, English, and\nmultilingua, however, there has been a lack of attention given to the area of\nclassical Chinese, also known as \"wen yan wen\", which has a rich history\nspanning thousands of years and holds signifi cant cultural and academic value.\nFor the prosperity of the NLP community, in this paper, we introduce the WYWEB\nevaluation benchmark, which consists of nine NLP tasks in classical Chinese,\nimplementing sentence classifi cation, sequence labeling, reading\ncomprehension, and machine translation. We evaluate the existing pre-trained\nlanguage models, which are all struggling with this benchmark. We also\nintroduce a number of supplementary datasets and additional tools to help\nfacilitate further progress on classical Chinese NLU. The github repository is\nhttps://github.com/baudzhou/WYWEB.",
        "score": 0.010038663633167744
      },
      {
        "title": "Tele-FLM Technical Report,",
        "abstract": "Large language models (LLMs) have showcased profound capabilities in language\nunderstanding and generation, facilitating a wide array of applications.\nHowever, there is a notable paucity of detailed, open-sourced methodologies on\nefficiently scaling LLMs beyond 50 billion parameters with minimum\ntrial-and-error cost and computational resources. In this report, we introduce\nTele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that\nfeatures a stable, efficient pre-training paradigm and enhanced factual\njudgment capabilities. Tele-FLM demonstrates superior multilingual language\nmodeling abilities, measured by BPB on textual corpus. Besides, in both English\nand Chinese foundation model evaluation, it is comparable to strong\nopen-sourced models that involve larger pre-training FLOPs, such as Llama2-70B\nand DeepSeek-67B. In addition to the model weights, we share the core designs,\nengineering practices, and training details, which we expect to benefit both\nthe academic and industrial communities.",
        "score": -0.254512757062912
      },
      {
        "title": "X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and\n  Few-shot Agents,",
        "abstract": "Task-oriented dialogue research has mainly focused on a few popular languages\nlike English and Chinese, due to the high dataset creation cost for a new\nlanguage. To reduce the cost, we apply manual editing to automatically\ntranslated data. We create a new multilingual benchmark, X-RiSAWOZ, by\ntranslating the Chinese RiSAWOZ to 4 languages: English, French, Hindi, Korean;\nand a code-mixed English-Hindi language. X-RiSAWOZ has more than 18,000\nhuman-verified dialogue utterances for each language, and unlike most\nmultilingual prior work, is an end-to-end dataset for building\nfully-functioning agents.\n  The many difficulties we encountered in creating X-RiSAWOZ led us to develop\na toolset to accelerate the post-editing of a new language dataset after\ntranslation. This toolset improves machine translation with a hybrid entity\nalignment technique that combines neural with dictionary-based methods, along\nwith many automated and semi-automated validation checks.\n  We establish strong baselines for X-RiSAWOZ by training dialogue agents in\nthe zero- and few-shot settings where limited gold data is available in the\ntarget language. Our results suggest that our translation and post-editing\nmethodology and toolset can be used to create new high-quality multilingual\ndialogue agents cost-effectively. Our dataset, code, and toolkit are released\nopen-source.",
        "score": -0.4935638904571533
      }
    ]
  },
  {
    "title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language\n  Models via Complexity Classes",
    "abstract": "  Complex reasoning ability is one of the most important features of current\nLLMs, which has also been leveraged to play an integral role in complex\ndecision-making tasks. Therefore, the investigation into the reasoning\ncapabilities of Large Language Models (LLMs) is critical: numerous benchmarks\nhave been established to assess the reasoning abilities of LLMs. However,\ncurrent benchmarks are inadequate in offering a rigorous evaluation of the full\nextent of reasoning abilities that LLMs are capable of achieving. They are also\nprone to the risk of overfitting, as these benchmarks, being publicly\naccessible and static, allow models to potentially tailor their responses to\nspecific benchmark metrics, thereby inflating their performance. Addressing\nthese limitations, our research introduces a new benchmark, named NPHardEval.\nThis benchmark is designed to evaluate the reasoning abilities of LLMs across a\nbroad spectrum of 900 algorithmic questions, extending up to the NP-Hard\ncomplexity class. These questions are meticulously chosen to represent a wide\nrange of complexity class below the NP-hard complexity class, offering a\nrigorous measure of the reasoning ability of LLMs. Through this study, we shed\nlight on the current state of reasoning in LLMs, providing an objective and\nrigorous perspective through the comparison of LLMs' performance across complex\nclasses. Moreover, this benchmark is designed with a dynamic update mechanism,\nwhere the datapoints are refreshed on a monthly basis. Such regular updates\nplay a crucial role in mitigating the risk of LLMs overfitting to the\nbenchmark, promoting a more accurate and reliable assessment of their reasoning\ncapabilities. The benchmark dataset and code of NPHardEval are available at\nhttps://github.com/casmlab/NPHardEval.\n",
    "related_paper_titles": [
      "Towards Reasoning in Large Language Models: A Survey",
      "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for\n  Reasoning Problems",
      "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks"
    ],
    "related_paper_abstract": [
      "  Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.\n",
      "  There has been considerable divergence of opinion on the reasoning abilities\nof Large Language Models (LLMs). While the initial optimism that reasoning\nmight emerge automatically with scale has been tempered thanks to a slew of\ncounterexamples, a wide spread belief in their iterative self-critique\ncapabilities persists. In this paper, we set out to systematically investigate\nthe effectiveness of iterative prompting of LLMs in the context of Graph\nColoring, a canonical NP-complete reasoning problem that is related to\npropositional satisfiability as well as practical problems like scheduling and\nallocation. We present a principled empirical study of the performance of GPT4\nin solving graph coloring instances or verifying the correctness of candidate\ncolorings. In iterative modes, we experiment with the model critiquing its own\nanswers and an external correct reasoner verifying proposed solutions. In both\ncases, we analyze whether the content of the criticisms actually affects bottom\nline performance. The study seems to indicate that (i) LLMs are bad at solving\ngraph coloring instances (ii) they are no better at verifying a solution--and\nthus are not effective in iterative modes with LLMs critiquing LLM-generated\nsolutions (iii) the correctness and content of the criticisms--whether by LLMs\nor external solvers--seems largely irrelevant to the performance of iterative\nprompting. We show that the observed increase in effectiveness is largely due\nto the correct solution being fortuitously present in the top-k completions of\nthe prompt (and being recognized as such by an external verifier). Our results\nthus call into question claims about the self-critiquing capabilities of state\nof the art LLMs.\n",
      "  Large language models (LLMs) have achieved remarkable performance in various\nevaluation benchmarks. However, concerns are raised about potential data\ncontamination in their considerable volume of training corpus. Moreover, the\nstatic nature and fixed complexity of current benchmarks may inadequately gauge\nthe advancing capabilities of LLMs. In this paper, we introduce DyVal, a\ngeneral and flexible protocol for dynamic evaluation of LLMs. Based on our\nframework, we build graph-informed DyVal by leveraging the structural advantage\nof directed acyclic graphs to dynamically generate evaluation samples with\ncontrollable complexities. DyVal generates challenging evaluation sets on\nreasoning tasks including mathematics, logical reasoning, and algorithm\nproblems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo\nand GPT-4. Experiments show that LLMs perform worse in DyVal-generated\nevaluation samples with different complexities, highlighting the significance\nof dynamic evaluation. We also analyze the failure cases and results of\ndifferent prompting methods. Moreover, DyVal-generated samples are not only\nevaluation sets, but also helpful data for fine-tuning to improve the\nperformance of LLMs on existing benchmarks. We hope that DyVal can shed light\non future evaluation research of LLMs. Code is available at:\nhttps://github.com/microsoft/promptbench.\n"
    ],
    "entities": [
      "Large Language Models Large Language Models",
      "RAG",
      "Mistral",
      "Large",
      "ESG",
      "LLMs",
      "LLaMA",
      "Large Language Models",
      "SFT",
      "Persian",
      "Llama",
      "ICL",
      "KGE",
      "Language Models",
      "CoT",
      "ChatGPT",
      "MATH",
      "KV",
      "DPO",
      "Retrieval Augmented",
      "IFT",
      "PDE",
      "OpenAI",
      "Human Feedback",
      "PDEs",
      "NLG",
      "Direct Preference",
      "GPT",
      "Knowledge Graphs",
      "Retrieval"
    ],
    "retrieved_papers": [
      {
        "title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language\n  Models via Complexity Classes,",
        "abstract": "Complex reasoning ability is one of the most important features of current\nLLMs, which has also been leveraged to play an integral role in complex\ndecision-making tasks. Therefore, the investigation into the reasoning\ncapabilities of Large Language Models (LLMs) is critical: numerous benchmarks\nhave been established to assess the reasoning abilities of LLMs. However,\ncurrent benchmarks are inadequate in offering a rigorous evaluation of the full\nextent of reasoning abilities that LLMs are capable of achieving. They are also\nprone to the risk of overfitting, as these benchmarks, being publicly\naccessible and static, allow models to potentially tailor their responses to\nspecific benchmark metrics, thereby inflating their performance. Addressing\nthese limitations, our research introduces a new benchmark, named NPHardEval.\nThis benchmark is designed to evaluate the reasoning abilities of LLMs across a\nbroad spectrum of 900 algorithmic questions, extending up to the NP-Hard\ncomplexity class. These questions are meticulously chosen to represent a wide\nrange of complexity class below the NP-hard complexity class, offering a\nrigorous measure of the reasoning ability of LLMs. Through this study, we shed\nlight on the current state of reasoning in LLMs, providing an objective and\nrigorous perspective through the comparison of LLMs' performance across complex\nclasses. Moreover, this benchmark is designed with a dynamic update mechanism,\nwhere the datapoints are refreshed on a monthly basis. Such regular updates\nplay a crucial role in mitigating the risk of LLMs overfitting to the\nbenchmark, promoting a more accurate and reliable assessment of their reasoning\ncapabilities. The benchmark dataset and code of NPHardEval are available at\nhttps://github.com/casmlab/NPHardEval.",
        "score": 7.287429332733154
      },
      {
        "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning\n  Graph,",
        "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.",
        "score": 4.100985527038574
      },
      {
        "title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks,",
        "abstract": "Large language models (LLMs) have achieved remarkable performance in various\nevaluation benchmarks. However, concerns are raised about potential data\ncontamination in their considerable volume of training corpus. Moreover, the\nstatic nature and fixed complexity of current benchmarks may inadequately gauge\nthe advancing capabilities of LLMs. In this paper, we introduce DyVal, a\ngeneral and flexible protocol for dynamic evaluation of LLMs. Based on our\nframework, we build graph-informed DyVal by leveraging the structural advantage\nof directed acyclic graphs to dynamically generate evaluation samples with\ncontrollable complexities. DyVal generates challenging evaluation sets on\nreasoning tasks including mathematics, logical reasoning, and algorithm\nproblems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo\nand GPT-4. Experiments show that LLMs perform worse in DyVal-generated\nevaluation samples with different complexities, highlighting the significance\nof dynamic evaluation. We also analyze the failure cases and results of\ndifferent prompting methods. Moreover, DyVal-generated samples are not only\nevaluation sets, but also helpful data for fine-tuning to improve the\nperformance of LLMs on existing benchmarks. We hope that DyVal can shed light\non future evaluation research of LLMs. Code is available at:\nhttps://github.com/microsoft/promptbench.",
        "score": 3.7348544597625732
      },
      {
        "title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large\n  Language Models,",
        "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across\ndiverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However,\nsuch a simple and fast COT approach often encounters limitations in dealing\nwith complicated problems, while a thorough method, which considers multiple\nreasoning pathways and verifies each step carefully, results in slower\ninference. This paper addresses the challenge of enabling LLMs to autonomously\nselect between fast and slow inference methods, thereby optimizing both\nefficiency and effectiveness. We introduce a dynamic decision-making framework\nthat categorizes tasks into two distinct pathways: 'Fast', designated for tasks\nwhere the LLM quickly identifies a high-confidence solution, and 'Slow',\nallocated for tasks that the LLM perceives as complex and for which it has low\nconfidence in immediate solutions as well as requiring more reasoning paths to\nverify. Experiments on five popular reasoning benchmarks demonstrated the\nsuperiority of the DynaThink over baselines.",
        "score": 3.5267114639282227
      },
      {
        "title": "Assessing the Emergent Symbolic Reasoning Abilities of Llama Large\n  Language Models,",
        "abstract": "Large Language Models (LLMs) achieve impressive performance in a wide range\nof tasks, even if they are often trained with the only objective of chatting\nfluently with users. Among other skills, LLMs show emergent abilities in\nmathematical reasoning benchmarks, which can be elicited with appropriate\nprompting methods. In this work, we systematically investigate the capabilities\nand limitations of popular open-source LLMs on different symbolic reasoning\ntasks. We evaluate three models of the Llama 2 family on two datasets that\nrequire solving mathematical formulas of varying degrees of difficulty. We test\na generalist LLM (Llama 2 Chat) as well as two fine-tuned versions of Llama 2\n(MAmmoTH and MetaMath) specifically designed to tackle mathematical problems.\nWe observe that both increasing the scale of the model and fine-tuning it on\nrelevant tasks lead to significant performance gains. Furthermore, using\nfine-grained evaluation measures, we find that such performance gains are\nmostly observed with mathematical formulas of low complexity, which\nnevertheless often remain challenging even for the largest fine-tuned models.",
        "score": 3.3976268768310547
      },
      {
        "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language\n  Models' Reasoning Performance,",
        "abstract": "As large language models (LLMs) are continuously being developed, their\nevaluation becomes increasingly important yet challenging. This work proposes\nChain-of-Thought Hub, an open-source evaluation suite on the multi-step\nreasoning capabilities of large language models. We are interested in this\nsetting for two reasons: (1) from the behavior of GPT and PaLM model family, we\nobserve that complex reasoning is likely to be a key differentiator between\nweaker and stronger LLMs; (2) we envisage large language models to become the\nnext-generation computational platform and foster an ecosystem of LLM-based new\napplications, this naturally requires the foundation models to perform complex\ntasks that often involve the composition of linguistic and logical operations.\nOur approach is to compile a suite of challenging reasoning benchmarks to track\nthe progress of LLMs. Our current results show that: (1) model scale clearly\ncorrelates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and\nPaLM-2 are the only two models that are comparable with GPT-4, while\nopen-sourced models still lag behind; (3) LLaMA-65B performs closely to\ncode-davinci-002, indicating that with successful further development such as\nreinforcement learning from human feedback (RLHF), it has great potential to be\nclose to GPT-3.5-Turbo. Our results also suggest that for the open-source\nefforts to catch up, the community may focus more on building better base\nmodels and exploring RLHF.",
        "score": 3.3470022678375244
      },
      {
        "title": "LLMs for Relational Reasoning: How Far are We?,",
        "abstract": "Large language models (LLMs) have revolutionized many areas (e.g. natural\nlanguage processing, software engineering, etc.) by achieving state-of-the-art\nperformance on extensive downstream tasks. Aiming to achieve robust and general\nartificial intelligence, there has been a surge of interest in investigating\nthe reasoning ability of the LLMs. Whereas the textual and numerical reasoning\nbenchmarks adopted by previous works are rather shallow and simple, it is hard\nto conclude that the LLMs possess strong reasoning ability by merely achieving\npositive results on these benchmarks. Recent efforts have demonstrated that the\nLLMs are poor at solving sequential decision-making problems that require\ncommon-sense planning by evaluating their performance on the reinforcement\nlearning benchmarks. In this work, we conduct an in-depth assessment of several\nstate-of-the-art LLMs' reasoning ability based on the inductive logic\nprogramming (ILP) benchmark, which is broadly recognized as a representative\nand challenging measurement for evaluating logic program induction/synthesis\nsystems as it requires inducing strict cause-effect logic to achieve robust\ndeduction on independent and identically distributed (IID) and\nout-of-distribution (OOD) test samples. Our evaluations illustrate that\ncompared with the neural program induction systems which are much smaller in\nmodel size, the state-of-the-art LLMs are much poorer in terms of reasoning\nability by achieving much lower performance and generalization using either\nnatural language prompting or truth-value matrix prompting.",
        "score": 3.325892925262451
      },
      {
        "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical\n  Reasoning Capabilities of Language Models,",
        "abstract": "Logical reasoning is fundamental for humans yet presents a substantial\nchallenge in the domain of Artificial Intelligence. Initially, researchers used\nKnowledge Representation and Reasoning (KR) systems that did not scale and\nrequired non-trivial manual effort. Recently, the emergence of large language\nmodels (LLMs) has demonstrated the ability to overcome various limitations of\nformal Knowledge Representation (KR) systems. Consequently, there's a growing\ninterest in using LLMs for logical reasoning via natural language. This work\nstrives to understand the proficiency of LLMs in logical reasoning by offering\na brief review of the latest progress in this area; with a focus on the logical\nreasoning datasets, tasks, and the methods adopted to utilize LLMs for\nreasoning. To offer a thorough analysis, we have compiled a benchmark titled\nLogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,\nand inductive reasoning. Utilizing LogiGLUE as a foundation, we have trained an\ninstruction fine-tuned language model, resulting in LogiT5. We study\nsingle-task training, multi-task training, and \"chain-of-thought\" knowledge\ndistillation fine-tuning technique to assess the performance of model across\nthe different logical reasoning categories. We also assess various LLMs using\nLogiGLUE, and the findings indicate that LLMs excel most in abductive\nreasoning, followed by deductive reasoning, while they are least effective at\ninductive reasoning. We aim to shed light on the capabilities and potential\npathways for enhancing logical reasoning proficiency in LLMs, paving the way\nfor more advanced and nuanced developments in this critical field.",
        "score": 3.318277597427368
      },
      {
        "title": "RUPBench: Benchmarking Reasoning Under Perturbations for Robustness\n  Evaluation in Large Language Models,",
        "abstract": "With the increasing use of large language models (LLMs), ensuring reliable\nperformance in diverse, real-world environments is essential. Despite their\nremarkable achievements, LLMs often struggle with adversarial inputs,\nsignificantly impacting their effectiveness in practical applications. To\nsystematically understand the robustness of LLMs, we present RUPBench, a\ncomprehensive benchmark designed to evaluate LLM robustness across diverse\nreasoning tasks. Our benchmark incorporates 15 reasoning datasets, categorized\ninto commonsense, arithmetic, logical, and knowledge-intensive reasoning, and\nintroduces nine types of textual perturbations at lexical, syntactic, and\nsemantic levels. By examining the performance of state-of-the-art LLMs such as\nGPT-4o, Llama3, Phi-3, and Gemma on both original and perturbed datasets, we\nprovide a detailed analysis of their robustness and error patterns. Our\nfindings highlight that larger models tend to exhibit greater robustness to\nperturbations. Additionally, common error types are identified through manual\ninspection, revealing specific challenges faced by LLMs in different reasoning\ncontexts. This work provides insights into areas where LLMs need further\nimprovement to handle diverse and noisy inputs effectively.",
        "score": 3.300436496734619
      },
      {
        "title": "A & B == B & A: Triggering Logical Reasoning Failures in Large Language\n  Models,",
        "abstract": "Recent advancements in large language models (LLMs) have propelled Artificial\nIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\nas writing assistance, code generation, and machine translation. A significant\ndistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\n\"reason.\" However, evaluating the reasoning ability of LLMs remains a challenge\nas most existing evaluations focus on their accuracy on the downstream tasks\nrather than directly assessing their reasoning processes. Efforts have been\nmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\nsuffer from data leakage or limited scope. In this paper, we introduce\nLogicAsker, an automatic approach that comprehensively evaluates and improves\nthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\nbased on propositional and predicate logic. The results provide insights into\nLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\nwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\nChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\nfrom LogicAsker can find logical reasoning failures in different LLMs with a\nrate of 25\\% - 94\\%. In addition, the test cases of LogicAsker can be further\nused to design demonstration examples for in-context learning, which\neffectively improves the logical reasoning ability of LLMs, e.g., 10\\% for\nGPT-4. As far as we know, our work is the first to create prompts based on\ntesting results to improve LLMs' formal reasoning ability effectively. All the\ncode, data, and results will be released for reproduction and future research.",
        "score": 3.260502576828003
      },
      {
        "title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large\n  Language Models,",
        "abstract": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical\nreasoning on large language models (LLMs), combining ruleset interpretation,\nplanning, and problem-solving. This benchmark is inspired by the Mathador game,\nwhere the objective is to reach a target number using basic arithmetic\noperations on a given set of base numbers, following a simple set of rules. We\nshow that, across leading LLMs, we obtain stable average performance while\ngenerating benchmark instances dynamically, following a target difficulty\nlevel. Thus, our benchmark alleviates concerns about test-set leakage into\ntraining data, an issue that often undermines popular benchmarks. Additionally,\nwe conduct a comprehensive evaluation of both open and closed-source\nstate-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary\nmodels struggle with Mathador-LM, scoring significantly lower than average 3rd\ngraders. This stands in stark contrast to their strong performance on popular\nmathematical reasoning benchmarks.",
        "score": 3.2054316997528076
      },
      {
        "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language\n  Models -- A Survey,",
        "abstract": "Large language models (LLMs) have recently shown impressive performance on\ntasks involving reasoning, leading to a lively debate on whether these models\npossess reasoning capabilities similar to humans. However, despite these\nsuccesses, the depth of LLMs' reasoning abilities remains uncertain. This\nuncertainty partly stems from the predominant focus on task performance,\nmeasured through shallow accuracy metrics, rather than a thorough investigation\nof the models' reasoning behavior. This paper seeks to address this gap by\nproviding a comprehensive review of studies that go beyond task accuracy,\noffering deeper insights into the models' reasoning processes. Furthermore, we\nsurvey prevalent methodologies to evaluate the reasoning behavior of LLMs,\nemphasizing current trends and efforts towards more nuanced reasoning analyses.\nOur review suggests that LLMs tend to rely on surface-level patterns and\ncorrelations in their training data, rather than on genuine reasoning\nabilities. Additionally, we identify the need for further research that\ndelineates the key differences between human and LLM-based reasoning. Through\nthis survey, we aim to shed light on the complex reasoning processes within\nLLMs.",
        "score": 3.1630043983459473
      },
      {
        "title": "Specializing Smaller Language Models towards Multi-Step Reasoning,",
        "abstract": "The surprising ability of Large Language Models (LLMs) to perform well on\ncomplex reasoning with only few-shot chain-of-thought prompts is believed to\nemerge only in very large-scale models (100+ billion parameters). We show that\nsuch abilities can, in fact, be distilled down from GPT-3.5 ($\\ge$ 175B) to T5\nvariants ($\\le$ 11B). We propose model specialization, to specialize the\nmodel's ability towards a target task. The hypothesis is that large models\n(commonly viewed as larger than 100B) have strong modeling power, but are\nspread on a large spectrum of tasks. Small models (commonly viewed as smaller\nthan 10B) have limited model capacity, but if we concentrate their capacity on\na specific target task, the model can achieve a decent improved performance. We\nuse multi-step math reasoning as our testbed because it is a very typical\nemergent ability. We show two important aspects of model abilities: (1). there\nexists a very complex balance/ tradeoff between language models'\nmulti-dimensional abilities; (2). by paying the price of decreased generic\nability, we can clearly lift up the scaling curve of models smaller than 10B\ntowards a specialized multi-step math reasoning ability. We further give\ncomprehensive discussions about important design choices for better\ngeneralization, including the tuning data format, the start model checkpoint,\nand a new model selection method. We hope our practice and discoveries can\nserve as an important attempt towards specialized smaller models in the new\nresearch paradigm set by LLMs.",
        "score": 3.09470272064209
      },
      {
        "title": "MR-BEN: A Comprehensive Meta-Reasoning Benchmark for Large Language\n  Models,",
        "abstract": "Large language models (LLMs) have shown increasing capability in\nproblem-solving and decision-making, largely based on the step-by-step\nchain-of-thought reasoning processes. However, it has been increasingly\nchallenging to evaluate the reasoning capability of LLMs. Concretely, existing\noutcome-based benchmarks begin to saturate and become less sufficient to\nmonitor the progress. To this end, we present a process-based benchmark MR-BEN\nthat demands a meta reasoning skill, where LMs are asked to locate and analyse\npotential errors in automatically generated reasoning steps. MR-BEN is a\ncomprehensive benchmark comprising 5,975 questions collected from human\nexperts, covering various subjects such as physics, chemistry, logic, coding,\nand more. Through our designed metrics for assessing meta-reasoning on this\nbenchmark, we identify interesting limitations and weaknesses of current LLMs\n(open-source and closed-source models). For example, open-source models are\nseemingly comparable to GPT-4 on outcome-based benchmarks, but they lag far\nbehind on our benchmark, revealing the underlying reasoning capability gap\nbetween them. Our dataset and codes are available on\nhttps://randolph-zeng.github.io/Mr-Ben.github.io/.",
        "score": 2.8442351818084717
      },
      {
        "title": "$\\texttt{ACCORD}$: Closing the Commonsense Measurability Gap,",
        "abstract": "We present $\\texttt{ACCORD}$, a framework and benchmark suite for\ndisentangling the commonsense grounding and reasoning abilities of large\nlanguage models (LLMs) through controlled, multi-hop counterfactuals.\n$\\texttt{ACCORD}$ introduces formal elements to commonsense reasoning to\nexplicitly control and quantify reasoning complexity beyond the typical 1 or 2\nhops. Uniquely, $\\texttt{ACCORD}$ can automatically generate benchmarks of\narbitrary reasoning complexity, and so it scales with future LLM improvements.\nBenchmarking state-of-the-art LLMs -- including GPT-4o (2024-05-13),\nLlama-3-70B-Instruct, and Mixtral-8x22B-Instruct-v0.1 -- shows performance\ndegrading to random chance with only moderate scaling, leaving substantial\nheadroom for improvement. We release a leaderboard of the benchmark suite\ntested in this work, as well as code for automatically generating more complex\nbenchmarks.",
        "score": 2.8251984119415283
      },
      {
        "title": "Complexity-Based Prompting for Multi-Step Reasoning,",
        "abstract": "We study the task of prompting large-scale language models to perform\nmulti-step reasoning. Existing work shows that when prompted with a chain of\nthoughts (CoT), sequences of short sentences describing intermediate reasoning\nsteps towards a final answer, large language models can generate new reasoning\nchains and predict answers for new inputs. A central question is which\nreasoning examples make the most effective prompts. In this work, we propose\ncomplexity-based prompting, a simple and effective example selection scheme for\nmulti-step reasoning. We show that prompts with higher reasoning complexity,\ni.e., chains with more reasoning steps, achieve substantially better\nperformance on multi-step reasoning tasks over strong baselines. We further\nextend our complexity-based criteria from prompting (selecting inputs) to\ndecoding (selecting outputs), where we sample multiple reasoning chains from\nthe model, then choose the majority of generated answers from complex reasoning\nchains (over simple chains). When used to prompt GPT-3 and Codex, our approach\nsubstantially improves multi-step reasoning accuracy and achieves new\nstate-of-the-art (SOTA) performance on three math benchmarks (GSM8K,\nMultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and\nPenguins), with an average +5.3 and up to +18 accuracy improvements. Compared\nwith existing example selection schemes like manual tuning or retrieval-based\nselection, selection based on reasoning complexity is intuitive, easy to\nimplement, and annotation-efficient. Further results demonstrate the robustness\nof performance gains from complex prompts under format perturbation and\ndistribution shift.",
        "score": 2.721224784851074
      },
      {
        "title": "Can LLMs perform structured graph reasoning?,",
        "abstract": "Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).",
        "score": 2.3819940090179443
      },
      {
        "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the\n  Abstraction and Reasoning Corpus,",
        "abstract": "The existing methods for evaluating the inference abilities of Large Language\nModels (LLMs) have been results-centric, making it difficult to assess the\ninference process. We introduce a new approach using the Abstract and Reasoning\nCorpus (ARC) dataset to evaluate the inference and contextual understanding\nabilities of large language models in a process-centric manner. ARC demands\nrigorous logical structures for problem-solving, making it a benchmark that\nfacilitates the comparison of model inference abilities with humans.\nExperimental results confirm that while large language models possess weak\ninference abilities, they still lag in terms of logical coherence,\ncompositionality, and productivity. Our experiments highlight the reasoning\ncapabilities of LLMs, proposing development paths for achieving human-level\nreasoning.",
        "score": 2.291583776473999
      },
      {
        "title": "Towards Reasoning in Large Language Models: A Survey,",
        "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.",
        "score": 2.278787612915039
      },
      {
        "title": "Can LLMs Reason in the Wild with Programs?,",
        "abstract": "Large Language Models (LLMs) have shown superior capability to solve\nreasoning problems with programs. While being a promising direction, most of\nsuch frameworks are trained and evaluated in settings with a prior knowledge of\ntask requirements. However, as LLMs become more capable, it is necessary to\nassess their reasoning abilities in more realistic scenarios where many\nreal-world problems are open-ended with ambiguous scope, and often require\nmultiple formalisms to solve. To investigate this, we introduce the task of\nreasoning in the wild, where an LLM is tasked to solve a reasoning problem of\nunknown type by identifying the subproblems and their corresponding formalisms,\nand writing a program to solve each subproblem, guided by a tactic. We create a\nlarge tactic-guided trajectory dataset containing detailed solutions to a\ndiverse set of reasoning problems, ranging from well-defined single-form\nreasoning (e.g., math, logic), to ambiguous and hybrid ones (e.g., commonsense,\ncombined math and logic). This allows us to test various aspects of LLMs\nreasoning at the fine-grained level such as the selection and execution of\ntactics, and the tendency to take undesired shortcuts. In experiments, we\nhighlight that existing LLMs fail significantly on problems with ambiguous and\nmixed scope, revealing critical limitations and overfitting issues (e.g.\naccuracy on GSM8K drops by at least 50\\%). We further show the potential of\nfinetuning a local LLM on the tactic-guided trajectories in achieving better\nperformance. Project repo is available at\ngithub.com/gblackout/Reason-in-the-Wild",
        "score": 2.072641372680664
      },
      {
        "title": "Advancing LLM Reasoning Generalists with Preference Trees,",
        "abstract": "We introduce Eurus, a suite of large language models (LLMs) optimized for\nreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve\nstate-of-the-art results among open-source models on a diverse set of\nbenchmarks covering mathematics, code generation, and logical reasoning\nproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a\ncomprehensive benchmarking across 12 tests covering five tasks, and achieves a\n33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging\nbenchmarks, substantially outperforming existing open-source models by margins\nmore than 13.3%. The strong performance of Eurus can be primarily attributed to\nUltraInteract, our newly-curated large-scale, high-quality alignment dataset\nspecifically designed for complex reasoning tasks. UltraInteract can be used in\nboth supervised fine-tuning and preference learning. For each instruction, it\nincludes a preference tree consisting of (1) reasoning chains with diverse\nplanning strategies in a unified format, (2) multi-turn interaction\ntrajectories with the environment and the critique, and (3) pairwise data to\nfacilitate preference learning. UltraInteract allows us to conduct an in-depth\nexploration of preference learning for reasoning tasks. Our investigation\nreveals that some well-established preference learning algorithms may be less\nsuitable for reasoning tasks compared to their effectiveness in general\nconversations. Inspired by this, we derive a novel reward modeling objective\nwhich, together with UltraInteract, leads to a strong reward model.",
        "score": 2.0584681034088135
      },
      {
        "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models\n  improve Complex Reasoning,",
        "abstract": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT)\nexhibit impressive reasoning capabilities. Recent attempts at prompt\ndecomposition toward solving complex, multi-step reasoning problems depend on\nthe ability of the LLM to simultaneously decompose and solve the problem. A\nsignificant disadvantage is that foundational LLMs are typically not available\nfor fine-tuning, making adaptation computationally prohibitive. We believe (and\ndemonstrate) that problem decomposition and solution generation are distinct\ncapabilites, better addressed in separate modules, than by one monolithic LLM.\nWe introduce DaSLaM, which uses a decomposition generator to decompose complex\nproblems into subproblems that require fewer reasoning steps. These subproblems\nare answered by a solver. We use a relatively small (13B parameters) LM as the\ndecomposition generator, which we train using policy gradient optimization to\ninteract with a solver LM (regarded as black-box) and guide it through\nsubproblems, thereby rendering our method solver-agnostic. Evaluation on\nmultiple different reasoning datasets reveal that with our method, a 175\nbillion parameter LM (text-davinci-003) can produce competitive or even better\nperformance, compared to its orders-of-magnitude larger successor, GPT-4.\nAdditionally, we show that DaSLaM is not limited by the solver's capabilities\nas a function of scale; e.g., solver LMs with diverse sizes give significant\nperformance improvement with our solver-agnostic decomposition technique.\nExhaustive ablation studies evince the superiority of our modular finetuning\ntechnique over exorbitantly large decomposer LLMs, based on prompting alone.",
        "score": 2.032097578048706
      },
      {
        "title": "A Notion of Complexity for Theory of Mind via Discrete World Models,",
        "abstract": "Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework to measure the complexity of ToM tasks. We quantify a problem's\ncomplexity as the number of states necessary to solve it correctly. Our\ncomplexity measure also accounts for spurious states of a ToM problem designed\nto make it apparently harder. We use our method to assess the complexity of\nfive widely adopted ToM benchmarks. On top of this framework, we design a\nprompting technique that augments the information available to a model with a\ndescription of how the environment changes with the agents' interactions. We\nname this technique Discrete World Models (DWM) and show how it elicits\nsuperior performance on ToM tasks.",
        "score": 1.6419689655303955
      },
      {
        "title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial\n  Reasoning Problems?,",
        "abstract": "Recent works show that the largest of the large language models (LLMs) can\nsolve many simple reasoning tasks expressed in natural language, without\nany/much supervision. But, can they also solve challenging first-order\ncombinatorial reasoning problems, such as graph coloring, knapsack and\ncryptarithmetic? To answer this question, we present PuzzleBench, a dataset of\n31 such challenging problems along with a few solved instances for each\nproblem. These problems are all first order, i.e., they can be instantiated\nwith problem instances of varying sizes, and most of them are NP-hard,\nrequiring several reasoning steps to reach the solution. We first observe that\nLLMs, even when aided by symbolic solvers, perform rather poorly on our\ndataset. In response, we propose a new approach, Puzzle-LM, which combines LLMs\nwith both symbolic solvers and program interpreters, along with feedback from\nsolved examples, to achieve huge performance gains. Our extensive\nexperimentation and analyses offer new insights into the reasoning abilities\nand limitations of present-day LLMs.",
        "score": 1.592512607574463
      },
      {
        "title": "An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning,",
        "abstract": "Large language models (LLMs) are displaying emergent abilities for math\nreasoning tasks,and there is a growing attention on enhancing the ability of\nopen-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to\nexplore a general data strategy for supervised data to help optimize and expand\nmath reasoning ability.Firstly, we determine the ability boundary of reasoning\npaths augmentation by identifying these paths' minimal optimal set.Secondly, we\nvalidate that different abilities of the model can be cumulatively enhanced by\nMix of Minimal Optimal Sets of corresponding types of data, while our models\nMMOS achieve SOTA performance on series base models under much lower\nconstruction costs.Besides, we point out GSM-HARD is not really hard and\ntoday's LLMs no longer lack numerical robustness.Also, we provide an Auto\nProblem Generator for robustness testing and educational applications.Our code\nand data are publicly available at https://github.com/cyzhh/MMOS.",
        "score": 1.5576211214065552
      },
      {
        "title": "ComplexityNet: Increasing LLM Inference Efficiency by Learning Task\n  Complexity,",
        "abstract": "We present ComplexityNet, a streamlined language model designed for assessing\ntask complexity. This model predicts the likelihood of accurate output by\nvarious language models, each with different capabilities. Our initial\napplication of ComplexityNet involves the Mostly Basic Python Problems (MBPP)\ndataset. We pioneered the creation of the first set of labels to define task\ncomplexity. ComplexityNet achieved a notable 79% accuracy in determining task\ncomplexity, a significant improvement over the 34% accuracy of the original,\nnon fine-tuned model. Furthermore, ComplexityNet effectively reduces\ncomputational resource usage by 90% compared to using the highest complexity\nmodel, while maintaining a high code generation accuracy of 86.7%. This study\ndemonstrates that fine-tuning smaller models to categorize tasks based on their\ncomplexity can lead to a more balanced trade-off between accuracy and\nefficiency in the use of Large Language Models. Our findings suggest a\npromising direction for optimizing LLM applications, especially in\nresource-constrained environments.",
        "score": 1.4713246822357178
      },
      {
        "title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance,\n  and the Reasoning Gap,",
        "abstract": "We propose a framework for robust evaluation of reasoning capabilities of\nlanguage models, using functional variants of benchmarks. Models that solve a\nreasoning test should exhibit no difference in performance over the static\nversion of a problem compared to a snapshot of the functional variant. We have\nrewritten the relevant fragment of the MATH benchmark into its functional\nvariant MATH(), with functionalization of other benchmarks to follow. When\nevaluating current state-of-the-art models over snapshots of MATH(), we find a\nreasoning gap -- the percentage difference between the static and functional\naccuracies. We find reasoning gaps from 58.35% to 80.31% among the\nstate-of-the-art closed and open weights models that perform well on static\nbenchmarks, with the caveat that the gaps are likely to be smaller with more\nsophisticated prompting strategies. Here we show that models which anecdotally\nhave good reasoning performance over real-world tasks, have quantifiable lower\ngaps, motivating the open problem of building \"gap 0\" models. Code for\nevaluation and new evaluation datasets, three MATH() snapshots, are publicly\navailable at https://github.com/consequentai/fneval/.",
        "score": 1.3726253509521484
      },
      {
        "title": "The CLRS-Text Algorithmic Reasoning Language Benchmark,",
        "abstract": "Eliciting reasoning capabilities from language models (LMs) is a critical\ndirection on the path towards building intelligent systems. Most recent studies\ndedicated to reasoning focus on out-of-distribution performance on\nprocedurally-generated synthetic benchmarks, bespoke-built to evaluate specific\nskills only. This trend makes results hard to transfer across publications,\nslowing down progress. Three years ago, a similar issue was identified and\nrectified in the field of neural algorithmic reasoning, with the advent of the\nCLRS benchmark. CLRS is a dataset generator comprising graph execution traces\nof classical algorithms from the Introduction to Algorithms textbook. Inspired\nby this, we propose CLRS-Text -- a textual version of these algorithmic traces.\nOut of the box, CLRS-Text is capable of procedurally generating trace data for\nthirty diverse, challenging algorithmic tasks across any desirable input\ndistribution, while offering a standard pipeline in which any additional\nalgorithmic tasks may be created in the benchmark. We fine-tune and evaluate\nvarious LMs as generalist executors on this benchmark, validating prior work\nand revealing a novel, interesting challenge for the LM reasoning community.\nOur code is available at\nhttps://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text.",
        "score": 1.1841386556625366
      },
      {
        "title": "NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear\n  Domain,",
        "abstract": "As LLMs have become increasingly popular, they have been used in almost every\nfield. But as the application for LLMs expands from generic fields to narrow,\nfocused science domains, there exists an ever-increasing gap in ways to\nevaluate their efficacy in those fields. For the benchmarks that do exist, a\nlot of them focus on questions that don't require proper understanding of the\nsubject in question. In this paper, we present NuclearQA, a human-made\nbenchmark of 100 questions to evaluate language models in the nuclear domain,\nconsisting of a varying collection of questions that have been specifically\ndesigned by experts to test the abilities of language models. We detail our\napproach and show how the mix of several types of questions makes our benchmark\nuniquely capable of evaluating models in the nuclear domain. We also present\nour own evaluation metric for assessing LLM's performances due to the\nlimitations of existing ones. Our experiments on state-of-the-art models\nsuggest that even the best LLMs perform less than satisfactorily on our\nbenchmark, demonstrating the scientific knowledge gap of existing LLMs.",
        "score": 0.9134406447410583
      },
      {
        "title": "A Principled Framework for Knowledge-enhanced Large Language Model,",
        "abstract": "Large Language Models (LLMs) are versatile, yet they often falter in tasks\nrequiring deep and reliable reasoning due to issues like hallucinations,\nlimiting their applicability in critical scenarios. This paper introduces a\nrigorously designed framework for creating LLMs that effectively anchor\nknowledge and employ a closed-loop reasoning process, enhancing their\ncapability for in-depth analysis. We dissect the framework to illustrate the\ncontribution of each component to the LLMs' performance, offering a theoretical\nassurance of improved reasoning under well-defined assumptions.",
        "score": 0.21095655858516693
      }
    ]
  },
  {
    "title": "Aligning Large Language Models with Human Preferences through\n  Representation Engineering",
    "abstract": "  Aligning large language models (LLMs) with human preferences is crucial for\nenhancing their utility in terms of helpfulness, truthfulness, safety,\nharmlessness, and interestingness. Existing methods for achieving this\nalignment often involves employing reinforcement learning from human feedback\n(RLHF) to fine-tune LLMs based on human labels assessing the relative quality\nof model responses. Nevertheless, RLHF is susceptible to instability during\nfine-tuning and presents challenges in implementation.Drawing inspiration from\nthe emerging field of representation engineering (RepE), this study aims to\nidentify relevant representations for high-level human preferences embedded in\npatterns of activity within an LLM, and achieve precise control of model\nbehavior by transforming its representations. This novel approach, denoted as\nRepresentation Alignment from Human Feedback (RAHF), proves to be effective,\ncomputationally efficient, and easy to implement.Extensive experiments\ndemonstrate the efficacy of RAHF in not only capturing but also manipulating\nrepresentations to align with a broad spectrum of human preferences or values,\nrather than being confined to a singular concept or function (e.g. honesty or\nbias). RAHF's versatility in accommodating diverse human preferences shows its\npotential for advancing LLM performance.\n",
    "related_paper_titles": [
      "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model",
      "Chain of Hindsight Aligns Language Models with Feedback",
      "Representation Engineering: A Top-Down Approach to AI Transparency"
    ],
    "related_paper_abstract": [
      "  While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.\n",
      "  Learning from human preferences is important for language models to match\nhuman needs and to align with human and social values. Prior works have\nachieved remarkable successes by learning from human feedback to understand and\nfollow instructions. Nonetheless, these methods are either founded on\nhand-picked model generations that are favored by human annotators, rendering\nthem inefficient in terms of data utilization and challenging to apply in\ngeneral, or they depend on reinforcement learning, which often suffers from\nimperfect reward functions and relies on extremely challenging optimizations.\nIn this work, we propose a novel technique, Chain of Hindsight, that is easy to\noptimize and can learn from any form of feedback, regardless of its polarity.\nOur idea is inspired by how humans learn from extensive feedback presented in\nthe form of languages. We convert all types of feedback into sequences of\nsentences, which are then used to fine-tune the model, allowing us to take\nadvantage of the language comprehension capabilities of language models. We\ncondition the model on a sequence of model generations paired with feedback. By\ndoing so, the model is trained to generate outputs based on feedback, while\nlearning to identify and correct negative attributes or errors. Applying our\nmethod to large language models, we observed that Chain of Hindsight\nsignificantly surpasses previous methods in aligning language models with human\npreferences. We report significant improvements on summarization and dialogue\nbenchmarks, with our approach markedly preferred in human evaluations.\n",
      "  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n"
    ],
    "entities": [
      "Large Language Models Large Language Models",
      "Persian",
      "Large Language Model",
      "Large Language",
      "DNN",
      "ChatGPT",
      "ICL",
      "Shapley",
      "CAD",
      "SSMs",
      "ToM",
      "Gemini",
      "NLG",
      "APIs",
      "RLHF",
      "Human Feedback",
      "KG",
      "KGC",
      "DTs",
      "Contrastive",
      "API",
      "MATH",
      "KGs",
      "Mind",
      "Natural Language",
      "GSM8K",
      "ESG",
      "LoRA",
      "Python",
      "NER"
    ],
    "retrieved_papers": [
      {
        "title": "Aligning Large Language Models with Human Preferences through\n  Representation Engineering,",
        "abstract": "Aligning large language models (LLMs) with human preferences is crucial for\nenhancing their utility in terms of helpfulness, truthfulness, safety,\nharmlessness, and interestingness. Existing methods for achieving this\nalignment often involves employing reinforcement learning from human feedback\n(RLHF) to fine-tune LLMs based on human labels assessing the relative quality\nof model responses. Nevertheless, RLHF is susceptible to instability during\nfine-tuning and presents challenges in implementation.Drawing inspiration from\nthe emerging field of representation engineering (RepE), this study aims to\nidentify relevant representations for high-level human preferences embedded in\npatterns of activity within an LLM, and achieve precise control of model\nbehavior by transforming its representations. This novel approach, denoted as\nRepresentation Alignment from Human Feedback (RAHF), proves to be effective,\ncomputationally efficient, and easy to implement.Extensive experiments\ndemonstrate the efficacy of RAHF in not only capturing but also manipulating\nrepresentations to align with a broad spectrum of human preferences or values,\nrather than being confined to a singular concept or function (e.g. honesty or\nbias). RAHF's versatility in accommodating diverse human preferences shows its\npotential for advancing LLM performance.",
        "score": 6.4385199546813965
      },
      {
        "title": "From Instructions to Intrinsic Human Values -- A Survey of Alignment\n  Goals for Big Models,",
        "abstract": "Big models, exemplified by Large Language Models (LLMs), are models typically\npre-trained on massive data and comprised of enormous parameters, which not\nonly obtain significantly improved performance across diverse tasks but also\npresent emergent capabilities absent in smaller models. However, the growing\nintertwining of big models with everyday human lives poses potential risks and\nmight cause serious social harm. Therefore, many efforts have been made to\nalign LLMs with humans to make them better follow user instructions and satisfy\nhuman preferences. Nevertheless, `what to align with' has not been fully\ndiscussed, and inappropriate alignment goals might even backfire. In this\npaper, we conduct a comprehensive survey of different alignment goals in\nexisting work and trace their evolution paths to help identify the most\nessential goal. Particularly, we investigate related works from two\nperspectives: the definition of alignment goals and alignment evaluation. Our\nanalysis encompasses three distinct levels of alignment goals and reveals a\ngoal transformation from fundamental abilities to value orientation, indicating\nthe potential of intrinsic human values as the alignment goal for enhanced\nLLMs. Based on such results, we further discuss the challenges of achieving\nsuch intrinsic value alignment and provide a collection of available resources\nfor future research on the alignment of big models.",
        "score": 4.64675235748291
      },
      {
        "title": "Personalisation within bounds: A risk taxonomy and policy framework for\n  the alignment of large language models with personalised feedback,",
        "abstract": "Large language models (LLMs) are used to generate content for a wide range of\ntasks, and are set to reach a growing audience in coming years due to\nintegration in product interfaces like ChatGPT or search engines like Bing.\nThis intensifies the need to ensure that models are aligned with human\npreferences and do not produce unsafe, inaccurate or toxic outputs. While\nalignment techniques like reinforcement learning with human feedback (RLHF) and\nred-teaming can mitigate some safety concerns and improve model capabilities,\nit is unlikely that an aggregate fine-tuning process can adequately represent\nthe full range of users' preferences and values. Different people may\nlegitimately disagree on their preferences for language and conversational\nnorms, as well as on values or ideologies which guide their communication.\nPersonalising LLMs through micro-level preference learning processes may result\nin models that are better aligned with each user. However, there are several\nnormative challenges in defining the bounds of a societally-acceptable and safe\ndegree of personalisation. In this paper, we ask how, and in what ways, LLMs\nshould be personalised. First, we review literature on current paradigms for\naligning LLMs with human feedback, and identify issues including (i) a lack of\nclarity regarding what alignment means; (ii) a tendency of technology providers\nto prescribe definitions of inherently subjective preferences and values; and\n(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in\nwho we are really aligning to. Second, we present a taxonomy of benefits and\nrisks associated with personalised LLMs, for individuals and society at large.\nFinally, we propose a three-tiered policy framework that allows users to\nexperience the benefits of personalised alignment, while restraining unsafe and\nundesirable LLM-behaviours within (supra-)national and organisational bounds.",
        "score": 4.220926284790039
      },
      {
        "title": "More RLHF, More Trust? On The Impact of Human Preference Alignment On\n  Language Model Trustworthiness,",
        "abstract": "The surge in Large Language Models (LLMs) development has led to improved\nperformance on cognitive tasks as well as an urgent need to align these models\nwith human values in order to safely exploit their power. Despite the\neffectiveness of preference learning algorithms like Reinforcement Learning\nFrom Human Feedback (RLHF) in aligning human preferences, their assumed\nimprovements on model trustworthiness haven't been thoroughly testified. Toward\nthis end, this study investigates how models that have been aligned with\ngeneral-purpose preference data on helpfulness and harmlessness perform across\nfive trustworthiness verticals: toxicity, stereotypical bias, machine ethics,\ntruthfulness, and privacy. For model alignment, we focus on three widely used\nRLHF variants: Supervised Finetuning (SFT), Proximal Policy Optimization (PPO),\nand Direct Preference Optimization (DPO). Through extensive empirical\ninvestigations, we discover that the improvement in trustworthiness by RLHF is\nfar from guaranteed, and there exists a complex interplay between preference\ndata, alignment algorithms, and specific trustworthiness aspects. Together, our\nresults underscore the need for more nuanced approaches for model alignment. By\nshedding light on the intricate dynamics of these components within model\nalignment, we hope this research will guide the community towards developing\nlanguage models that are both capable and trustworthy.",
        "score": 4.157238483428955
      },
      {
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback\n  without tears,",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment\nof large language models with human preferences, significantly enhancing the\nquality of interactions between humans and models. InstructGPT implements RLHF\nthrough several stages, including Supervised Fine-Tuning (SFT), reward model\ntraining, and Proximal Policy Optimization (PPO). However, PPO is sensitive to\nhyperparameters and requires multiple models in its standard implementation,\nmaking it hard to train and scale up to larger parameter counts. In contrast,\nwe propose a novel learning paradigm called RRHF, which scores sampled\nresponses from different sources via a logarithm of conditional probabilities\nand learns to align these probabilities with human preferences through ranking\nloss. RRHF can leverage sampled responses from various sources including the\nmodel responses from itself, other large language model responses, and human\nexpert responses to learn to rank them. RRHF only needs 1 to 2 models during\ntuning and can efficiently align language models with human preferences\nrobustly without complex hyperparameter tuning. Additionally, RRHF can be\nconsidered an extension of SFT and reward model training while being simpler\nthan PPO in terms of coding, model counts, and hyperparameters. We evaluate\nRRHF on the Helpful and Harmless dataset, demonstrating comparable alignment\nperformance with PPO by reward model score and human labeling. Extensive\nexperiments show that the performance of RRHF is highly related to sampling\nquality which suggests RRHF is a best-of-n learner. Codes available at\nhttps://github.com/GanjinZero/RRHF.",
        "score": 4.074118614196777
      },
      {
        "title": "OpenAssistant Conversations -- Democratizing Large Language Model\n  Alignment,",
        "abstract": "Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.",
        "score": 3.927316904067993
      },
      {
        "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization\n  Method for Alignment of Large Language Models,",
        "abstract": "Reinforcement learning from human feedback (RLHF) has been extensively\nemployed to align large language models with user intent. However, proximal\npolicy optimization (PPO) based RLHF is occasionally unstable requiring\nsignificant hyperparameter finetuning, and computationally expensive to\nmaximize the estimated reward during alignment. Recently, direct preference\noptimization (DPO) is proposed to address those challenges. However, DPO relies\non contrastive responses generated from human annotator and alternative LLM,\ninstead of the policy model, limiting the effectiveness of the RLHF. In this\npaper, we addresses both challenges by systematically combining rejection\nsampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the\ndevelopment of a supervised fine-tuned policy model (SFT). A varied set of k\nresponses per prompt are sampled directly from the SFT model. RS-DPO identifies\npairs of contrastive samples based on their reward distribution. Finally, we\napply DPO with the contrastive samples to align the model to human preference.\nOur experiments indicate that our proposed method effectively fine-tunes LLMs\nwith limited resource environments, leading to improved alignment with user\nintent. Furthermore, it outperforms existing methods, including RS, PPO, and\nDPO.",
        "score": 3.8688502311706543
      },
      {
        "title": "Data-Efficient Alignment of Large Language Models with Human Feedback\n  Through Natural Language,",
        "abstract": "Learning from human feedback is a prominent technique to align the output of\nlarge language models (LLMs) with human expectations. Reinforcement learning\nfrom human feedback (RLHF) leverages human preference signals that are in the\nform of ranking of response pairs to perform this alignment. However, human\npreference on LLM outputs can come in much richer forms including natural\nlanguage, which may provide detailed feedback on strengths and weaknesses of a\ngiven response. In this work we investigate data efficiency of modeling human\nfeedback that is in natural language. Specifically, we fine-tune an open-source\nLLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or\neven less) of human feedback in natural language in the form of critiques and\nrevisions of responses. We show that this model is able to improve the quality\nof responses from even some of the strongest LLMs such as ChatGPT, BARD, and\nVicuna, through critique and revision of those responses. For instance, through\none iteration of revision of ChatGPT responses, the revised responses have\n56.6% win rate over the original ones, and this win rate can be further\nimproved to 65.9% after applying the revision for five iterations.",
        "score": 3.8515732288360596
      },
      {
        "title": "Safer-Instruct: Aligning Language Models with Automated Preference Data,",
        "abstract": "Reinforcement learning from human feedback (RLHF) is a vital strategy for\nenhancing model capability in language models. However, annotating preference\ndata for RLHF is a resource-intensive and creativity-demanding process, while\nexisting automatic generation methods face limitations in data diversity and\nquality. In response, we present Safer-Instruct, a novel pipeline for\nautomatically constructing large-scale preference data. Our approach leverages\nreversed instruction tuning, instruction induction, and expert model evaluation\nto efficiently generate high-quality preference data without human annotators.\nTo verify the effectiveness of Safer-Instruct, we apply the pipeline to\nconstruct a safety preference dataset as a case study. Finetuning an Alpaca\nmodel on this synthetic dataset not only demonstrates improved harmlessness but\nalso outperforms models fine-tuned on human-annotated safety preference data,\nall the while maintaining a competitive edge in downstream tasks. Importantly,\nour Safer-Instruct framework is versatile and can be applied to generate\npreference data across various domains, extending its utility beyond safety\npreferences. It addresses the challenges in preference data acquisition and\nadvances the development of more capable and responsible AI systems. For\ndataset and code implementation, see\nhttps://github.com/uscnlp-lime/safer-instruct",
        "score": 3.847573757171631
      },
      {
        "title": "ULMA: Unified Language Model Alignment with Human Demonstration and\n  Point-wise Preference,",
        "abstract": "Aligning language models to human expectations, e.g., being helpful and\nharmless, has become a pressing challenge for large language models. A typical\nalignment procedure consists of supervised fine-tuning and preference learning.\nMost preference learning methods, such as RLHF and DPO, depend on pairwise\npreference data, which inadequately address scenarios where human feedback is\npoint-wise, leading to potential information loss and suboptimal performance.\nAddressing this gap, we introduce Point-wise Direct Preference Optimization, a\nnovel preference learning method designed to harness point-wise feedback\neffectively. Our work also uncovers a novel connection between supervised\nfine-tuning and point-wise preference learning, culminating in Unified Language\nModel Alignment, a single-step method that unifies the alignment with human\ndemonstrations and point-wise preferences. Extensive experiments on point-wise\npreference datasets with binary or continuous labels validate the effectiveness\nof our methods. Our code and a new dataset with high-quality demonstration\nsamples on harmlessness are released.",
        "score": 3.833507537841797
      },
      {
        "title": "CycleAlign: Iterative Distillation from Black-box LLM to White-box\n  Models for Better Human Alignment,",
        "abstract": "Language models trained on large-scale corpus often generate content that is\nharmful, toxic, or contrary to human preferences, making their alignment with\nhuman values a critical concern. Reinforcement learning from human feedback\n(RLHF) with algorithms like PPO is a prevalent approach for alignment but is\noften complex, unstable, and resource-intensive. Recently, ranking-based\nalignment methods have emerged, offering stability and effectiveness by\nreplacing the RL framework with supervised fine-tuning, but they are costly due\nto the need for annotated data. Considering that existing large language models\n(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,\nresearchers have begun to align the language model with human preference from\nAI feedback. The common practices, which unidirectionally distill the\ninstruction-following responses from LLMs, are constrained by their bottleneck.\nThus we introduce CycleAlign to distill alignment capabilities from\nparameter-invisible LLMs (black-box) to a parameter-visible model (white-box)\nin an iterative manner. With in-context learning (ICL) as the core of the\ncycle, the black-box models are able to rank the model-generated responses\nguided by human-craft instruction and demonstrations about their preferences.\nDuring iterative interaction, the white-box models also have a judgment about\nresponses generated by them. Consequently, the agreement ranking could be\nviewed as a pseudo label to dynamically update the in-context demonstrations\nand improve the preference ranking ability of black-box models. Through\nmultiple interactions, the CycleAlign framework could align the white-box model\nwith the black-box model effectively in a low-resource way. Empirical results\nillustrate that the model fine-tuned by CycleAlign remarkably exceeds existing\nmethods, and achieves the state-of-the-art performance in alignment with human\nvalue.",
        "score": 3.7555458545684814
      },
      {
        "title": "Aligning Large Language Models via Fine-grained Supervision,",
        "abstract": "Pre-trained large-scale language models (LLMs) excel at producing coherent\narticles, yet their outputs may be untruthful, toxic, or fail to align with\nuser expectations. Current approaches focus on using reinforcement learning\nwith human feedback (RLHF) to improve model alignment, which works by\ntransforming coarse human preferences of LLM outputs into a feedback signal\nthat guides the model learning process. However, because this approach operates\non sequence-level feedback, it lacks the precision to identify the exact parts\nof the output affecting user preferences. To address this gap, we propose a\nmethod to enhance LLM alignment through fine-grained token-level supervision.\nSpecifically, we ask annotators to minimally edit less preferred responses\nwithin the standard reward modeling dataset to make them more favorable,\nensuring changes are made only where necessary while retaining most of the\noriginal content. The refined dataset is used to train a token-level reward\nmodel, which is then used for training our fine-grained Proximal Policy\nOptimization (PPO) model. Our experiment results demonstrate that this approach\ncan achieve up to an absolute improvement of $5.1\\%$ in LLM performance, in\nterms of win rate against the reference model, compared with the traditional\nPPO model.",
        "score": 3.7495200634002686
      },
      {
        "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human\n  Feedback,",
        "abstract": "ChatGLM is a free-to-use AI service powered by the ChatGLM family of large\nlanguage models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline --\na reinforcement learning from human feedback (RLHF) system -- designed to\nenhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses\nthree major components: the collection of human preference data, the training\nof the reward model, and the optimization of policies. Throughout the process\nof integrating ChatGLM-RLHF into production, we encountered and addressed\nseveral unprecedented challenges. We introduce the strategies to mitigate\nreward variance for stabilized large-scale training, implement model\nparallelism with fused gradient-descent, and design regularization constraints\nto avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF\nbrings significant improvements in alignment tasks compared to the supervised\nfine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\\%\nmore wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our\npractices of aligning LLMs with human preferences, offering insights into the\nchallenges and solutions in RLHF implementations.",
        "score": 3.707890510559082
      },
      {
        "title": "Secrets of RLHF in Large Language Models Part I: PPO,",
        "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes, aiming to make modest\ncontributions to the advancement of LLMs.",
        "score": 3.6274287700653076
      },
      {
        "title": "Dissecting Human and LLM Preferences,",
        "abstract": "As a relative quality comparison of model responses, human and Large Language\nModel (LLM) preferences serve as common alignment goals in model fine-tuning\nand criteria in evaluation. Yet, these preferences merely reflect broad\ntendencies, resulting in less explainable and controllable models with\npotential safety risks. In this work, we dissect the preferences of human and\n32 different LLMs to understand their quantitative composition, using\nannotations from real-world user-model conversations for a fine-grained,\nscenario-wise analysis. We find that humans are less sensitive to errors, favor\nresponses that support their stances, and show clear dislike when models admit\ntheir limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize\ncorrectness, clarity, and harmlessness more. Additionally, LLMs of similar\nsizes tend to exhibit similar preferences, regardless of their training\nmethods, and fine-tuning for alignment does not significantly alter the\npreferences of pretrained-only LLMs. Finally, we show that preference-based\nevaluation can be intentionally manipulated. In both training-free and\ntraining-based settings, aligning a model with the preferences of judges boosts\nscores, while injecting the least preferred properties lowers them. This\nresults in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94\non AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this\nstrategic adaptation. Interactive Demo:\nhttps://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset:\nhttps://huggingface.co/datasets/GAIR/preference-dissection Code:\nhttps://github.com/GAIR-NLP/Preference-Dissection",
        "score": 3.5933849811553955
      },
      {
        "title": "SALMON: Self-Alignment with Instructable Reward Models,",
        "abstract": "Supervised Fine-Tuning (SFT) on response demonstrations combined with\nReinforcement Learning from Human Feedback (RLHF) constitutes a powerful\nparadigm for aligning LLM-based AI agents. However, a significant limitation of\nsuch an approach is its dependency on high-quality human annotations, making\nits application to intricate tasks challenging due to difficulties in obtaining\nconsistent response demonstrations and in-distribution response preferences.\nThis paper presents a novel approach, namely SALMON, to align base language\nmodels with minimal human supervision, using only a small set of human-defined\nprinciples, yet achieving superior performance. Central to our approach is an\ninstructable reward model. Trained on synthetic preference data, this model can\ngenerate reward scores based on arbitrary human-defined principles. By merely\nadjusting these principles during the RL training phase, we gain full control\nover the preferences with the instructable reward model, subsequently\ninfluencing the behavior of the RL-trained policy models, and reducing the\nreliance on the collection of online human preferences. Applying our method to\nthe LLaMA-2-70b base language model, we developed an AI assistant named\nDromedary-2. With only 6 exemplars for in-context learning and 31 human-defined\nprinciples, Dromedary-2 significantly surpasses the performance of several\nstate-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark\ndatasets. We have open-sourced the code and model weights to encourage further\nresearch into aligning LLM-based AI agents with enhanced supervision\nefficiency, improved controllability, and scalable oversight.",
        "score": 3.5498127937316895
      },
      {
        "title": "Aligning to Thousands of Preferences via System Message Generalization,",
        "abstract": "Although humans inherently have diverse values, current large language model\n(LLM) alignment methods often assume that aligning LLMs with the general\npublic's preferences is optimal. A major challenge in adopting a more\nindividualized approach to LLM alignment is its lack of scalability, as it\ninvolves repeatedly acquiring preference data and training new reward models\nand LLMs for each individual's preferences. To address these challenges, we\npropose a new paradigm where users specify what they value most within the\nsystem message, steering the LLM's generation behavior to better align with the\nuser's intentions. However, a naive application of such an approach is\nnon-trivial since LLMs are typically trained on a uniform system message (e.g.,\n\"You are a helpful assistant\") which limits their ability to generalize to\ndiverse, unseen system messages. To improve this generalization, we create the\nMultifaceted Collection, a preference dataset with 192k combinations of values\nbeyond generic helpfulness and harmlessness, spanning 65k user instructions.\nUsing this dataset, we train a 7B LLM called Janus and test it on 921 prompts\nfrom 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\nby adding various unseen system messages that reflect user preferences. Janus\nachieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct\nv0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks\nfocused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto\nv0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%\nmargin, underscoring that training with a vast array of system messages could\nalso enhance alignment to the general public's preference as well. Our code,\ndataset, benchmark, and models are available at\nhttps://github.com/kaistAI/Janus.",
        "score": 3.4983434677124023
      },
      {
        "title": "Tradeoffs Between Alignment and Helpfulness in Language Models with\n  Representation Engineering,",
        "abstract": "Language model alignment has become an important component of AI safety,\nallowing safe interactions between humans and language models, by enhancing\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\nmodel or inserting preset aligning prompts. Recently, representation\nengineering, a method which alters the model's behavior via changing its\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\net al., 2023a). Representation engineering yields gains in alignment oriented\ntasks such as resistance to adversarial attacks and reduction of social biases,\nbut was also shown to cause a decrease in the ability of the model to perform\nbasic tasks. In this paper we study the tradeoff between the increase in\nalignment and decrease in helpfulness of the model. We propose a theoretical\nframework which provides bounds for these two quantities, and demonstrate their\nrelevance empirically. First, we find that under the conditions of our\nframework, alignment can be guaranteed with representation engineering, and at\nthe same time that helpfulness is harmed in the process. Second, we show that\nhelpfulness is harmed quadratically with the norm of the representation\nengineering vector, while the alignment increases linearly with it, indicating\na regime in which it is efficient to use representation engineering. We\nvalidate our findings empirically, and chart the boundaries to the usefulness\nof representation engineering for alignment.",
        "score": 3.3824024200439453
      },
      {
        "title": "Verbosity Bias in Preference Labeling by Large Language Models,",
        "abstract": "In recent years, Large Language Models (LLMs) have witnessed a remarkable\nsurge in prevalence, altering the landscape of natural language processing and\nmachine learning. One key factor in improving the performance of LLMs is\nalignment with humans achieved with Reinforcement Learning from Human Feedback\n(RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies\nare investigating the replacement of human feedback with feedback from other\nLLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the\nbiases that come along with evaluating LLMs with other LLMs and take a closer\nlook into verbosity bias -- a bias where LLMs sometimes prefer more verbose\nanswers even if they have similar qualities. We see that in our problem\nsetting, GPT-4 prefers longer answers more than humans. We also propose a\nmetric to measure this bias.",
        "score": 3.273348331451416
      },
      {
        "title": "DeAL: Decoding-time Alignment for Large Language Models,",
        "abstract": "Large Language Models (LLMs) are nowadays expected to generate content\naligned with human preferences. Current work focuses on alignment at model\ntraining time, through techniques such as Reinforcement Learning with Human\nFeedback (RLHF). However, it is unclear if such methods are an effective choice\nto teach alignment objectives to the model. First, the inability to incorporate\nmultiple, custom rewards and reliance on a model developer's view of universal\nand static principles are key limitations. Second, the residual gaps in model\ntraining and the reliability of such approaches are also questionable (e.g.\nsusceptibility to jail-breaking even after safety training). To address these,\nwe propose DeAL, a framework that allows the user to customize reward functions\nand enables Decoding-time Alignment of LLMs (DeAL). At its core, we view\ndecoding as a heuristic-guided search process and facilitate the use of a wide\nvariety of alignment objectives. Our experiments with programmatic constraints\nsuch as keyword and length constraints (studied widely in the pre-LLM era) and\nabstract objectives such as harmlessness and helpfulness (proposed in the\npost-LLM era) show that we can DeAL with fine-grained trade-offs, improve\nadherence to alignment objectives, and address residual gaps in LLMs. Lastly,\nwhile DeAL can be effectively paired with RLHF and prompting techniques, its\ngenerality makes decoding slower, an optimization we leave for future work.",
        "score": 3.182542085647583
      },
      {
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment,",
        "abstract": "Generative foundation models are susceptible to implicit biases that can\narise from extensive unsupervised training data. Such biases can produce\nsuboptimal samples, skewed outcomes, and unfairness, with potentially serious\nconsequences. Consequently, aligning these models with human ethics and\npreferences is an essential step toward ensuring their responsible and\neffective deployment in real-world applications. Prior research has primarily\nemployed Reinforcement Learning from Human Feedback (RLHF) to address this\nproblem, where generative models are fine-tuned with RL algorithms guided by a\nhuman-feedback-informed reward model. However, the inefficiencies and\ninstabilities associated with RL algorithms frequently present substantial\nobstacles to the successful alignment, necessitating the development of a more\nrobust and streamlined approach. To this end, we introduce a new framework,\nReward rAnked FineTuning (RAFT), designed to align generative models\neffectively. Utilizing a reward model and a sufficient number of samples, our\napproach selects the high-quality samples, discarding those that exhibit\nundesired behavior, and subsequently enhancing the model by fine-tuning on\nthese filtered samples. Our studies show that RAFT can effectively improve the\nmodel performance in both reward learning and other automated metrics in both\nlarge language models and diffusion models.",
        "score": 3.1683237552642822
      },
      {
        "title": "Decoding-time Realignment of Language Models,",
        "abstract": "Aligning language models with human preferences is crucial for reducing\nerrors and biases in these models. Alignment techniques, such as reinforcement\nlearning from human feedback (RLHF), are typically cast as optimizing a\ntradeoff between human preference rewards and a proximity regularization term\nthat encourages staying close to the unaligned model. Selecting an appropriate\nlevel of regularization is critical: insufficient regularization can lead to\nreduced model capabilities due to reward hacking, whereas excessive\nregularization hinders alignment. Traditional methods for finding the optimal\nregularization level require retraining multiple models with varying\nregularization strengths. This process, however, is resource-intensive,\nespecially for large models. To address this challenge, we propose\ndecoding-time realignment (DeRa), a simple method to explore and evaluate\ndifferent regularization strengths in aligned models without retraining. DeRa\nenables control over the degree of alignment, allowing users to smoothly\ntransition between unaligned and aligned models. It also enhances the\nefficiency of hyperparameter tuning by enabling the identification of effective\nregularization strengths using a validation dataset.",
        "score": 3.1362268924713135
      },
      {
        "title": "LIRE: listwise reward enhancement for preference alignment,",
        "abstract": "Recently, tremendous strides have been made to align the generation of Large\nLanguage Models (LLMs) with human values to mitigate toxic or unhelpful\ncontent. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves\neffective and is widely adopted by researchers. However, implementing RLHF is\ncomplex, and its sensitivity to hyperparameters renders achieving stable\nperformance and scalability challenging. Furthermore, prevailing approaches to\npreference alignment primarily concentrate on pairwise comparisons, with\nlimited exploration into multi-response scenarios, thereby overlooking the\npotential richness within the candidate pool. For the above reasons, we propose\na new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a\ngradient-based reward optimization approach that incorporates the offline\nrewards of multiple responses into a streamlined listwise framework, thus\neliminating the need for online sampling during training. LIRE is\nstraightforward to implement, requiring minimal parameter tuning, and\nseamlessly aligns with the pairwise paradigm while naturally extending to\nmulti-response scenarios. Moreover, we introduce a self-enhancement algorithm\naimed at iteratively refining the reward during training. Our experiments\ndemonstrate that LIRE consistently outperforms existing methods across several\nbenchmarks on dialogue and summarization tasks, with good transferability to\nout-of-distribution data, assessed using proxy reward models and human\nannotators.",
        "score": 3.04681658744812
      },
      {
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and\n  Mixture-of-Experts,",
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. The\nRLHF process typically starts by training a reward model (RM) using human\npreference data. Conventional RMs are trained on pairwise responses to the same\nuser request, with relative ratings indicating which response humans prefer.\nThe trained RM serves as a proxy for human preferences. However, due to the\nblack-box nature of RMs, their outputs lack interpretability, as humans cannot\nintuitively understand why an RM thinks a response is good or not. As RMs act\nas human preference proxies, we believe they should be human-interpretable to\nensure that their internal decision processes are consistent with human\npreferences and to prevent reward hacking in LLM alignment. To build RMs with\ninterpretable preferences, we propose a two-stage approach: i) train an\nAbsolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional\nabsolute-rating data, each dimension corresponding to a human-interpretable\nobjective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts\n(MoE) strategy with a gating network that automatically selects the most\nsuitable reward objectives based on the context. We efficiently trained an\nArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top\nof the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art\nperformance on RewardBench, a benchmark evaluating RMs for language modeling.\nNotably, the performance of our model surpasses the LLM-as-a-judge method with\nGPT-4 judges by a margin, and approaches the performance of the much larger\nNemotron-4 340B reward model.",
        "score": 2.918048858642578
      },
      {
        "title": "Prototypical Reward Network for Data-Efficient RLHF,",
        "abstract": "The reward model for Reinforcement Learning from Human Feedback (RLHF) has\nproven effective in fine-tuning Large Language Models (LLMs). Notably,\ncollecting human feedback for RLHF can be resource-intensive and lead to\nscalability issues for LLMs and complex tasks. Our proposed framework Proto-RM\nleverages prototypical networks to enhance reward models under limited human\nfeedback. By enabling stable and reliable structural learning from fewer\nsamples, Proto-RM significantly enhances LLMs' adaptability and accuracy in\ninterpreting human preferences. Extensive experiments on various datasets\ndemonstrate that Proto-RM significantly improves the performance of reward\nmodels and LLMs in human feedback tasks, achieving comparable and usually\nbetter results than traditional methods, while requiring significantly less\ndata. in data-limited scenarios. This research offers a promising direction for\nenhancing the efficiency of reward models and optimizing the fine-tuning of\nlanguage models under restricted feedback conditions.",
        "score": 2.689920663833618
      },
      {
        "title": "Personalized Language Modeling from Personalized Human Feedback,",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) is commonly used to\nfine-tune large language models to better align with human preferences.\nHowever, the underlying premise of algorithms developed under this framework\ncan be problematic when user preferences encoded in human feedback are diverse.\nIn this work, we aim to address this problem by developing methods for building\npersonalized language models. We first formally introduce the task of learning\nfrom personalized human feedback and explain why vanilla RLHF can be\nineffective in this context. We then propose a general Personalized-RLHF\n(P-RLHF) framework, including a user model that maps user information to user\nrepresentations and can flexibly encode our assumptions on user preferences. We\ndevelop new learning objectives to perform personalized Direct Preference\nOptimization that jointly learns a user model and a personalized language\nmodel. We demonstrate the efficacy of our proposed method through (1) a\nsynthetic task where we fine-tune a GPT-J 6B model to align with users with\nconflicting preferences on generation length; and (2) an instruction following\ntask where we fine-tune a Tulu-7B model to generate responses for users with\ndiverse preferences on the style of responses. In both cases, our learned\nmodels can generate personalized responses that are better aligned with the\npreferences of individual users.",
        "score": 2.54296875
      },
      {
        "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to\n  RLHF,",
        "abstract": "Model alignment with human preferences is an essential step in making Large\nLanguage Models (LLMs) helpful and consistent with human values. It typically\nconsists of supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF) stages. However, RLHF faces inherent limitations stemming from\na complex training setup and its tendency to align the model with implicit\nvalues that end users cannot control at run-time. Moreover, reward models in\nRLHF stage commonly rely on single-dimensional feedback as opposed to explicit,\nmultifaceted signals that indicate attributes such as helpfulness, humor, and\ntoxicity. To address these limitations, we propose SteerLM, a supervised\nfine-tuning method that empowers end-users to control responses during\ninference. SteerLM conditions responses to conform to an explicitly defined\nmulti-dimensional set of attributes, thereby empowering a steerable AI capable\nof generating helpful and high-quality responses while maintaining\ncustomizability. Experiments show that SteerLM trained on open source datasets\ngenerates responses that are preferred by human and automatic evaluators to\nmany state-of-the-art baselines trained with RLHF while being much easier to\ntrain. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B",
        "score": 2.4475040435791016
      },
      {
        "title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement,",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been commonly used to\nalign the behaviors of Large Language Models (LLMs) with human preferences.\nRecently, a popular alternative is Direct Policy Optimization (DPO), which\nreplaces an LLM-based reward model with the policy itself, thus obviating the\nneed for extra memory and training time to learn the reward model. However, DPO\ndoes not consider the relative qualities of the positive and negative\nresponses, and can lead to sub-optimal training outcomes. To alleviate this\nproblem, we investigate the use of intrinsic knowledge within the on-the-fly\nfine-tuning LLM to obtain relative qualities and help to refine the loss\nfunction. Specifically, we leverage the knowledge of the LLM to design a\nrefinement function to estimate the quality of both the positive and negative\nresponses. We show that the constructed refinement function can help\nself-refine the loss function under mild assumptions. The refinement function\nis integrated into DPO and its variant Identity Policy Optimization (IPO).\nExperiments across various evaluators indicate that they can improve the\nperformance of the fine-tuned models over DPO and IPO.",
        "score": 2.380402088165283
      },
      {
        "title": "RLCD: Reinforcement Learning from Contrastive Distillation for Language\n  Model Alignment,",
        "abstract": "We propose Reinforcement Learning from Contrastive Distillation (RLCD), a\nmethod for aligning language models to follow principles expressed in natural\nlanguage (e.g., to be more harmless) without using human feedback. RLCD creates\npreference pairs from two contrasting model outputs, one using a positive\nprompt designed to encourage following the given principles, and one using a\nnegative prompt designed to encourage violating them. Using two different\nprompts causes model outputs to be more differentiated on average, resulting in\ncleaner preference labels in the absence of human annotations. We then use the\npreference pairs to train a preference model, which is in turn used to improve\na base unaligned language model via reinforcement learning. Empirically, RLCD\noutperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al.,\n2022) baselines across three diverse alignment tasks--harmlessness,\nhelpfulness, and story outline generation--and when using both 7B and 30B model\nscales for simulating preference data.",
        "score": 2.251162052154541
      },
      {
        "title": "West-of-N: Synthetic Preference Generation for Improved Reward Modeling,",
        "abstract": "The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.",
        "score": 1.9590204954147339
      }
    ]
  },
  {
    "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination\n  in Large Language Models",
    "abstract": "  In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.\n",
    "related_paper_titles": [
      "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of\n  LLMs by Validating Low-Confidence Generation",
      "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial\n  Examples",
      "Med-HALT: Medical Domain Hallucination Test for Large Language Models"
    ],
    "related_paper_abstract": [
      "  Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with GPT-3.5 (text-davinci-003) on the 'article\ngeneration task', we first demonstrate the individual efficacy of our detection\nand mitigation techniques. Specifically, the detection technique achieves a\nrecall of ~88% and the mitigation technique successfully mitigates 57.6% of the\ncorrectly detected hallucinations. Importantly, our mitigation technique does\nnot introduce new hallucinations even in the case of incorrectly detected\nhallucinations, i.e., false positives. Then, we show that the proposed active\ndetection and mitigation approach successfully reduces the hallucinations of\nthe GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the\neffectiveness and wide applicability of our approach through additional studies\nincluding performance on different types of questions (multi-hop and false\npremise questions) and with another LLM from a different model family (Vicuna).\nIn summary, our work contributes to improving the reliability and\ntrustworthiness of large language models, a crucial step en route to enabling\ntheir widespread adoption in real-world applications.\n",
      "  Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be\nknowledgeable and able to adapt to many tasks. However, we still cannot\ncompletely trust their answers, since LLMs suffer from\n\\textbf{hallucination}\\textemdash fabricating non-existent facts, deceiving\nusers with or without their awareness. However, the reasons for their existence\nand pervasiveness remain unclear. In this paper, we demonstrate that\nnonsensical prompts composed of random tokens can also elicit the LLMs to\nrespond with hallucinations. Moreover, we provide both theoretical and\nexperimental evidence that transformers can be manipulated to produce specific\npre-define tokens by perturbing its input sequence. This phenomenon forces us\nto revisit that \\emph{hallucination may be another view of adversarial\nexamples}, and it shares similar characteristics with conventional adversarial\nexamples as a basic property of LLMs. Therefore, we formalize an automatic\nhallucination triggering method as the \\textit{hallucination attack} in an\nadversarial way. Finally, we explore the basic properties of attacked\nadversarial prompts and propose a simple yet effective defense strategy. Our\ncode is released on\nGitHub\\footnote{https://github.com/PKU-YuanGroup/Hallucination-Attack}.\n",
      "  This research paper focuses on the challenges posed by hallucinations in\nlarge language models (LLMs), particularly in the context of the medical\ndomain. Hallucination, wherein these models generate plausible yet unverified\nor incorrect information, can have serious consequences in healthcare\napplications. We propose a new benchmark and dataset, Med-HALT (Medical Domain\nHallucination Test), designed specifically to evaluate and reduce\nhallucinations. Med-HALT provides a diverse multinational dataset derived from\nmedical examinations across various countries and includes multiple innovative\ntesting modalities. Med-HALT includes two categories of tests reasoning and\nmemory-based hallucination tests, designed to assess LLMs's problem-solving and\ninformation retrieval abilities.\n  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,\nMPT, and Falcon, revealing significant differences in their performance. The\npaper provides detailed insights into the dataset, promoting transparency and\nreproducibility. Through this work, we aim to contribute to the development of\nsafer and more reliable language models in healthcare. Our benchmark can be\nfound at medhalt.github.io\n"
    ],
    "entities": [
      "KAN",
      "Large Language Models Large Language Models",
      "Global",
      "Large",
      "KGC",
      "Retrieval Augmented",
      "LLaMA",
      "SFT",
      "Mistral",
      "Large Language Model",
      "ESG",
      "Large Language",
      "Language Models",
      "NLG",
      "ICL",
      "Direct Preference",
      "DMs",
      "Adam",
      "CAD",
      "Verilog",
      "HumanEval",
      "Llama",
      "CoT",
      "ToM",
      "APIs",
      "DPO",
      "Gemini",
      "ChatGPT",
      "Large Language Models Large",
      "KV"
    ],
    "retrieved_papers": [
      {
        "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination\n  in Large Language Models,",
        "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
        "score": 5.202032566070557
      },
      {
        "title": "A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions,",
        "abstract": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), leading to remarkable\nadvancements in text understanding and generation. Nevertheless, alongside\nthese strides, LLMs exhibit a critical tendency to produce hallucinations,\nresulting in content that is inconsistent with real-world facts or user inputs.\nThis phenomenon poses substantial challenges to their practical deployment and\nraises concerns over the reliability of LLMs in real-world scenarios, which\nattracts increasing attention to detect and mitigate these hallucinations. In\nthis survey, we aim to provide a thorough and in-depth overview of recent\nadvances in the field of LLM hallucinations. We begin with an innovative\ntaxonomy of LLM hallucinations, then delve into the factors contributing to\nhallucinations. Subsequently, we present a comprehensive overview of\nhallucination detection methods and benchmarks. Additionally, representative\napproaches designed to mitigate hallucinations are introduced accordingly.\nFinally, we analyze the challenges that highlight the current limitations and\nformulate open questions, aiming to delineate pathways for future research on\nhallucinations in LLMs.",
        "score": 2.072763204574585
      },
      {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large\n  Language Models,",
        "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate\nhallucinations, i.e., content that conflicts with the source or cannot be\nverified by the factual knowledge. To understand what types of content and to\nwhich extent LLMs are apt to hallucinate, we introduce the Hallucination\nEvaluation benchmark for Large Language Models (HaluEval), a large collection\nof generated and human-annotated hallucinated samples for evaluating the\nperformance of LLMs in recognizing hallucination. To generate these samples, we\npropose a ChatGPT-based two-step framework, i.e., sampling-then-filtering.\nBesides, we also hire some human labelers to annotate the hallucinations in\nChatGPT responses. The empirical results suggest that ChatGPT is likely to\ngenerate hallucinated content in specific topics by fabricating unverifiable\ninformation (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face\ngreat challenges in recognizing the hallucinations in texts. However, our\nexperiments also prove that providing external knowledge or adding reasoning\nsteps can help LLMs recognize hallucinations. Our benchmark can be accessed at\nhttps://github.com/RUCAIBox/HaluEval.",
        "score": 2.0318045616149902
      },
      {
        "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language\n  Models,",
        "abstract": "While Large language models (LLMs) have garnered widespread applications\nacross various domains due to their powerful language understanding and\ngeneration capabilities, the detection of non-factual or hallucinatory content\ngenerated by LLMs remains scarce. Currently, one significant challenge in\nhallucination detection is the laborious task of time-consuming and expensive\nmanual annotation of the hallucinatory generation. To address this issue, this\npaper first introduces a method for automatically constructing model-specific\nhallucination datasets based on existing fact-checking datasets called\nAutoHall. Furthermore, we propose a zero-resource and black-box hallucination\ndetection method based on self-contradiction. We conduct experiments towards\nprevalent open-/closed-source LLMs, achieving superior hallucination detection\nperformance compared to extant baselines. Moreover, our experiments reveal\nvariations in hallucination proportions and types among different models.",
        "score": 1.769448161125183
      },
      {
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large\n  Language Models,",
        "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.",
        "score": 1.4367316961288452
      },
      {
        "title": "Alleviating Hallucinations of Large Language Models through Induced\n  Hallucinations,",
        "abstract": "Despite their impressive capabilities, large language models (LLMs) have been\nobserved to generate responses that include inaccurate or fabricated\ninformation, a phenomenon commonly known as ``hallucination''. In this work, we\npropose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to\nalleviate hallucinations. We first construct a factually weak LLM by inducing\nhallucinations from the original LLMs. Then, we penalize these induced\nhallucinations during decoding to enhance the factuality of the generated\ncontent. Concretely, we determine the final next-token predictions by\namplifying the predictions from the original model and downplaying the induced\nuntruthful predictions via contrastive decoding. Experimental results on both\ndiscrimination-based and generation-based hallucination evaluation benchmarks,\nsuch as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD\nmethods can effectively enhance the factuality of LLMs across various model\nsizes and families. For example, when equipped with ICD, Llama2-7B-Chat and\nMistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on\nTruthfulQA, respectively.",
        "score": 1.3215115070343018
      },
      {
        "title": "HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level\n  Hallucination Evaluation,",
        "abstract": "Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), achieving remarkable performance across diverse\ntasks and enabling widespread real-world applications. However, LLMs are prone\nto hallucination, generating content that either conflicts with established\nknowledge or is unfaithful to the original sources. Existing hallucination\nbenchmarks primarily focus on sentence- or passage-level hallucination\ndetection, neglecting dialogue-level evaluation, hallucination localization,\nand rationale provision. They also predominantly target factuality\nhallucinations while underestimating faithfulness hallucinations, often relying\non labor-intensive or non-specialized evaluators. To address these limitations,\nwe propose HalluDial, the first comprehensive large-scale benchmark for\nautomatic dialogue-level hallucination evaluation. HalluDial encompasses both\nspontaneous and induced hallucination scenarios, covering factuality and\nfaithfulness hallucinations. The benchmark includes 4,094 dialogues with a\ntotal of 146,856 samples. Leveraging HalluDial, we conduct a comprehensive\nmeta-evaluation of LLMs' hallucination evaluation capabilities in\ninformation-seeking dialogues and introduce a specialized judge language model,\nHalluJudge. The high data quality of HalluDial enables HalluJudge to achieve\nsuperior or competitive performance in hallucination evaluation, facilitating\nthe automatic assessment of dialogue-level hallucinations in LLMs and providing\nvaluable insights into this phenomenon. The dataset and the code are available\nat https://github.com/FlagOpen/HalluDial.",
        "score": 1.230367660522461
      },
      {
        "title": "Unsupervised Real-Time Hallucination Detection based on the Internal\n  States of Large Language Models,",
        "abstract": "Hallucinations in large language models (LLMs) refer to the phenomenon of\nLLMs producing responses that are coherent yet factually inaccurate. This issue\nundermines the effectiveness of LLMs in practical applications, necessitating\nresearch into detecting and mitigating hallucinations of LLMs. Previous studies\nhave mainly concentrated on post-processing techniques for hallucination\ndetection, which tend to be computationally intensive and limited in\neffectiveness due to their separation from the LLM's inference process. To\novercome these limitations, we introduce MIND, an unsupervised training\nframework that leverages the internal states of LLMs for real-time\nhallucination detection without requiring manual annotations. Additionally, we\npresent HELM, a new benchmark for evaluating hallucination detection across\nmultiple LLMs, featuring diverse LLM outputs and the internal states of LLMs\nduring their inference process. Our experiments demonstrate that MIND\noutperforms existing state-of-the-art methods in hallucination detection.",
        "score": 1.1260497570037842
      },
      {
        "title": "Hallucination of Multimodal Large Language Models: A Survey,",
        "abstract": "This survey presents a comprehensive analysis of the phenomenon of\nhallucination in multimodal large language models (MLLMs), also known as Large\nVision-Language Models (LVLMs), which have demonstrated significant\nadvancements and remarkable abilities in multimodal tasks. Despite these\npromising developments, MLLMs often generate outputs that are inconsistent with\nthe visual content, a challenge known as hallucination, which poses substantial\nobstacles to their practical deployment and raises concerns regarding their\nreliability in real-world applications. This problem has attracted increasing\nattention, prompting efforts to detect and mitigate such inaccuracies. We\nreview recent advances in identifying, evaluating, and mitigating these\nhallucinations, offering a detailed overview of the underlying causes,\nevaluation benchmarks, metrics, and strategies developed to address this issue.\nAdditionally, we analyze the current challenges and limitations, formulating\nopen questions that delineate potential pathways for future research. By\ndrawing the granular classification and landscapes of hallucination causes,\nevaluation benchmarks, and mitigation methods, this survey aims to deepen the\nunderstanding of hallucinations in MLLMs and inspire further advancements in\nthe field. Through our thorough and in-depth review, we contribute to the\nongoing dialogue on enhancing the robustness and reliability of MLLMs,\nproviding valuable insights and resources for researchers and practitioners\nalike. Resources are available at:\nhttps://github.com/showlab/Awesome-MLLM-Hallucination.",
        "score": 1.0466647148132324
      },
      {
        "title": "Redefining \"Hallucination\" in LLMs: Towards a psychology-informed\n  framework for mitigating misinformation,",
        "abstract": "In recent years, large language models (LLMs) have become incredibly popular,\nwith ChatGPT for example being used by over a billion users. While these models\nexhibit remarkable language understanding and logical prowess, a notable\nchallenge surfaces in the form of \"hallucinations.\" This phenomenon results in\nLLMs outputting misinformation in a confident manner, which can lead to\ndevastating consequences with such a large user base. However, we question the\nappropriateness of the term \"hallucination\" in LLMs, proposing a psychological\ntaxonomy based on cognitive biases and other psychological phenomena. Our\napproach offers a more fine-grained understanding of this phenomenon, allowing\nfor targeted solutions. By leveraging insights from how humans internally\nresolve similar challenges, we aim to develop strategies to mitigate LLM\nhallucinations. This interdisciplinary approach seeks to move beyond\nconventional terminology, providing a nuanced understanding and actionable\npathways for improvement in LLM reliability.",
        "score": 0.8940437436103821
      },
      {
        "title": "Unified Hallucination Detection for Multimodal Large Language Models,",
        "abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.",
        "score": 0.5594128370285034
      },
      {
        "title": "In Search of Truth: An Interrogation Approach to Hallucination Detection,",
        "abstract": "Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 62% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\n(B-ACC) of 87%, all without relying on external knowledge.",
        "score": 0.5260909795761108
      },
      {
        "title": "A Survey on Hallucination in Large Vision-Language Models,",
        "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.",
        "score": 0.519214391708374
      },
      {
        "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak\n  Large Language Models,",
        "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP). Although convenient for research and practical applications, open-source\nLLMs with fewer parameters often suffer from severe hallucinations compared to\ntheir larger counterparts. This paper focuses on measuring and reducing\nhallucinations in BLOOM 7B, a representative of such weaker open-source LLMs\nthat are publicly available for research and commercial applications. We\nintroduce HaloCheck, a lightweight BlackBox knowledge-free framework designed\nto quantify the severity of hallucinations in LLMs. Additionally, we explore\ntechniques like knowledge injection and teacher-student approaches to alleviate\nhallucinations in low-parameter LLMs. Our experiments effectively demonstrate\nthe reduction of hallucinations in challenging domains for these LLMs.",
        "score": 0.474153995513916
      },
      {
        "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for\n  Hallucination Mitigation,",
        "abstract": "Large language models (LLMs) frequently hallucinate and produce factual\nerrors, yet our understanding of why they make these errors remains limited. In\nthis study, we delve into the underlying mechanisms of LLM hallucinations from\nthe perspective of inner representations, and discover a salient pattern\nassociated with hallucinations: correct generations tend to have sharper\ncontext activations in the hidden states of the in-context tokens, compared to\nthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\nto quantify the ``sharpness'' among the in-context hidden states and\nincorporate it into the decoding process to formulate a constrained decoding\napproach. Experiments on various knowledge-seeking and hallucination benchmarks\ndemonstrate our approach's consistent effectiveness, for example, achieving up\nto an 8.6 point improvement on TruthfulQA. We believe this study can improve\nour understanding of hallucinations and serve as a practical solution for\nhallucination mitigation.",
        "score": 0.3015368580818176
      },
      {
        "title": "HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination\n  Tendency of LLMs,",
        "abstract": "Hallucinations pose a significant challenge to the reliability and alignment\nof Large Language Models (LLMs), limiting their widespread acceptance beyond\nchatbot applications. Despite ongoing efforts, hallucinations remain a\nprevalent challenge in LLMs. The detection of hallucinations itself is also a\nformidable task, frequently requiring manual labeling or constrained\nevaluations. This paper introduces an automated scalable framework that\ncombines benchmarking LLMs' hallucination tendencies with efficient\nhallucination detection. We leverage LLMs to generate challenging tasks related\nto hypothetical phenomena, subsequently employing them as agents for efficient\nhallucination detection. The framework is domain-agnostic, allowing the use of\nany language model for benchmark creation or evaluation in any domain. We\nintroduce the publicly available HypoTermQA Benchmarking Dataset, on which\nstate-of-the-art models' performance ranged between 3% and 11%, and evaluator\nagents demonstrated a 6% error rate in hallucination prediction. The proposed\nframework provides opportunities to test and improve LLMs. Additionally, it has\nthe potential to generate benchmarking datasets tailored to specific domains,\nsuch as law, health, and finance.",
        "score": 0.19067928194999695
      },
      {
        "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation\n  Framework for Large Vision Language Models,",
        "abstract": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.",
        "score": 0.17240002751350403
      },
      {
        "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models,",
        "abstract": "As large language models continue to develop in the field of AI, text\ngeneration systems are susceptible to a worrisome phenomenon known as\nhallucination. In this study, we summarize recent compelling insights into\nhallucinations in LLMs. We present a novel taxonomy of hallucinations from\nvarious text generation tasks, thus provide theoretical insights, detection\nmethods and improvement approaches. Based on this, future research directions\nare proposed. Our contribution are threefold: (1) We provide a detailed and\ncomplete taxonomy for hallucinations appearing in text generation tasks; (2) We\nprovide theoretical analyses of hallucinations in LLMs and provide existing\ndetection and improvement methods; (3) We propose several research directions\nthat can be developed in the future. As hallucinations garner significant\nattention from the community, we will maintain updates on relevant research\nprogress.",
        "score": 0.082246333360672
      },
      {
        "title": "Fine-grained Hallucination Detection and Editing for Language Models,",
        "abstract": "Large language models (LMs) are prone to generate factual errors, which are\noften called hallucinations. In this paper, we introduce a comprehensive\ntaxonomy of hallucinations and argue that hallucinations manifest in diverse\nforms, each requiring varying degrees of careful assessments to verify\nfactuality. We propose a novel task of automatic fine-grained hallucination\ndetection and construct a new evaluation benchmark, FavaBench, that includes\nabout one thousand fine-grained human judgments on three LM outputs across\nvarious domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B)\nexhibit diverse types of hallucinations in the majority of their outputs in\ninformation-seeking scenarios. We train FAVA, a retrieval-augmented LM by\ncarefully creating synthetic data to detect and correct fine-grained\nhallucinations. On our benchmark, our automatic and human evaluations show that\nFAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination\ndetection, and edits suggested by FAVA improve the factuality of LM-generated\ntext.",
        "score": 0.017923720180988312
      },
      {
        "title": "Hallucination Detection and Hallucination Mitigation: An Investigation,",
        "abstract": "Large language models (LLMs), including ChatGPT, Bard, and Llama, have\nachieved remarkable successes over the last two years in a range of different\napplications. In spite of these successes, there exist concerns that limit the\nwide application of LLMs. A key problem is the problem of hallucination.\nHallucination refers to the fact that in addition to correct responses, LLMs\ncan also generate seemingly correct but factually incorrect responses. This\nreport aims to present a comprehensive review of the current literature on both\nhallucination detection and hallucination mitigation. We hope that this report\ncan serve as a good reference for both engineers and researchers who are\ninterested in LLMs and applying them to real world tasks.",
        "score": -0.007799514569342136
      },
      {
        "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus,",
        "abstract": "Large Language Models (LLMs) have gained significant popularity for their\nimpressive performance across diverse fields. However, LLMs are prone to\nhallucinate untruthful or nonsensical outputs that fail to meet user\nexpectations in many real-world applications. Existing works for detecting\nhallucinations in LLMs either rely on external knowledge for reference\nretrieval or require sampling multiple responses from the LLM for consistency\nverification, making these methods costly and inefficient. In this paper, we\npropose a novel reference-free, uncertainty-based method for detecting\nhallucinations in LLMs. Our approach imitates human focus in factuality\nchecking from three aspects: 1) focus on the most informative and important\nkeywords in the given text; 2) focus on the unreliable tokens in historical\ncontext which may lead to a cascade of hallucinations; and 3) focus on the\ntoken properties such as token type and token frequency. Experimental results\non relevant datasets demonstrate the effectiveness of our proposed method,\nwhich achieves state-of-the-art performance across all the evaluation metrics\nand eliminates the need for additional information.",
        "score": -0.09316116571426392
      },
      {
        "title": "Lynx: An Open Source Hallucination Evaluation Model,",
        "abstract": "Retrieval Augmented Generation (RAG) techniques aim to mitigate\nhallucinations in Large Language Models (LLMs). However, LLMs can still produce\ninformation that is unsupported or contradictory to the retrieved contexts. We\nintroduce LYNX, a SOTA hallucination detection LLM that is capable of advanced\nreasoning on challenging real-world hallucination scenarios. To evaluate LYNX,\nwe present HaluBench, a comprehensive hallucination evaluation benchmark,\nconsisting of 15k samples sourced from various real-world domains. Our\nexperiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and\nclosed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,\nHaluBench and our evaluation code for public access.",
        "score": -0.11711166054010391
      },
      {
        "title": "Do LLMs Know about Hallucination? An Empirical Investigation of LLM's\n  Hidden States,",
        "abstract": "Large Language Models (LLMs) can make up answers that are not real, and this\nis known as hallucination. This research aims to see if, how, and to what\nextent LLMs are aware of hallucination. More specifically, we check whether and\nhow an LLM reacts differently in its hidden states when it answers a question\nright versus when it hallucinates. To do this, we introduce an experimental\nframework which allows examining LLM's hidden states in different hallucination\nsituations. Building upon this framework, we conduct a series of experiments\nwith language models in the LLaMA family (Touvron et al., 2023). Our empirical\nfindings suggest that LLMs react differently when processing a genuine response\nversus a fabricated one. We then apply various model interpretation techniques\nto help understand and explain the findings better. Moreover, informed by the\nempirical observations, we show great potential of using the guidance derived\nfrom LLM's hidden representation space to mitigate hallucination. We believe\nthis work provides insights into how LLMs produce hallucinated answers and how\nto make them occur less often.",
        "score": -0.11981219798326492
      },
      {
        "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise\n  Hallucinations in Large Language Models,",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still\nsuffer from the issue of hallucinations. A significant type of this issue is\nthe false premise hallucination, which we define as the phenomenon when LLMs\ngenerate hallucinated text when confronted with false premise questions. In\nthis paper, we perform a comprehensive analysis of the false premise\nhallucination and elucidate its internal working mechanism: a small subset of\nattention heads (which we designate as false premise heads) disturb the\nknowledge extraction process, leading to the occurrence of false premise\nhallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse\npremise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating\n\\textbf{H}allucinations), a novel and effective method to mitigate false\npremise hallucinations. It constrains the false premise attention heads during\nthe model inference process. Impressively, extensive experiments demonstrate\nthat constraining only approximately $1\\%$ of the attention heads in the model\nyields a notable increase of nearly $20\\%$ of model performance.",
        "score": -0.3287433385848999
      },
      {
        "title": "The Two Sides of the Coin: Hallucination Generation and Detection with\n  LLMs as Evaluators for LLMs,",
        "abstract": "Hallucination detection in Large Language Models (LLMs) is crucial for\nensuring their reliability. This work presents our participation in the CLEF\nELOQUENT HalluciGen shared task, where the goal is to develop evaluators for\nboth generating and detecting hallucinated content. We explored the\ncapabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this\npurpose. We also employed ensemble majority voting to incorporate all four\nmodels for the detection task. The results provide valuable insights into the\nstrengths and weaknesses of these LLMs in handling hallucination generation and\ndetection tasks.",
        "score": -0.5234541296958923
      },
      {
        "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy\n  Retrieval-Augmented Language Models,",
        "abstract": "Retrieval-augmented generation (RAG) has become a main technique for\nalleviating hallucinations in large language models (LLMs). Despite the\nintegration of RAG, LLMs may still present unsupported or contradictory claims\nto the retrieved contents. In order to develop effective hallucination\nprevention strategies under RAG, it is important to create benchmark datasets\nthat can measure the extent of hallucination. This paper presents RAGTruth, a\ncorpus tailored for analyzing word-level hallucinations in various domains and\ntasks within the standard RAG frameworks for LLM applications. RAGTruth\ncomprises nearly 18,000 naturally generated responses from diverse LLMs using\nRAG. These responses have undergone meticulous manual annotations at both the\nindividual cases and word levels, incorporating evaluations of hallucination\nintensity. We not only benchmark hallucination frequencies across different\nLLMs, but also critically assess the effectiveness of several existing\nhallucination detection methodologies. Furthermore, we show that using a\nhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\nsmall LLM and achieve a competitive level of performance in hallucination\ndetection when compared to the existing prompt-based approaches using\nstate-of-the-art large language models such as GPT-4.",
        "score": -0.629791796207428
      },
      {
        "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of\n  LLMs by Validating Low-Confidence Generation,",
        "abstract": "Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with GPT-3.5 (text-davinci-003) on the 'article\ngeneration task', we first demonstrate the individual efficacy of our detection\nand mitigation techniques. Specifically, the detection technique achieves a\nrecall of ~88% and the mitigation technique successfully mitigates 57.6% of the\ncorrectly detected hallucinations. Importantly, our mitigation technique does\nnot introduce new hallucinations even in the case of incorrectly detected\nhallucinations, i.e., false positives. Then, we show that the proposed active\ndetection and mitigation approach successfully reduces the hallucinations of\nthe GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the\neffectiveness and wide applicability of our approach through additional studies\nincluding performance on different types of questions (multi-hop and false\npremise questions) and with another LLM from a different model family (Vicuna).\nIn summary, our work contributes to improving the reliability and\ntrustworthiness of large language models, a crucial step en route to enabling\ntheir widespread adoption in real-world applications.",
        "score": -0.6782721281051636
      },
      {
        "title": "Mechanistic Understanding and Mitigation of Language Model Non-Factual\n  Hallucinations,",
        "abstract": "State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. To explore the mechanistic\ncauses of these hallucinations, we create diagnostic datasets with\nsubject-relation queries and adapt interpretability methods to trace\nhallucinations through internal model representations. We discover two general\nand distinct mechanistic causes of hallucinations shared across LMs (Llama-2,\nPythia, GPT-J): 1) knowledge enrichment hallucinations: insufficient subject\nattribute knowledge in lower layer MLPs, and 2) answer extraction\nhallucinations: failure to select the correct object attribute in upper layer\nattention heads. We also found these two internal mechanistic causes of\nhallucinations are reflected in external manifestations. Based on insights from\nour mechanistic analysis, we propose a novel hallucination mitigation method\nthrough targeted restoration of the LM's internal fact recall pipeline,\ndemonstrating superior performance compared to baselines.",
        "score": -0.9243448376655579
      },
      {
        "title": "Halu-J: Critique-Based Hallucination Judge,",
        "abstract": "Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .",
        "score": -1.1027674674987793
      },
      {
        "title": "Chainpoll: A high efficacy method for LLM hallucination detection,",
        "abstract": "Large language models (LLMs) have experienced notable advancements in\ngenerating coherent and contextually relevant responses. However,\nhallucinations - incorrect or unfounded claims - are still prevalent, prompting\nthe creation of automated metrics to detect these in LLM outputs. Our\ncontributions include: introducing ChainPoll, an innovative hallucination\ndetection method that excels compared to its counterparts, and unveiling\nRealHall, a refined collection of benchmark datasets to assess hallucination\ndetection metrics from recent studies. While creating RealHall, we assessed\ntasks and datasets from previous hallucination detection studies and observed\nthat many are not suitable for the potent LLMs currently in use. Overcoming\nthis, we opted for four datasets challenging for modern LLMs and pertinent to\nreal-world scenarios. Using RealHall, we conducted a comprehensive comparison\nof ChainPoll with numerous hallucination metrics from recent studies. Our\nfindings indicate that ChainPoll outperforms in all RealHall benchmarks,\nachieving an overall AUROC of 0.781. This surpasses the next best theoretical\nmethod by 11% and exceeds industry standards by over 23%. Additionally,\nChainPoll is cost-effective and offers greater transparency than other metrics.\nWe introduce two novel metrics to assess LLM hallucinations: Adherence and\nCorrectness. Adherence is relevant to Retrieval Augmented Generation workflows,\nevaluating an LLM's analytical capabilities within given documents and\ncontexts. In contrast, Correctness identifies logical and reasoning errors.",
        "score": -1.3743590116500854
      }
    ]
  },
  {
    "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from\n  Large Language Models for Commonsense Reasoning",
    "abstract": "  The sequential process of conceptualization and instantiation is essential to\ngeneralizable commonsense reasoning as it allows the application of existing\nknowledge to unfamiliar scenarios. However, existing works tend to undervalue\nthe step of instantiation and heavily rely on pre-built concept taxonomies and\nhuman annotations to collect both types of knowledge, resulting in a lack of\ninstantiated knowledge to complete reasoning, high cost, and limited\nscalability. To tackle these challenges, we introduce CANDLE, a distillation\nframework that iteratively performs contextualized conceptualization and\ninstantiation over commonsense knowledge bases by instructing large language\nmodels to generate both types of knowledge with critic filtering. By applying\nCANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six\nmillion conceptualizations and instantiated commonsense knowledge triples. Both\ntypes of knowledge are firmly rooted in the original ATOMIC dataset, and\nintrinsic evaluations demonstrate their exceptional quality and diversity.\nEmpirical results indicate that distilling CANDLE on student models provides\nbenefits across four downstream tasks. Our code, data, and models are publicly\navailable at https://github.com/HKUST-KnowComp/CANDLE.\n",
    "related_paper_titles": [],
    "related_paper_abstract": [],
    "entities": [
      "RAG",
      "ChatGPT",
      "MLLMs",
      "CoT",
      "GPT",
      "ICL",
      "DPO",
      "RLHF",
      "English",
      "BERT",
      "Chinese",
      "GNNs",
      "Large Language Model",
      "Code",
      "SOTA",
      "ASR",
      "SFT",
      "Artificial Intelligence",
      "MLLM",
      "OpenAI",
      "Generative",
      "Python",
      "Language",
      "VLMs",
      "MoE",
      "NER",
      "Multimodal",
      "Gemini",
      "Large Language",
      "PEFT"
    ],
    "retrieved_papers": [
      {
        "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from\n  Large Language Models for Commonsense Reasoning,",
        "abstract": "The sequential process of conceptualization and instantiation is essential to\ngeneralizable commonsense reasoning as it allows the application of existing\nknowledge to unfamiliar scenarios. However, existing works tend to undervalue\nthe step of instantiation and heavily rely on pre-built concept taxonomies and\nhuman annotations to collect both types of knowledge, resulting in a lack of\ninstantiated knowledge to complete reasoning, high cost, and limited\nscalability. To tackle these challenges, we introduce CANDLE, a distillation\nframework that iteratively performs contextualized conceptualization and\ninstantiation over commonsense knowledge bases by instructing large language\nmodels to generate both types of knowledge with critic filtering. By applying\nCANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six\nmillion conceptualizations and instantiated commonsense knowledge triples. Both\ntypes of knowledge are firmly rooted in the original ATOMIC dataset, and\nintrinsic evaluations demonstrate their exceptional quality and diversity.\nEmpirical results indicate that distilling CANDLE on student models provides\nbenefits across four downstream tasks. Our code, data, and models are publicly\navailable at https://github.com/HKUST-KnowComp/CANDLE.",
        "score": 6.204645156860352
      },
      {
        "title": "CAT: A Contextualized Conceptualization and Instantiation Framework for\n  Commonsense Reasoning,",
        "abstract": "Commonsense reasoning, aiming at endowing machines with a human-like ability\nto make situational presumptions, is extremely challenging to generalize. For\nsomeone who barely knows about \"meditation,\" while is knowledgeable about\n\"singing,\" he can still infer that \"meditation makes people relaxed\" from the\nexisting knowledge that \"singing makes people relaxed\" by first conceptualizing\n\"singing\" as a \"relaxing event\" and then instantiating that event to\n\"meditation.\" This process, known as conceptual induction and deduction, is\nfundamental to commonsense reasoning while lacking both labeled data and\nmethodologies to enhance commonsense modeling. To fill such a research gap, we\npropose CAT (Contextualized ConceptuAlization and InsTantiation), a\nsemi-supervised learning framework that integrates event conceptualization and\ninstantiation to conceptualize commonsense knowledge bases at scale. Extensive\nexperiments show that our framework achieves state-of-the-art performances on\ntwo conceptualization tasks, and the acquired abstract commonsense knowledge\ncan significantly improve commonsense inference modeling. Our code, data, and\nfine-tuned models are publicly available at\nhttps://github.com/HKUST-KnowComp/CAT.",
        "score": 1.7494480609893799
      },
      {
        "title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question\n  Answering with Large Language Models,",
        "abstract": "This study explores the realm of knowledge base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nHowever, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field.",
        "score": 1.2788797616958618
      },
      {
        "title": "Acquiring and Modelling Abstract Commonsense Knowledge via\n  Conceptualization,",
        "abstract": "Conceptualization, or viewing entities and situations as instances of\nabstract concepts in mind and making inferences based on that, is a vital\ncomponent in human intelligence for commonsense reasoning. Despite recent\nprogress in artificial intelligence to acquire and model commonsense attributed\nto neural language models and commonsense knowledge graphs (CKGs),\nconceptualization is yet to be introduced thoroughly, making current approaches\nineffective to cover knowledge about countless diverse entities and situations\nin the real world.\n  To address the problem, we thoroughly study the role of conceptualization in\ncommonsense reasoning, and formulate a framework to replicate human conceptual\ninduction by acquiring abstract knowledge about events regarding abstract\nconcepts, as well as higher-level triples or inferences upon them. We then\napply the framework to ATOMIC, a large-scale human-annotated CKG, aided by the\ntaxonomy Probase. We annotate a dataset on the validity of contextualized\nconceptualizations from ATOMIC on both event and triple levels, develop a\nseries of heuristic rules based on linguistic features, and train a set of\nneural models to generate and verify abstract knowledge. Based on these\ncomponents, a pipeline to acquire abstract knowledge is built. A large abstract\nCKG upon ATOMIC is then induced, ready to be instantiated to infer about unseen\nentities or situations. Finally, we empirically show the benefits of augmenting\nCKGs with abstract knowledge in downstream tasks like commonsense inference and\nzero-shot commonsense QA.",
        "score": 1.188204288482666
      },
      {
        "title": "TinyLLM: Learning a Small Student from Multiple Large Language Models,",
        "abstract": "Transferring the reasoning capability from stronger large language models\n(LLMs) to smaller ones has been quite appealing, as smaller LLMs are more\nflexible to deploy with less expense. Among the existing solutions, knowledge\ndistillation stands out due to its outstanding efficiency and generalization.\nHowever, existing methods suffer from several drawbacks, including limited\nknowledge diversity and the lack of rich contextual information. To solve the\nproblems and facilitate the learning of compact language models, we propose\nTinyLLM, a new knowledge distillation paradigm to learn a small student LLM\nfrom multiple large teacher LLMs. In particular, we encourage the student LLM\nto not only generate the correct answers but also understand the rationales\nbehind these answers. Given that different LLMs possess diverse reasoning\nskills, we guide the student model to assimilate knowledge from various teacher\nLLMs. We further introduce an in-context example generator and a\nteacher-forcing Chain-of-Thought strategy to ensure that the rationales are\naccurate and grounded in contextually appropriate scenarios. Extensive\nexperiments on six datasets across two reasoning tasks demonstrate the\nsuperiority of our method. Results show that TinyLLM can outperform large\nteacher LLMs significantly, despite a considerably smaller model size.",
        "score": 0.8490927219390869
      },
      {
        "title": "Symbolic Knowledge Distillation: from General Language Models to\n  Commonsense Models,",
        "abstract": "The common practice for training commonsense models has gone\nfrom-human-to-corpus-to-machine: humans author commonsense knowledge graphs in\norder to train commonsense models. In this work, we investigate an alternative,\nfrom-machine-to-corpus-to-machine: general language models author these\ncommonsense knowledge graphs to train commonsense models. Our study leads to a\nnew framework, Symbolic Knowledge Distillation. As with prior art in Knowledge\nDistillation (Hinton et al., 2015), our approach uses larger models to teach\nsmaller models. A key difference is that we distill knowledge symbolically-as\ntext-in addition to the neural model. We also distill only one aspect-the\ncommonsense of a general language model teacher, allowing the student to be a\ndifferent type, a commonsense model. Altogether, we show that careful prompt\nengineering and a separately trained critic model allow us to selectively\ndistill high-quality causal commonsense from GPT-3, a general language model.\nEmpirical results demonstrate that, for the first time, a human-authored\ncommonsense knowledge graph is surpassed by our automatically distilled variant\nin all three criteria: quantity, quality, and diversity. In addition, it\nresults in a neural commonsense model that surpasses the teacher model's\ncommonsense capabilities despite its 100x smaller size. We apply this to the\nATOMIC resource, and share our new symbolic knowledge graph and commonsense\nmodels.",
        "score": 0.24659980833530426
      },
      {
        "title": "Retrieval Augmentation for Commonsense Reasoning: A Unified Approach,",
        "abstract": "A common thread of retrieval-augmented methods in the existing literature\nfocuses on retrieving encyclopedic knowledge, such as Wikipedia, which\nfacilitates well-defined entity and relation spaces that can be modeled.\nHowever, applying such methods to commonsense reasoning tasks faces two unique\nchallenges, i.e., the lack of a general large-scale corpus for retrieval and a\ncorresponding effective commonsense retriever. In this paper, we systematically\ninvestigate how to leverage commonsense knowledge retrieval to improve\ncommonsense reasoning tasks. We proposed a unified framework of\nretrieval-augmented commonsense reasoning (called RACo), including a newly\nconstructed commonsense corpus with over 20 million documents and novel\nstrategies for training a commonsense retriever. We conducted experiments on\nfour different commonsense reasoning tasks. Extensive evaluation results showed\nthat our proposed RACo can significantly outperform other knowledge-enhanced\nmethod counterparts, achieving new SoTA performance on the CommonGen and CREAK\nleaderboards.",
        "score": 0.1495160609483719
      },
      {
        "title": "Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge\n  Distillation in Small Models for Scientific QA,",
        "abstract": "Large Language Models (LLMs) have shown outstanding performance across wide\nrange of downstream tasks. This competency is attributed to their substantial\nparameter size and pre-training on extensive corpus. Moreover, LLMs have\nexhibited enhanced reasoning capabilities in tackling complex reasoning tasks,\nowing to the utilization of a method named ``Chain-of-Thought (CoT)\nprompting''. This method is designed to generate intermediate reasoning steps\nthat guide the inference of the final answer. However, it is essential to\nhighlight that these advanced reasoning abilities appear to emerge in models\nwith a minimum of 10 billion parameters, thereby limiting its efficacy in\nsituations where computational resources are constrained. In this paper, we\ninvestigate the possibility of transferring the reasoning capabilities of LLMs\nto smaller models via knowledge distillation. Specifically, we propose Sci-CoT,\na two-stage framework that separates the processes of generating rationales and\ninferring answers. This method enables a more efficient use of rationales\nduring the answer inference stage, leading to improved performance on\nscientific question-answering tasks. Utilizing Sci-CoT, our 80-million\nparameter model is able to exceed the performance of BLOOM-176B in the ARC-Easy\ndataset under the few shot setting.",
        "score": 0.1137482225894928
      },
      {
        "title": "Distilling Reasoning Capabilities into Smaller Language Models,",
        "abstract": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to\nbe very effective in inducing reasoning capabilities in large language models.\nHowever, the success of the CoT approach is fundamentally tied to the model\nsize, and billion parameter-scale models are often needed to get CoT to work.\nIn this paper, we propose a knowledge distillation approach that leverages the\nstep-by-step CoT reasoning capabilities of larger models and distills these\nabilities into smaller models.\n  In this work, we propose an alternative reasoning scheme, Socratic CoT, that\nlearns a decomposition of the original problem into a sequence of subproblems\nand uses it to guide the intermediate reasoning steps. We use Socratic CoT to\ntrain a combination of two small distilled models: a problem decomposer and a\nsubproblem solver. In practice, given a new problem, the two distilled models\nwork in sync to decompose and solve complex problems. On multiple reasoning\ndatasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies\nboosts the performance of smaller models over 70% compared to the baselines.\nFinally, we investigate when Socratic CoT is an effective alternative to CoT,\ndemonstrating cases where a much smaller model (GPT-2 large) can outperform a\n10X larger model (GPT-3 6B). Our code is available here:\nhttps://github.com/kumar-shridhar/Distiiling-LM",
        "score": -0.0038358550518751144
      },
      {
        "title": "Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach,",
        "abstract": "Pre-trained models (PTMs) have lead to great improvements in natural language\ngeneration (NLG). However, it is still unclear how much commonsense knowledge\nthey possess. With the goal of evaluating commonsense knowledge of NLG models,\nrecent work has proposed the problem of generative commonsense reasoning, e.g.,\nto compose a logical sentence given a set of unordered concepts. Existing\napproaches to this problem hypothesize that PTMs lack sufficient parametric\nknowledge for this task, which can be overcome by introducing external\nknowledge or task-specific pre-training objectives. Different from this trend,\nwe argue that PTM's inherent ability for generative commonsense reasoning is\nunderestimated due to the order-agnostic property of its input. In particular,\nwe hypothesize that the order of the input concepts can affect the PTM's\nability to utilize its commonsense knowledge. To this end, we propose a\npre-ordering approach to elaborately manipulate the order of the given concepts\nbefore generation. Experiments show that our approach can outperform the more\nsophisticated models that have access to a lot of external data and resources.",
        "score": -0.01871572993695736
      },
      {
        "title": "ELAD: Explanation-Guided Large Language Models Active Distillation,",
        "abstract": "The deployment and application of Large Language Models (LLMs) is hindered by\ntheir memory inefficiency, computational demands, and the high costs of API\ninferences. Traditional distillation methods, which transfer the capabilities\nof LLMs to smaller models, often fail to determine whether the knowledge has\nbeen sufficiently transferred, potentially resulting in high costs or\nincomplete distillation. In this paper, we propose an Explanation-Guided LLMs\nActive Distillation (ELAD) framework that employs an active learning strategy\nto optimize the balance between annotation costs and model performance. To\nimprove efficient sample selection, we introduce an explanation-guided sample\nselection method that identifies samples challenging its reasoning by\nexploiting uncertainties in explanation steps. Additionally, we present a\ncustomized LLM-annotated explanation revision technique where the teacher model\ndetects and corrects flaws in the student model's reasoning. Our experiments\nacross various reasoning datasets demonstrate that our framework significantly\nenhances the efficiency of LLM knowledge distillation.",
        "score": -0.17304861545562744
      },
      {
        "title": "Generated Knowledge Prompting for Commonsense Reasoning,",
        "abstract": "It remains an open question whether incorporating external knowledge benefits\ncommonsense reasoning while maintaining the flexibility of pretrained sequence\nmodels. To investigate this question, we develop generated knowledge prompting,\nwhich consists of generating knowledge from a language model, then providing\nthe knowledge as additional input when answering a question. Our method does\nnot require task-specific supervision for knowledge integration, or access to a\nstructured knowledge base, yet it improves performance of large-scale,\nstate-of-the-art models on four commonsense reasoning tasks, achieving\nstate-of-the-art results on numerical commonsense (NumerSense), general\ncommonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks.\nGenerated knowledge prompting highlights large-scale language models as\nflexible sources of external knowledge for improving commonsense reasoning. Our\ncode is available at https://github.com/liujch1998/GKP",
        "score": -0.21520733833312988
      },
      {
        "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in\n  Knowledge-Intensive Tasks,",
        "abstract": "Large Language Models (LLMs) have shown promising performance in\nknowledge-intensive reasoning tasks that require a compound understanding of\nknowledge. However, deployment of the LLMs in real-world applications can be\nchallenging due to their high computational requirements and concerns on data\nprivacy. Previous studies have focused on building task-specific small Language\nModels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,\nthese approaches are ill-suited for knowledge-intensive reasoning tasks due to\nthe limited capacity of small LMs in memorizing the knowledge required.\nMotivated by our theoretical analysis on memorization, we propose\nKnowledge-Augmented Reasoning Distillation (KARD), a novel method that\nfine-tunes small LMs to generate rationales obtained from LLMs with augmented\nknowledge retrieved from an external knowledge base. Moreover, we further\npropose a neural reranker to obtain documents relevant to rationale generation.\nWe empirically show that KARD significantly improves the performance of small\nT5 and GPT models on the challenging knowledge-intensive reasoning datasets,\nnamely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the\n250M T5 models achieve superior performance against the fine-tuned 3B models,\nhaving 12 times larger parameters, on both MedQA-USMLE and StrategyQA\nbenchmarks.",
        "score": -0.2579325735569
      },
      {
        "title": "ArT: All-round Thinker for Unsupervised Commonsense Question-Answering,",
        "abstract": "Without labeled question-answer pairs for necessary training, unsupervised\ncommonsense question-answering (QA) appears to be extremely challenging due to\nits indispensable unique prerequisite on commonsense source like knowledge\nbases (KBs), which are usually highly resource consuming in construction.\nRecently pre-trained language models (PLMs) show effectiveness as an\nalternative for commonsense clues when they play a role of knowledge generator.\nHowever, existing work either relies on large-scale in-domain or out-of-domain\nlabeled data, or fails to generate knowledge of high quality in a general way.\nMotivated by human thinking experience, we propose an approach of All-round\nThinker (ArT) by fully taking association during knowledge generating. In\ndetail, our model first focuses on key parts in the given context, and then\ngenerates highly related knowledge on such a basis in an association way like\nhuman thinking. Besides, for causal reasoning, a reverse thinking mechanism is\nespecially added to further enhance bidirectional inferring between cause and\neffect. ArT is totally unsupervised and KBs-free. We evaluate it on three\ncommonsense QA benchmarks: COPA, SocialIQA and SCT. On all scales of PLM\nbackbones, ArT shows its brilliant performance and outperforms previous\nadvanced unsupervised models. Our code is available at\nhttps://github.com/WangJW424/commonsenseQA-ArT.",
        "score": -0.30301642417907715
      },
      {
        "title": "Probing Commonsense Knowledge in Pre-trained Language Models with\n  Sense-level Precision and Expanded Vocabulary,",
        "abstract": "Progress on commonsense reasoning is usually measured from performance\nimprovements on Question Answering tasks designed to require commonsense\nknowledge. However, fine-tuning large Language Models (LMs) on these specific\ntasks does not directly evaluate commonsense learned during pre-training. The\nmost direct assessments of commonsense knowledge in pre-trained LMs are\narguably cloze-style tasks targeting commonsense assertions (e.g., A pen is\nused for [MASK].). However, this approach is restricted by the LM's vocabulary\navailable for masked predictions, and its precision is subject to the context\nprovided by the assertion. In this work, we present a method for enriching LMs\nwith a grounded sense inventory (i.e., WordNet) available at the vocabulary\nlevel, without further training. This modification augments the prediction\nspace of cloze-style prompts to the size of a large ontology while enabling\nfiner-grained (sense-level) queries and predictions. In order to evaluate LMs\nwith higher precision, we propose SenseLAMA, a cloze-style task featuring\nverbalized relations from disambiguated triples sourced from WordNet, WikiData,\nand ConceptNet. Applying our method to BERT, producing a WordNet-enriched\nversion named SynBERT, we find that LMs can learn non-trivial commonsense\nknowledge from self-supervision, covering numerous relations, and more\neffectively than comparable similarity-based approaches.",
        "score": -0.3681083023548126
      },
      {
        "title": "Meta-KD: A Meta Knowledge Distillation Framework for Language Model\n  Compression across Domains,",
        "abstract": "Pre-trained language models have been applied to various NLP tasks with\nconsiderable performance gains. However, the large model sizes, together with\nthe long inference time, limit the deployment of such models in real-time\napplications. One line of model compression approaches considers knowledge\ndistillation to distill large teacher models into small student models. Most of\nthese studies focus on single-domain only, which ignores the transferable\nknowledge from other domains. We notice that training a teacher with\ntransferable knowledge digested across domains can achieve better\ngeneralization capability to help knowledge distillation. Hence we propose a\nMeta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model\nthat captures transferable knowledge across domains and passes such knowledge\nto students. Specifically, we explicitly force the meta-teacher to capture\ntransferable knowledge at both instance-level and feature-level from multiple\ndomains, and then propose a meta-distillation algorithm to learn single-domain\nstudent models with guidance from the meta-teacher. Experiments on public\nmulti-domain NLP tasks show the effectiveness and superiority of the proposed\nMeta-KD framework. Further, we also demonstrate the capability of Meta-KD in\nthe settings where the training data is scarce.",
        "score": -0.465913325548172
      },
      {
        "title": "Teaching Small Language Models to Reason,",
        "abstract": "Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.",
        "score": -0.5049982666969299
      },
      {
        "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs,",
        "abstract": "Event commonsense reasoning requires the ability to reason about the\nrelationship between events, as well as infer implicit context underlying that\nrelationship. However, data scarcity makes it challenging for language models\nto learn to generate commonsense inferences for contexts and questions\ninvolving interactions between complex events. To address this demand, we\npresent COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop\nlogical queries (e.g., the joint effect or cause of both event A and B, or the\neffect of the effect of event C) from an existing commonsense knowledge graph\n(CSKG), and verbalizing them using handcrafted rules and large language models\ninto multiple-choice and text generation questions. Our experiments show that\nlanguage models trained on COM2 exhibit significant improvements in complex\nreasoning ability, resulting in enhanced zero-shot performance in both\nin-domain and out-of-domain tasks for question answering and generative\ncommonsense reasoning, without expensive human annotations. Code and data are\navailable at https://github.com/tqfang/complex-commonsense-reasoning.",
        "score": -0.9036890268325806
      },
      {
        "title": "Struct-X: Enhancing Large Language Models Reasoning with Structured Data,",
        "abstract": "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext.",
        "score": -0.9318328499794006
      },
      {
        "title": "NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge\n  Distillation,",
        "abstract": "We present NovaCOMET, an open commonsense knowledge model, that combines the\nbest aspects of knowledge and general task models. Compared to previous\nknowledge models, NovaCOMET allows open-format relations enabling direct\napplication to reasoning tasks; compared to general task models like Flan-T5,\nit explicitly centers knowledge, enabling superior performance for commonsense\nreasoning.\n  NovaCOMET leverages the knowledge of opaque proprietary models to create an\nopen knowledge pipeline. First, knowledge is symbolically distilled into\nNovATOMIC, a publicly-released discrete knowledge graph which can be audited,\ncritiqued, and filtered. Next, we train NovaCOMET on NovATOMIC by fine-tuning\nan open-source pretrained model. NovaCOMET uses an open-format training\nobjective, replacing the fixed relation sets of past knowledge models, enabling\narbitrary structures within the data to serve as inputs or outputs.\n  The resulting generation model, optionally augmented with human annotation,\nmatches or exceeds comparable open task models like Flan-T5 on a range of\ncommonsense generation tasks. NovaCOMET serves as a counterexample to the\ncontemporary focus on instruction tuning only, demonstrating a distinct\nadvantage to explicitly modeling commonsense knowledge as well.",
        "score": -1.0352859497070312
      },
      {
        "title": "Unpacking Large Language Models with Conceptual Consistency,",
        "abstract": "If a Large Language Model (LLM) answers \"yes\" to the question \"Are mountains\ntall?\" then does it know what a mountain is? Can you rely on it responding\ncorrectly or incorrectly to other questions about mountains? The success of\nLarge Language Models (LLMs) indicates they are increasingly able to answer\nqueries like these accurately, but that ability does not necessarily imply a\ngeneral understanding of concepts relevant to the anchor query. We propose\nconceptual consistency to measure a LLM's understanding of relevant concepts.\nThis novel metric measures how well a model can be characterized by finding out\nhow consistent its responses to queries about conceptually relevant background\nknowledge are. To compute it we extract background knowledge by traversing\npaths between concepts in a knowledge base and then try to predict the model's\nresponse to the anchor query from the background knowledge. We investigate the\nperformance of current LLMs in a commonsense reasoning setting using the CSQA\ndataset and the ConceptNet knowledge base. While conceptual consistency, like\nother metrics, does increase with the scale of the LLM used, we find that\npopular models do not necessarily have high conceptual consistency. Our\nanalysis also shows significant variation in conceptual consistency across\ndifferent kinds of relations, concepts, and prompts. This serves as a step\ntoward building models that humans can apply a theory of mind to, and thus\ninteract with intuitively.",
        "score": -1.129254937171936
      },
      {
        "title": "Large Language Models Are Reasoning Teachers,",
        "abstract": "Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model's ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher.",
        "score": -1.1330533027648926
      },
      {
        "title": "Propagating Knowledge Updates to LMs Through Distillation,",
        "abstract": "Modern language models have the capacity to store and use immense amounts of\nknowledge about real-world entities, but it remains unclear how to update such\nknowledge stored in model parameters. While prior methods for updating\nknowledge in LMs successfully inject atomic facts, updated LMs fail to make\ninferences based on injected facts. In this work, we demonstrate that a context\ndistillation-based approach can both impart knowledge about entities and\npropagate that knowledge to enable broader inferences. Our approach consists of\ntwo stages: transfer set generation and distillation on the transfer set. We\nfirst generate a transfer set by prompting a language model to generate\ncontinuations from the entity definition. Then, we update the model parameters\nso that the distribution of the LM (the student) matches the distribution of\nthe LM conditioned on the definition (the teacher) on the transfer set. Our\nexperiments demonstrate that this approach is more effective at propagating\nknowledge updates than fine-tuning and other gradient-based knowledge-editing\nmethods. Moreover, it does not compromise performance in other contexts, even\nwhen injecting the definitions of up to 150 entities at once.",
        "score": -1.421870231628418
      },
      {
        "title": "COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,",
        "abstract": "Conceptual knowledge is fundamental to human cognition and knowledge bases.\nHowever, existing knowledge probing works only focus on evaluating factual\nknowledge of pre-trained language models (PLMs) and ignore conceptual\nknowledge. Since conceptual knowledge often appears as implicit commonsense\nbehind texts, designing probes for conceptual knowledge is hard. Inspired by\nknowledge representation schemata, we comprehensively evaluate conceptual\nknowledge of PLMs by designing three tasks to probe whether PLMs organize\nentities by conceptual similarities, learn conceptual properties, and\nconceptualize entities in contexts, respectively. For the tasks, we collect and\nannotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual\nknowledge Probing bENchmark. Extensive experiments on different sizes and types\nof PLMs show that existing PLMs systematically lack conceptual knowledge and\nsuffer from various spurious correlations. We believe this is a critical\nbottleneck for realizing human-like cognition in PLMs. COPEN and our codes are\npublicly released at https://github.com/THU-KEG/COPEN.",
        "score": -1.4566200971603394
      },
      {
        "title": "Query of CC: Unearthing Large Scale Domain-Specific Knowledge from\n  Public Corpora,",
        "abstract": "Large language models have demonstrated remarkable potential in various\ntasks, however, there remains a significant scarcity of open-source models and\ndata for specific domains. Previous works have primarily focused on manually\nspecifying resources and collecting high-quality data on specific domains,\nwhich significantly consume time and effort. To address this limitation, we\npropose an efficient data collection method $\\textit{Query of CC}$ based on\nlarge language models. This method bootstraps seed information through a large\nlanguage model and retrieves related data from public corpora. It not only\ncollects knowledge-related data for specific domains but unearths the data with\npotential reasoning procedures. Through the application of this method, we have\ncurated a high-quality dataset called KNOWLEDGE PILE, encompassing four major\ndomains, including stem and humanities sciences, among others. Experimental\nresults demonstrate that KNOWLEDGE PILE significantly improves the performance\nof large language models in mathematical and knowledge-related reasoning\nability tests. To facilitate academic sharing, we open-source our dataset and\ncode, providing valuable support to the academic community.",
        "score": -1.6035895347595215
      },
      {
        "title": "Logical Reasoning for Natural Language Inference Using Generated Facts\n  as Atoms,",
        "abstract": "State-of-the-art neural models can now reach human performance levels across\nvarious natural language understanding tasks. However, despite this impressive\nperformance, models are known to learn from annotation artefacts at the expense\nof the underlying task. While interpretability methods can identify influential\nfeatures for each prediction, there are no guarantees that these features are\nresponsible for the model decisions. Instead, we introduce a model-agnostic\nlogical framework to determine the specific information in an input responsible\nfor each model decision. This method creates interpretable Natural Language\nInference (NLI) models that maintain their predictive power. We achieve this by\ngenerating facts that decompose complex NLI observations into individual\nlogical atoms. Our model makes predictions for each atom and uses logical rules\nto decide the class of the observation based on the predictions for each atom.\nWe apply our method to the highly challenging ANLI dataset, where our framework\nimproves the performance of both a DeBERTa-base and BERT baseline. Our method\nperforms best on the most challenging examples, achieving a new\nstate-of-the-art for the ANLI round 3 test set. We outperform every baseline in\na reduced-data setting, and despite using no annotations for the generated\nfacts, our model predictions for individual facts align with human\nexpectations.",
        "score": -1.7164949178695679
      },
      {
        "title": "Fact-driven Logical Reasoning for Machine Reading Comprehension,",
        "abstract": "Recent years have witnessed an increasing interest in training machines with\nreasoning ability, which deeply relies on accurately and clearly presented clue\nforms. The clues are usually modeled as entity-aware knowledge in existing\nstudies. However, those entity-aware clues are primarily focused on\ncommonsense, making them insufficient for tasks that require knowledge of\ntemporary facts or events, particularly in logical reasoning for reading\ncomprehension. To address this challenge, we are motivated to cover both\ncommonsense and temporary knowledge clues hierarchically. Specifically, we\npropose a general formalism of knowledge units by extracting backbone\nconstituents of the sentence, such as the subject-verb-object formed ``facts''.\nWe then construct a supergraph on top of the fact units, allowing for the\nbenefit of sentence-level (relations among fact groups) and entity-level\ninteractions (concepts or actions inside a fact). Experimental results on\nlogical reasoning benchmarks and dialogue modeling datasets show that our\napproach improves the baselines substantially, and it is general across\nbackbone models. Code is available at\n\\url{https://github.com/ozyyshr/FocalReasoner}.",
        "score": -1.7565542459487915
      },
      {
        "title": "A Learn-Then-Reason Model Towards Generalization in Knowledge Base\n  Question Answering,",
        "abstract": "Large-scale knowledge bases (KBs) like Freebase and Wikidata house millions\nof structured knowledge. Knowledge Base Question Answering (KBQA) provides a\nuser-friendly way to access these valuable KBs via asking natural language\nquestions. In order to improve the generalization capabilities of KBQA models,\nextensive research has embraced a retrieve-then-reason framework to retrieve\nrelevant evidence for logical expression generation. These multi-stage efforts\nprioritize acquiring external sources but overlook the incorporation of new\nknowledge into their model parameters. In effect, even advanced language models\nand retrievers have knowledge boundaries, thereby limiting the generalization\ncapabilities of previous KBQA models. Therefore, this paper develops KBLLaMA,\nwhich follows a learn-then-reason framework to inject new KB knowledge into a\nlarge language model for flexible end-to-end KBQA. At the core of KBLLaMA, we\nstudy (1) how to organize new knowledge about KBQA and (2) how to facilitate\nthe learning of the organized knowledge. Extensive experiments on various KBQA\ngeneralization tasks showcase the state-of-the-art performance of KBLLaMA.\nEspecially on the general benchmark GrailQA and domain-specific benchmark\nBio-chemical, KBLLaMA respectively derives a performance gain of up to 3.8% and\n9.8% compared to the baselines.",
        "score": -2.209059476852417
      },
      {
        "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized\n  Language Models,",
        "abstract": "By design, large language models (LLMs) are static general-purpose models,\nexpensive to retrain or update frequently. As they are increasingly adopted for\nknowledge-intensive tasks, it becomes evident that these design choices lead to\nfailures to generate factual, relevant, and up-to-date knowledge. To this end,\nwe propose Knowledge Card, a modular framework to plug in new factual and\nrelevant knowledge into general-purpose LLMs. We first introduce knowledge\ncards -- specialized language models trained on corpora from specific domains\nand sources. Knowledge cards serve as parametric repositories that are selected\nat inference time to generate background knowledge for the base LLM. We then\npropose three content selectors to dynamically select and retain information in\ndocuments generated by knowledge cards, specifically controlling for relevance,\nbrevity, and factuality of outputs. Finally, we propose two complementary\nintegration approaches to augment the base LLM with the (relevant, factual)\nknowledge curated from the specialized LMs. Through extensive experiments, we\ndemonstrate that Knowledge Card achieves state-of-the-art performance on six\nbenchmark datasets. Ultimately, Knowledge Card framework enables dynamic\nsynthesis and updates of knowledge from diverse domains. Its modularity will\nensure that relevant knowledge can be continuously updated through the\ncollective efforts of the research community.",
        "score": -2.3864829540252686
      },
      {
        "title": "Knowledge Rumination for Pre-trained Language Models,",
        "abstract": "Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.",
        "score": -2.5822761058807373
      }
    ]
  },
  {
    "title": "Large Language Models are Superpositions of All Characters: Attaining\n  Arbitrary Role-play via Self-Alignment",
    "abstract": "  Considerable efforts have been invested in augmenting the role-playing\nproficiency of open-source large language models (LLMs) by emulating\nproprietary counterparts. Nevertheless, we posit that LLMs inherently harbor\nrole-play capabilities, owing to the extensive knowledge of characters and\npotential dialogues ingrained in their vast training corpora. Thus, in this\nstudy, we introduce Ditto, a self-alignment method for role-play. Ditto\ncapitalizes on character knowledge, encouraging an instruction-following LLM to\nsimulate role-play dialogues as a variant of reading comprehension. This method\ncreates a role-play training set comprising 4,000 characters, surpassing the\nscale of currently available datasets by tenfold regarding the number of roles.\nSubsequently, we fine-tune the LLM using this self-generated dataset to augment\nits role-playing capabilities. Upon evaluating our meticulously constructed and\nreproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in\nvarious parameter scales, consistently maintains a consistent role identity and\nprovides accurate role-specific knowledge in multi-turn role-play\nconversations. Notably, it outperforms all open-source role-play baselines,\nshowcasing performance levels comparable to advanced proprietary chatbots.\nFurthermore, we present the first comprehensive cross-supervision alignment\nexperiment in the role-play domain, revealing that the intrinsic capabilities\nof LLMs confine the knowledge within role-play. Meanwhile, the role-play styles\ncan be easily acquired with the guidance of smaller models. We open-source\nrelated resources at https://github.com/OFA-Sys/Ditto.\n",
    "related_paper_titles": [
      "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities\n  of Large Language Models",
      "ChatHaruhi: Reviving Anime Character in Reality via Large Language Model",
      "A Survey of Large Language Models"
    ],
    "related_paper_abstract": [
      "  The advent of Large Language Models (LLMs) has paved the way for complex\ntasks such as role-playing, which enhances user interactions by enabling models\nto imitate various characters. However, the closed-source nature of\nstate-of-the-art LLMs and their general-purpose training limit role-playing\noptimization. In this paper, we introduce RoleLLM, a framework to benchmark,\nelicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four\nstages: (1) Role Profile Construction for 100 roles; (2) Context-Based\nInstruction Generation (Context-Instruct) for role-specific knowledge\nextraction; (3) Role Prompting using GPT (RoleGPT) for speaking style\nimitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning\nopen-source models along with role customization. By Context-Instruct and\nRoleGPT, we create RoleBench, the first systematic and fine-grained\ncharacter-level benchmark dataset for role-playing with 168,093 samples.\nMoreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),\nsignificantly enhancing role-playing abilities and even achieving comparable\nresults with RoleGPT (using GPT-4).\n",
      "  Role-playing chatbots built on large language models have drawn interest, but\nbetter techniques are needed to enable mimicking specific fictional characters.\nWe propose an algorithm that controls language models via an improved prompt\nand memories of the character extracted from scripts. We construct ChatHaruhi,\na dataset covering 32 Chinese / English TV / anime characters with over 54k\nsimulated dialogues. Both automatic and human evaluations show our approach\nimproves role-playing ability over baselines. Code and data are available at\nhttps://github.com/LC1332/Chat-Haruhi-Suzumiya .\n",
      "  Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n"
    ],
    "entities": [
      "Large Language Models Large Language Models",
      "LLMs",
      "LLM",
      "Large",
      "Large Language Models Large",
      "RAG",
      "Retrieval Augmented",
      "VAE",
      "ESG",
      "KGC",
      "Global",
      "Language Models",
      "CAD",
      "Adam",
      "Large Language Models",
      "HumanEval",
      "DMs",
      "DTs",
      "KV",
      "KGE",
      "SQL",
      "SVM",
      "Verilog",
      "Large Language",
      "ToM",
      "Copilot",
      "Gemini",
      "Korean",
      "Monte Carlo Tree Search",
      "WSIs"
    ],
    "retrieved_papers": [
      {
        "title": "Large Language Models are Superpositions of All Characters: Attaining\n  Arbitrary Role-play via Self-Alignment,",
        "abstract": "Considerable efforts have been invested in augmenting the role-playing\nproficiency of open-source large language models (LLMs) by emulating\nproprietary counterparts. Nevertheless, we posit that LLMs inherently harbor\nrole-play capabilities, owing to the extensive knowledge of characters and\npotential dialogues ingrained in their vast training corpora. Thus, in this\nstudy, we introduce Ditto, a self-alignment method for role-play. Ditto\ncapitalizes on character knowledge, encouraging an instruction-following LLM to\nsimulate role-play dialogues as a variant of reading comprehension. This method\ncreates a role-play training set comprising 4,000 characters, surpassing the\nscale of currently available datasets by tenfold regarding the number of roles.\nSubsequently, we fine-tune the LLM using this self-generated dataset to augment\nits role-playing capabilities. Upon evaluating our meticulously constructed and\nreproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in\nvarious parameter scales, consistently maintains a consistent role identity and\nprovides accurate role-specific knowledge in multi-turn role-play\nconversations. Notably, it outperforms all open-source role-play baselines,\nshowcasing performance levels comparable to advanced proprietary chatbots.\nFurthermore, we present the first comprehensive cross-supervision alignment\nexperiment in the role-play domain, revealing that the intrinsic capabilities\nof LLMs confine the knowledge within role-play. Meanwhile, the role-play styles\ncan be easily acquired with the guidance of smaller models. We open-source\nrelated resources at https://github.com/OFA-Sys/Ditto.",
        "score": 7.019327163696289
      },
      {
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\n  Models,",
        "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents. Codes\nare available at https://github.com/uclaml/SPIN.",
        "score": 2.2225534915924072
      },
      {
        "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models,",
        "abstract": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
        "score": 2.166414260864258
      },
      {
        "title": "Evaluating Character Understanding of Large Language Models via\n  Character Profiling from Fictional Works,",
        "abstract": "Large language models (LLMs) have demonstrated impressive performance and\nspurred numerous AI applications, in which role-playing agents (RPAs) are\nparticularly popular, especially for fictional characters. The prerequisite for\nthese RPAs lies in the capability of LLMs to understand characters from\nfictional works. Previous efforts have evaluated this capability via basic\nclassification tasks or characteristic imitation, failing to capture the\nnuanced character understanding with LLMs. In this paper, we propose evaluating\nLLMs' character understanding capability via the character profiling task,\ni.e., summarizing character profiles from corresponding materials, a widely\nadopted yet understudied practice for RPA development. Specifically, we\nconstruct the CroSS dataset from literature experts and assess the generated\nprofiles by comparing ground truth references and their applicability in\ndownstream tasks. Our experiments, which cover various summarization methods\nand LLMs, have yielded promising results. These results strongly validate the\ncharacter understanding capability of LLMs. Resources are available at\nhttps://github.com/Joanna0123/character_profiling.",
        "score": 1.8418629169464111
      },
      {
        "title": "Large Language Models Meet Harry Potter: A Bilingual Dataset for\n  Aligning Dialogue Agents with Characters,",
        "abstract": "In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT\nand GPT4 have demonstrated immense potential in constructing open-domain\ndialogue agents. However, aligning these agents with specific characters or\nindividuals remains a considerable challenge due to the complexities of\ncharacter representation and the lack of comprehensive annotations. In this\npaper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to\nadvance the study of dialogue agents and character alignment. The dataset\nencompasses all dialogue sessions (in both English and Chinese) from the Harry\nPotter series and is annotated with vital background information, including\ndialogue scenes, speakers, character relationships, and attributes. These\nextensive annotations may empower LLMs to unlock character-driven dialogue\ncapabilities. Furthermore, it can serve as a universal benchmark for evaluating\nhow well can a LLM aligning with a specific character. We benchmark LLMs on HPD\nusing both fine-tuning and in-context learning settings. Evaluation results\nreveal that although there is substantial room for improvement in generating\nhigh-quality, character-aligned responses, the proposed dataset is valuable in\nguiding models toward responses that better align with the character of Harry\nPotter.",
        "score": 1.568919062614441
      },
      {
        "title": "RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language\n  Models,",
        "abstract": "This study presents RoleCraft-GLM, an innovative framework aimed at enhancing\npersonalized role-playing with Large Language Models (LLMs). RoleCraft-GLM\naddresses the key issue of lacking personalized interactions in conversational\nAI, and offers a solution with detailed and emotionally nuanced character\nportrayals. We contribute a unique conversational dataset that shifts from\nconventional celebrity-centric characters to diverse, non-celebrity personas,\nthus enhancing the realism and complexity of language modeling interactions.\nAdditionally, our approach includes meticulous character development, ensuring\ndialogues are both realistic and emotionally resonant. The effectiveness of\nRoleCraft-GLM is validated through various case studies, highlighting its\nversatility and skill in different scenarios. Our framework excels in\ngenerating dialogues that accurately reflect characters' personality traits and\nemotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks\na significant leap in personalized AI interactions, and paves the way for more\nauthentic and immersive AI-assisted role-playing experiences by enabling more\nnuanced and emotionally rich dialogues",
        "score": 1.438317060470581
      },
      {
        "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities\n  of Large Language Models,",
        "abstract": "The advent of Large Language Models (LLMs) has paved the way for complex\ntasks such as role-playing, which enhances user interactions by enabling models\nto imitate various characters. However, the closed-source nature of\nstate-of-the-art LLMs and their general-purpose training limit role-playing\noptimization. In this paper, we introduce RoleLLM, a framework to benchmark,\nelicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four\nstages: (1) Role Profile Construction for 100 roles; (2) Context-Based\nInstruction Generation (Context-Instruct) for role-specific knowledge\nextraction; (3) Role Prompting using GPT (RoleGPT) for speaking style\nimitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning\nopen-source models along with role customization. By Context-Instruct and\nRoleGPT, we create RoleBench, the first systematic and fine-grained\ncharacter-level benchmark dataset for role-playing with 168,093 samples.\nMoreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),\nsignificantly enhancing role-playing abilities and even achieving comparable\nresults with RoleGPT (using GPT-4).",
        "score": 1.424002766609192
      },
      {
        "title": "Aligning Large Language Models with Human: A Survey,",
        "abstract": "Large Language Models (LLMs) trained on extensive textual corpora have\nemerged as leading solutions for a broad array of Natural Language Processing\n(NLP) tasks. Despite their notable performance, these models are prone to\ncertain limitations such as misunderstanding human instructions, generating\npotentially biased content, or factually incorrect (hallucinated) information.\nHence, aligning LLMs with human expectations has become an active area of\ninterest within the research community. This survey presents a comprehensive\noverview of these alignment technologies, including the following aspects. (1)\nData collection: the methods for effectively collecting high-quality\ninstructions for LLM alignment, including the use of NLP benchmarks, human\nannotations, and leveraging strong LLMs. (2) Training methodologies: a detailed\nreview of the prevailing training methods employed for LLM alignment. Our\nexploration encompasses Supervised Fine-tuning, both Online and Offline human\npreference training, along with parameter-efficient training mechanisms. (3)\nModel Evaluation: the methods for evaluating the effectiveness of these\nhuman-aligned LLMs, presenting a multifaceted approach towards their\nassessment. In conclusion, we collate and distill our findings, shedding light\non several promising future research avenues in the field. This survey,\ntherefore, serves as a valuable resource for anyone invested in understanding\nand advancing the alignment of LLMs to better suit human-oriented tasks and\nexpectations. An associated GitHub link collecting the latest papers is\navailable at https://github.com/GaryYufei/AlignLLMHumanSurvey.",
        "score": 1.3629335165023804
      },
      {
        "title": "RoleEval: A Bilingual Role Evaluation Benchmark for Large Language\n  Models,",
        "abstract": "The rapid evolution of large language models necessitates effective\nbenchmarks for evaluating their role knowledge, which is essential for\nestablishing connections with the real world and providing more immersive\ninteractions. This paper introduces RoleEval, a bilingual benchmark designed to\nassess the memorization, utilization, and reasoning capabilities of role\nknowledge. RoleEval comprises RoleEval-Global (including internationally\nrecognized characters) and RoleEval-Chinese (including characters popular in\nChina), with 6,000 Chinese-English parallel multiple-choice questions focusing\non 300 influential people and fictional characters drawn from a variety of\ndomains including celebrities, anime, comics, movies, TV series, games, and\nfictions. These questions cover basic knowledge and multi-hop reasoning\nabilities, aiming to systematically probe various aspects such as personal\ninformation, relationships, abilities, and experiences of the characters. To\nmaintain high standards, we perform a hybrid quality check process combining\nboth automatic and human verification, ensuring that the questions are diverse,\nchallenging, and discriminative.\n  Our extensive evaluations with RoleEval across various open-source and\nproprietary large language models, under both the zero- and few-shot settings,\nreveal insightful findings. Notably, while GPT-4 outperforms other models on\nRoleEval-Global, Chinese large language models excel on RoleEval-Chinese,\nhighlighting significant knowledge distribution differences. We expect that\nRoleEval would highlight the significance of assessing role knowledge for large\nlanguage models across various languages and cultural settings.",
        "score": 1.2830359935760498
      },
      {
        "title": "A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO,\n  DPO and More,",
        "abstract": "With advancements in self-supervised learning, the availability of trillions\ntokens in a pre-training corpus, instruction fine-tuning, and the development\nof large Transformers with billions of parameters, large language models (LLMs)\nare now capable of generating factual and coherent responses to human queries.\nHowever, the mixed quality of training data can lead to the generation of\nundesired responses, presenting a significant challenge. Over the past two\nyears, various methods have been proposed from different perspectives to\nenhance LLMs, particularly in aligning them with human expectation. Despite\nthese efforts, there has not been a comprehensive survey paper that categorizes\nand details these approaches. In this work, we aim to address this gap by\ncategorizing these papers into distinct topics and providing detailed\nexplanations of each alignment method, thereby helping readers gain a thorough\nunderstanding of the current state of the field.",
        "score": 1.2727283239364624
      },
      {
        "title": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent\n  Evaluation,",
        "abstract": "Recently, the advent of large language models (LLMs) has revolutionized\ngenerative agents. Among them, Role-Playing Conversational Agents (RPCAs)\nattract considerable attention due to their ability to emotionally engage\nusers. However, the absence of a comprehensive benchmark impedes progress in\nthis field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark\nfor comprehensive RPCA assessment, complemented by a tailored high-quality\ndataset. The dataset comprises 1,785 multi-turn role-playing dialogues,\nencompassing 23,020 examples and featuring 77 characters derived from Chinese\nnovels and scripts. It was carefully constructed, beginning with initial\ndialogue extraction via GPT-4, followed by rigorous human-led quality control,\nand enhanced with in-depth character profiles sourced from Baidu Baike.\nCharacterEval employs a multifaceted evaluation approach, encompassing thirteen\ntargeted metrics on four dimensions. Comprehensive experiments on CharacterEval\ndemonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in\nChinese role-playing conversation. Source code, data source and reward model\nwill be publicly accessible at https://github.com/morecry/CharacterEval.",
        "score": 1.2258561849594116
      },
      {
        "title": "Enhancing Role-playing Systems through Aggressive Queries: Evaluation\n  and Improvement,",
        "abstract": "The advent of Large Language Models (LLMs) has propelled dialogue generation\ninto new realms, particularly in the field of role-playing systems (RPSs).\nWhile enhanced with ordinary role-relevant training dialogues, existing\nLLM-based RPSs still struggle to align with roles when handling intricate and\ntrapped queries in boundary scenarios. In this paper, we design the Modular\nORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve\nthe role-playing LLMs' performance. MORTISE can produce highly role-relevant\naggressive queries through the collaborative effort of multiple LLM-based\nmodules, and formulate corresponding responses to create an adversarial\ntraining dataset via a consistent response generator. We select 190 Chinese and\nEnglish roles to construct aggressive queries to benchmark existing\nrole-playing LLMs. Through comprehensive evaluation, we find that existing\nmodels exhibit a general deficiency in role alignment capabilities. We further\nselect 180 of the roles to collect an adversarial training dataset (named\nRoleAD) and retain the other 10 roles for testing. Experiments on models\nimproved by RoleAD indicate that our adversarial dataset ameliorates this\ndeficiency, with the improvements demonstrating a degree of generalizability in\nordinary scenarios.",
        "score": 1.2144389152526855
      },
      {
        "title": "Better Zero-Shot Reasoning with Role-Play Prompting,",
        "abstract": "Modern large language models (LLMs) exhibit a remarkable capacity for\nrole-playing, enabling them to embody not only human characters but also\nnon-human entities. This versatility allows them to simulate complex human-like\ninteractions and behaviors within various contexts, as well as to emulate\nspecific objects or systems. While these capabilities have enhanced user\nengagement and introduced novel modes of interaction, the influence of\nrole-playing on LLMs' reasoning abilities remains underexplored. In this study,\nwe introduce a strategically designed role-play prompting methodology and\nassess its performance under the zero-shot setting across twelve diverse\nreasoning benchmarks. Our empirical results illustrate that role-play prompting\nconsistently surpasses the standard zero-shot approach across most datasets.\nNotably, in experiments conducted using ChatGPT, accuracy on AQuA rises from\n53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%.Upon further comparison\nwith the Zero-Shot-CoT technique, which prompts the model to \"think step by\nstep\", our study demonstrates that role-play prompting acts as a more effective\ntrigger for the CoT process. This highlights its potential to augment the\nreasoning capabilities of LLMs. We release our code at\nhttps://github.com/NKU-HLT/Role-Play-Prompting.",
        "score": 1.1330058574676514
      },
      {
        "title": "Insights into Alignment: Evaluating DPO and its Variants Across Multiple\n  Tasks,",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na spectrum of tasks. Recently, Direct Preference Optimization (DPO) has emerged\nas an RL-free approach to optimize the policy model on human preferences.\nHowever, several limitations hinder the widespread adoption of this method. To\naddress these shortcomings, various versions of DPO have been introduced. Yet,\na comprehensive evaluation of these variants across diverse tasks is still\nlacking. In this study, we aim to bridge this gap by investigating the\nperformance of alignment methods across three distinct scenarios: (1) keeping\nthe Supervised Fine-Tuning (SFT) part, (2) skipping the SFT part, and (3)\nskipping the SFT part and utilizing an instruction-tuned model. Furthermore, we\nexplore the impact of different training sizes on their performance. Our\nevaluation spans a range of tasks including dialogue systems, reasoning,\nmathematical problem-solving, question answering, truthfulness, and multi-task\nunderstanding, encompassing 13 benchmarks such as MT-Bench, Big Bench, and Open\nLLM Leaderboard. Key observations reveal that alignment methods achieve optimal\nperformance with smaller training data subsets, exhibit limited effectiveness\nin reasoning tasks yet significantly impact mathematical problem-solving, and\nemploying an instruction-tuned model notably influences truthfulness. We\nanticipate that our findings will catalyze further research aimed at developing\nmore robust models to address alignment challenges.",
        "score": 1.075199842453003
      },
      {
        "title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and\n  Personalization,",
        "abstract": "The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey",
        "score": 0.8576440811157227
      },
      {
        "title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models\n  with Personality-Indicative Data,",
        "abstract": "Role-playing agents (RPA) have been a popular application area for large\nlanguage models (LLMs), attracting significant interest from both industry and\nacademia.While existing RPAs well portray the characters' knowledge and tones,\nthey face challenges in capturing their minds, especially for small\nrole-playing language models (RPLMs). In this paper, we propose to enhance\nRPLMs via personality-indicative data. Specifically, we leverage questions from\npsychological scales and distill advanced RPAs to generate dialogues that grasp\nthe minds of characters. Experimental results validate that RPLMs trained with\nour dataset exhibit advanced role-playing capabilities for both general and\npersonality-related evaluations. Code and data are available at\n\\href{https://github.com/alienet1109/RolePersonality}{this URL}.",
        "score": 0.8540593385696411
      },
      {
        "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character\n  Role-Playing Agent,",
        "abstract": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents\nbut encounter challenges in multi-character role-playing (MCRP) scenarios. To\naddress the issue, we present Neeko, an innovative framework designed for\nefficient multiple characters imitation. Unlike existing methods, Neeko employs\na dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to\ndiverse characters. Our framework breaks down the role-playing process into\nagent pre-training, multiple characters playing, and character incremental\nlearning, effectively handling both seen and unseen roles. This dynamic\napproach, coupled with distinct LoRA blocks for each character, enhances\nNeeko's adaptability to unique attributes, personalities, and speaking\npatterns. As a result, Neeko demonstrates superior performance in MCRP over\nmost existing methods, offering more engaging and versatile user interaction\nexperiences. Code and data are available at\nhttps://github.com/weiyifan1023/Neeko.",
        "score": 0.4955909848213196
      },
      {
        "title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues,",
        "abstract": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}.",
        "score": 0.3956254720687866
      },
      {
        "title": "YuLan: An Open-source Large Language Model,",
        "abstract": "Large language models (LLMs) have become the foundation of many applications,\nleveraging their extensive capabilities in processing and understanding natural\nlanguage. While many open-source LLMs have been released with technical\nreports, the lack of training details hinders further research and development.\nThis paper presents the development of YuLan, a series of open-source LLMs with\n$12$ billion parameters. The base model of YuLan is pre-trained on\napproximately $1.7$T tokens derived from a diverse corpus, including massive\nEnglish, Chinese, and multilingual texts. We design a three-stage pre-training\nmethod to enhance YuLan's overall capabilities. Subsequent phases of training\nincorporate instruction-tuning and human alignment, employing a substantial\nvolume of high-quality synthesized data. To facilitate the learning of complex\nand long-tail knowledge, we devise a curriculum-learning framework throughout\nacross these stages, which helps LLMs learn knowledge in an easy-to-hard\nmanner. YuLan's training is finished on Jan, 2024 and has achieved performance\non par with state-of-the-art LLMs across various English and Chinese\nbenchmarks. This paper outlines a comprehensive technical roadmap for\ndeveloping LLMs from scratch. Our model and codes are available at\nhttps://github.com/RUC-GSAI/YuLan-Chat.",
        "score": 0.23618671298027039
      },
      {
        "title": "SimsChat: A Customisable Persona-Driven Role-Playing Agent,",
        "abstract": "Large Language Models (LLMs) possess the remarkable capability to understand\nhuman instructions and generate high-quality text, enabling them to act as\nagents that simulate human behaviours. This capability allows LLMs to emulate\nhuman beings in a more advanced manner, beyond merely replicating simple human\nbehaviours. However, there is a lack of exploring into leveraging LLMs to craft\ncharacters from several aspects. In this work, we introduce the Customisable\nConversation Agent Framework, which employs LLMs to simulate real-world\ncharacters that can be freely customised according to different user\npreferences. The customisable framework is helpful for designing customisable\ncharacters and role-playing agents according to human's preferences. We first\npropose the SimsConv dataset, which comprises 68 different customised\ncharacters, 1,360 multi-turn role-playing dialogues, and encompasses 13,971\ninteraction dialogues in total. The characters are created from several\nreal-world elements, such as career, aspiration, trait, and skill. Building on\nthese foundations, we present SimsChat, a freely customisable role-playing\nagent. It incorporates different real-world scenes and topic-specific character\ninteraction dialogues, simulating characters' life experiences in various\nscenarios and topic-specific interactions with specific emotions. Experimental\nresults show that our proposed framework achieves desirable performance and\nprovides helpful guideline for building better simulacra of human beings in the\nfuture. Our data and code are available at\nhttps://github.com/Bernard-Yang/SimsChat.",
        "score": 0.224249467253685
      },
      {
        "title": "PIPPA: A Partially Synthetic Conversational Dataset,",
        "abstract": "With the emergence of increasingly powerful large language models, there is a\nburgeoning interest in leveraging these models for casual conversation and\nrole-play applications. However, existing conversational and role-playing\ndatasets often fail to capture the diverse and nuanced interactions typically\nexhibited by real-world role-play participants. To address this limitation and\ncontribute to the rapidly growing field, we introduce a partially-synthetic\ndataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA\nis a result of a community-driven crowdsourcing effort involving a group of\nrole-play enthusiasts. The dataset comprises over 1 million utterances that are\ndistributed across 26,000 conversation sessions and provides a rich resource\nfor researchers and AI developers to explore and refine conversational AI\nsystems in the context of role-play scenarios.",
        "score": -0.031592175364494324
      },
      {
        "title": "Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain\n  Human-Machine Conversation,",
        "abstract": "Recently, various methods have been proposed to create open-domain\nconversational agents with Large Language Models (LLMs). These models are able\nto answer user queries, but in a one-way Q&A format rather than a true\nconversation. Fine-tuning on particular datasets is the usual way to modify\ntheir style to increase conversational ability, but this is expensive and\nusually only available in a few languages. In this study, we explore role-play\nzero-shot prompting as an efficient and cost-effective solution for open-domain\nconversation, using capable multilingual LLMs (Beeching et al., 2023) trained\nto obey instructions. We design a prompting system that, when combined with an\ninstruction-following model - here Vicuna (Chiang et al., 2023) - produces\nconversational agents that match and even surpass fine-tuned models in human\nevaluation in French in two different tasks.",
        "score": -0.19218628108501434
      },
      {
        "title": "RoleInteract: Evaluating the Social Interaction of Role-Playing Agents,",
        "abstract": "Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce RoleInteract,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nRoleInteract confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/RoleInteract.",
        "score": -0.34027308225631714
      },
      {
        "title": "Prompt Framework for Role-playing: Generation and Evaluation,",
        "abstract": "Large language models (LLM) have demonstrated remarkable abilities in\ngenerating natural language, understanding user instruction, and mimicking\nhuman language use. These capabilities have garnered considerable interest in\napplications such as role-playing. However, the process of collecting\nindividual role scripts (or profiles) data and manually evaluating the\nperformance can be costly. We introduce a framework that uses prompts to\nleverage the state-of-the-art (SOTA) LLMs to construct role-playing dialogue\ndatasets and evaluate the role-playing performance. Additionally, we employ\nrecall-oriented evaluation Rouge-L metric to support the result of the LLM\nevaluator.",
        "score": -0.452052503824234
      },
      {
        "title": "Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs,",
        "abstract": "Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.",
        "score": -0.5460659861564636
      },
      {
        "title": "Zero-shot Cross-lingual Conversational Semantic Role Labeling,",
        "abstract": "While conversational semantic role labeling (CSRL) has shown its usefulness\non Chinese conversational tasks, it is still under-explored in non-Chinese\nlanguages due to the lack of multilingual CSRL annotations for the parser\ntraining. To avoid expensive data collection and error-propagation of\ntranslation-based methods, we present a simple but effective approach to\nperform zero-shot cross-lingual CSRL. Our model implicitly learns\nlanguage-agnostic, conversational structure-aware and semantically rich\nrepresentations with the hierarchical encoders and elaborately designed\npre-training objectives. Experimental results show that our model outperforms\nall baselines by large margins on two newly collected English CSRL test sets.\nMore importantly, we confirm the usefulness of CSRL to non-Chinese\nconversational tasks such as the question-in-context rewriting task in English\nand the multi-turn dialogue response generation tasks in English, German and\nJapanese by incorporating the CSRL information into the downstream\nconversation-based models. We believe this finding is significant and will\nfacilitate the research of non-Chinese dialogue tasks which suffer the problems\nof ellipsis and anaphora.",
        "score": -0.8641740083694458
      },
      {
        "title": "\"In Dialogues We Learn\": Towards Personalized Dialogue Without\n  Pre-defined Profiles through In-Dialogue Learning,",
        "abstract": "Personalized dialogue systems have gained significant attention in recent\nyears for their ability to generate responses in alignment with different\npersonas. However, most existing approaches rely on pre-defined personal\nprofiles, which are not only time-consuming and labor-intensive to create but\nalso lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning\nframework that enhances the ability of pre-trained large language models to\nleverage dialogue history to characterize persona for completing personalized\ndialogue generation tasks without pre-defined profiles. Our experiments on\nthree datasets demonstrate that IDL brings substantial improvements, with BLEU\nand ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,\nthe results of human evaluations further validate the efficacy of our proposed\nmethod.",
        "score": -1.0543556213378906
      },
      {
        "title": "Building a Role Specified Open-Domain Dialogue System Leveraging\n  Large-Scale Language Models,",
        "abstract": "Recent open-domain dialogue models have brought numerous breakthroughs.\nHowever, building a chat system is not scalable since it often requires a\nconsiderable volume of human-human dialogue data, especially when enforcing\nfeatures such as persona, style, or safety. In this work, we study the\nchallenge of imposing roles on open-domain dialogue systems, with the goal of\nmaking the systems maintain consistent roles while conversing naturally with\nhumans. To accomplish this, the system must satisfy a role specification that\nincludes certain conditions on the stated features as well as a system policy\non whether or not certain types of utterances are allowed. For this, we propose\nan efficient data collection framework leveraging in-context few-shot learning\nof large-scale language models for building role-satisfying dialogue dataset\nfrom scratch. We then compare various architectures for open-domain dialogue\nsystems in terms of meeting role specifications while maintaining\nconversational abilities. Automatic and human evaluations show that our models\nreturn few out-of-bounds utterances, keeping competitive performance on general\nmetrics. We release a Korean dialogue dataset we built for further research.",
        "score": -1.2866367101669312
      },
      {
        "title": "MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring\n  and Utilizing Latent Space,",
        "abstract": "Personalized Dialogue Generation (PDG) aims to create coherent responses\naccording to roles or personas. Traditional PDG relies on external role data,\nwhich can be scarce and raise privacy concerns. Approaches address these issues\nby extracting role information from dialogue history, which often fail to\ngenerically model roles in continuous space. To overcome these limitations, we\nintroduce a novel framework \\textbf{MO}dels \\textbf{R}oles from\n\\textbf{P}ersonalized Dialogue \\textbf{H}istory by \\textbf{E}xploring and\n\\textbf{U}tilizing Latent \\textbf{S}pace (MORPHEUS) through a three-stage\ntraining process. Specifically, we create a persona codebook to represent roles\nin latent space compactly, and this codebook is used to construct a posterior\ndistribution of role information. This method enables the model to generalize\nacross roles, allowing the generation of personalized dialogues even for unseen\nroles. Experiments on both Chinese and English datasets demonstrate that\nMORPHEUS enhances the extraction of role information, and improves response\ngeneration without external role data. Additionally, MORPHEUS can be considered\nan efficient fine-tuning for large language models.",
        "score": -1.741904377937317
      },
      {
        "title": "DeepStruct: Pretraining of Language Models for Structure Prediction,",
        "abstract": "We introduce a method for improving the structural understanding abilities of\nlanguage models. Unlike previous approaches that finetune the models with\ntask-specific augmentation, we pretrain language models on a collection of\ntask-agnostic corpora to generate structures from text. Our structure\npretraining enables zero-shot transfer of the learned knowledge that models\nhave about the structure tasks. We study the performance of this approach on 28\ndatasets, spanning 10 structure prediction tasks including open information\nextraction, joint entity and relation extraction, named entity recognition,\nrelation classification, semantic role labeling, event extraction, coreference\nresolution, factual probe, intent detection, and dialogue state tracking. We\nfurther enhance the pretraining with the task-specific training sets. We show\nthat a 10B parameter language model transfers non-trivially to most tasks and\nobtains state-of-the-art performance on 21 of 28 datasets that we evaluate.",
        "score": -1.9697495698928833
      }
    ]
  },
  {
    "title": "Advancing Large Language Models to Capture Varied Speaking Styles and\n  Respond Properly in Spoken Conversations",
    "abstract": "  In spoken dialogue, even if two current turns are the same sentence, their\nresponses might still differ when they are spoken in different styles. The\nspoken styles, containing paralinguistic and prosodic information, mark the\nmost significant difference between text and speech modality. When using\ntext-only LLMs to model spoken dialogue, text-only LLMs cannot give different\nresponses based on the speaking style of the current turn. In this paper, we\nfocus on enabling LLMs to listen to the speaking styles and respond properly.\nOur goal is to teach the LLM that \"even if the sentences are identical if they\nare spoken in different styles, their corresponding responses might be\ndifferent\". Since there is no suitable dataset for achieving this goal, we\ncollect a speech-to-speech dataset, StyleTalk, with the following desired\ncharacteristics: when two current speeches have the same content but are spoken\nin different styles, their responses will be different. To teach LLMs to\nunderstand and respond properly to the speaking styles, we propose the\nSpoken-LLM framework that can model the linguistic content and the speaking\nstyles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage\ntraining pipeline to help the Spoken-LLM better learn the speaking styles.\nBased on extensive experiments, we show that Spoken-LLM outperforms text-only\nbaselines and prior speech LLMs methods.\n",
    "related_paper_titles": [
      "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue",
      "E-chat: Emotion-sensitive Spoken Dialogue System with Large Language\n  Models",
      "Spoken Question Answering and Speech Continuation Using\n  Spectrogram-Powered LLM"
    ],
    "related_paper_abstract": [
      "  Large Language Models (LLMs) have demonstrated superior abilities in tasks\nsuch as chatting, reasoning, and question-answering. However, standard LLMs may\nignore crucial paralinguistic information, such as sentiment, emotion, and\nspeaking style, which are essential for achieving natural, human-like spoken\nconversation, especially when such information is conveyed by acoustic cues. We\ntherefore propose Paralinguistics-enhanced Generative Pretrained Transformer\n(ParalinGPT), an LLM that utilizes text and speech modalities to better model\nthe linguistic content and paralinguistic attributes of spoken dialogue. The\nmodel takes the conversational context of text, speech embeddings, and\nparalinguistic attributes as input prompts within a serialized multitasking\nmultimodal framework. Specifically, our framework serializes tasks in the order\nof current paralinguistic attribute prediction, response paralinguistic\nattribute prediction, and response text generation with autoregressive\nconditioning. We utilize the Switchboard-1 corpus, including its sentiment\nlabels as the paralinguistic attribute, as our spoken dialogue dataset.\nExperimental results indicate the proposed serialized multitasking method\noutperforms typical sequence classification techniques on current and response\nsentiment classification. Furthermore, leveraging conversational context and\nspeech embeddings significantly improves both response text generation and\nsentiment prediction. Our proposed framework achieves relative improvements of\n6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment\naccuracy, and response text BLEU score, respectively.\n",
      "  This study focuses on emotion-sensitive spoken dialogue in human-machine\nspeech interaction. With the advancement of Large Language Models (LLMs),\ndialogue systems can handle multimodal data, including audio. Recent models\nhave enhanced the understanding of complex audio signals through the\nintegration of various audio events. However, they are unable to generate\nappropriate responses based on emotional speech. To address this, we introduce\nthe Emotional chat Model (E-chat), a novel spoken dialogue system capable of\ncomprehending and responding to emotions conveyed from speech. This model\nleverages an emotion embedding extracted by a speech encoder, combined with\nLLMs, enabling it to respond according to different emotional contexts.\nAdditionally, we introduce the E-chat200 dataset, designed explicitly for\nemotion-sensitive spoken dialogue. In various evaluation metrics, E-chat\nconsistently outperforms baseline model, demonstrating its potential in\nemotional comprehension and human-machine interaction.\n",
      "  We present Spectron, a novel approach to adapting pre-trained large language\nmodels (LLMs) to perform spoken question answering (QA) and speech\ncontinuation. By endowing the LLM with a pre-trained speech encoder, our model\nbecomes able to take speech inputs and generate speech outputs. The entire\nsystem is trained end-to-end and operates directly on spectrograms, simplifying\nour architecture. Key to our approach is a training objective that jointly\nsupervises speech recognition, text continuation, and speech synthesis using\nonly paired speech-text pairs, enabling a `cross-modal' chain-of-thought within\na single decoding pass. Our method surpasses existing spoken language models in\nspeaker preservation and semantic coherence. Furthermore, the proposed model\nimproves upon direct initialization in retaining the knowledge of the original\nLLM as demonstrated through spoken QA datasets. We release our audio samples\n(https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset\n(https://github.com/google-research-datasets/LLAMA1-Test-Set).\n"
    ],
    "entities": [
      "LLMs",
      "Large Language Models",
      "LLM",
      "RAG",
      "Large Language Models Large Language Models",
      "Large",
      "CAD",
      "Large Language",
      "Adam",
      "Mistral",
      "Persian",
      "SFT",
      "DTs",
      "LLaMA",
      "SVM",
      "ICL",
      "ChatGPT",
      "ANN",
      "DPO",
      "CoT",
      "WSIs",
      "Language Models",
      "LiDAR",
      "Llama",
      "GPT",
      "Retrieval Augmented",
      "ESG",
      "RLHF",
      "OpenAI",
      "Large Language Model"
    ],
    "retrieved_papers": [
      {
        "title": "Advancing Large Language Models to Capture Varied Speaking Styles and\n  Respond Properly in Spoken Conversations,",
        "abstract": "In spoken dialogue, even if two current turns are the same sentence, their\nresponses might still differ when they are spoken in different styles. The\nspoken styles, containing paralinguistic and prosodic information, mark the\nmost significant difference between text and speech modality. When using\ntext-only LLMs to model spoken dialogue, text-only LLMs cannot give different\nresponses based on the speaking style of the current turn. In this paper, we\nfocus on enabling LLMs to listen to the speaking styles and respond properly.\nOur goal is to teach the LLM that \"even if the sentences are identical if they\nare spoken in different styles, their corresponding responses might be\ndifferent\". Since there is no suitable dataset for achieving this goal, we\ncollect a speech-to-speech dataset, StyleTalk, with the following desired\ncharacteristics: when two current speeches have the same content but are spoken\nin different styles, their responses will be different. To teach LLMs to\nunderstand and respond properly to the speaking styles, we propose the\nSpoken-LLM framework that can model the linguistic content and the speaking\nstyles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage\ntraining pipeline to help the Spoken-LLM better learn the speaking styles.\nBased on extensive experiments, we show that Spoken-LLM outperforms text-only\nbaselines and prior speech LLMs methods.",
        "score": 6.5462727546691895
      },
      {
        "title": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented\n  Dialogue Agents,",
        "abstract": "Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/.",
        "score": 3.0851080417633057
      },
      {
        "title": "Towards Joint Modeling of Dialogue Response and Speech Synthesis based\n  on Large Language Model,",
        "abstract": "This paper explores the potential of constructing an AI spoken dialogue\nsystem that \"thinks how to respond\" and \"thinks how to speak\" simultaneously,\nwhich more closely aligns with the human speech production process compared to\nthe current cascade pipeline of independent chatbot and Text-to-Speech (TTS)\nmodules. We hypothesize that Large Language Models (LLMs) with billions of\nparameters possess significant speech understanding capabilities and can\njointly model dialogue responses and linguistic features. We conduct two sets\nof experiments: 1) Prosodic structure prediction, a typical front-end task in\nTTS, demonstrating the speech understanding ability of LLMs, and 2) Further\nintegrating dialogue response and a wide array of linguistic features using a\nunified encoding format. Our results indicate that the LLM-based approach is a\npromising direction for building unified spoken dialogue systems.",
        "score": 2.671898603439331
      },
      {
        "title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue,",
        "abstract": "Large Language Models (LLMs) have demonstrated superior abilities in tasks\nsuch as chatting, reasoning, and question-answering. However, standard LLMs may\nignore crucial paralinguistic information, such as sentiment, emotion, and\nspeaking style, which are essential for achieving natural, human-like spoken\nconversation, especially when such information is conveyed by acoustic cues. We\ntherefore propose Paralinguistics-enhanced Generative Pretrained Transformer\n(ParalinGPT), an LLM that utilizes text and speech modalities to better model\nthe linguistic content and paralinguistic attributes of spoken dialogue. The\nmodel takes the conversational context of text, speech embeddings, and\nparalinguistic attributes as input prompts within a serialized multitasking\nmultimodal framework. Specifically, our framework serializes tasks in the order\nof current paralinguistic attribute prediction, response paralinguistic\nattribute prediction, and response text generation with autoregressive\nconditioning. We utilize the Switchboard-1 corpus, including its sentiment\nlabels as the paralinguistic attribute, as our spoken dialogue dataset.\nExperimental results indicate the proposed serialized multitasking method\noutperforms typical sequence classification techniques on current and response\nsentiment classification. Furthermore, leveraging conversational context and\nspeech embeddings significantly improves both response text generation and\nsentiment prediction. Our proposed framework achieves relative improvements of\n6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment\naccuracy, and response text BLEU score, respectively.",
        "score": 2.666245460510254
      },
      {
        "title": "BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues,",
        "abstract": "Interacting with human via high-quality multi-turn dialogues is a key feature\nof large language models (LLMs). However, human-based evaluation of such\ncapability involves intensive manual labor. This report provides a preliminary\nevaluation of existing large language models for human-style multi-turn\nchatting, through an LLM-based approach. We start from real-world human\ndialogues and keep the very first utterances as the ChatSEED. Then we prompt\nLLMs to generate a full multi-turn dialogue (tens of utterances) based on the\nChatSEED, utterance by utterance. Finally, we adopt state-of-the-art LLMs\n(GPT-4, \\etc) as the judge to evaluate the generated dialogues. With different\nevaluation protocols, we come to substantially identical conclusions. We find\nthat GPT-4 can generate human-style multi-turn dialogues with impressive\nquality, significantly outperforms its counterparts. It's difficult for a\ndiscriminator to distinguish between GPT-4 generated dialogues and human\ndialogues. In contrast, other LLMs struggle to generate multi-turn dialogues of\nsatisfactory quality due to poor instruction-following capability, tendency to\ngenerate lengthy utterances, or limited general capability. All data and codes\nwill be provided in https://github.com/open-compass/BotChat/ and we hope they\ncan serve as a valuable resource for evaluating multi-turn chatting\ncapabilities of LLMs.",
        "score": 2.3799805641174316
      },
      {
        "title": "LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues,",
        "abstract": "Spurred by recent advances in Large Language Models (LLMs), virtual\nassistants are poised to take a leap forward in terms of their dialogue\ncapabilities. Yet a major bottleneck to achieving genuinely transformative\ntask-oriented dialogue capabilities remains the scarcity of high quality data.\nExisting datasets, while impressive in scale, have limited domain coverage and\ncontain few genuinely challenging conversational phenomena; those which are\npresent are typically unlabelled, making it difficult to assess the strengths\nand weaknesses of models without time-consuming and costly human evaluation.\nMoreover, creating high quality dialogue data has until now required\nconsiderable human input, limiting both the scale of these datasets and the\nability to rapidly bootstrap data for a new target domain. We aim to overcome\nthese issues with LUCID, a modularised and highly automated LLM-driven data\ngeneration system that produces realistic, diverse and challenging dialogues.\nWe use LUCID to generate a seed dataset of 4,277 conversations across 100\nintents to demonstrate its capabilities, with a human review finding\nconsistently high quality labels in the generated data.",
        "score": 2.3430674076080322
      },
      {
        "title": "Speaker Verification in Agent-Generated Conversations,",
        "abstract": "The recent success of large language models (LLMs) has attracted widespread\ninterest to develop role-playing conversational agents personalized to the\ncharacteristics and styles of different speakers to enhance their abilities to\nperform both general and special purpose dialogue tasks. However, the ability\nto personalize the generated utterances to speakers, whether conducted by human\nor LLM, has not been well studied. To bridge this gap, our study introduces a\nnovel evaluation challenge: speaker verification in agent-generated\nconversations, which aimed to verify whether two sets of utterances originate\nfrom the same speaker. To this end, we assemble a large dataset collection\nencompassing thousands of speakers and their utterances. We also develop and\nevaluate speaker verification models under experiment setups. We further\nutilize the speaker verification models to evaluate the personalization\nabilities of LLM-based role-playing models. Comprehensive experiments suggest\nthat the current role-playing models fail in accurately mimicking speakers,\nprimarily due to their inherent linguistic characteristics.",
        "score": 2.3108553886413574
      },
      {
        "title": "Unified Speech-Text Pretraining for Spoken Dialog Modeling,",
        "abstract": "While recent work shows promising results in expanding the capabilities of\nlarge language models (LLM) to directly understand and synthesize speech, an\nLLM-based strategy for modeling spoken dialogs remains elusive and calls for\nfurther investigation. This work proposes an extensive speech-text LLM\nframework, named the Unified Spoken Dialog Model (USDM), to generate coherent\nspoken responses with organic prosodic features relevant to the given input\nspeech without relying on automatic speech recognition (ASR) or text-to-speech\n(TTS) solutions. Our approach employs a multi-step speech-text inference scheme\nthat leverages chain-of-reasoning capabilities exhibited by the underlying LLM.\nWe also propose a generalized speech-text pretraining scheme that helps with\ncapturing cross-modal semantics. Automatic and human evaluations show that the\nproposed approach is effective in generating natural-sounding spoken responses,\noutperforming both prior and cascaded baselines. Detailed comparative studies\nreveal that, despite the cascaded approach being stronger in individual\ncomponents, the joint speech-text modeling improves robustness against\nrecognition errors and speech quality. Demo is available at\nhttps://unifiedsdm.github.io.",
        "score": 2.1178364753723145
      },
      {
        "title": "Prompting Large Language Models with Audio for General-Purpose Speech\n  Summarization,",
        "abstract": "In this work, we introduce a framework for speech summarization that\nleverages the processing and reasoning capabilities of large language models\n(LLMs). We propose an end-to-end system that combines an instruction-tuned LLM\nwith an audio encoder that converts speech into token representations that the\nLLM can interpret. Using a dataset with paired speech-text data, the overall\nsystem is trained to generate consistent responses to prompts with the same\nsemantic information regardless of the input modality. The resulting framework\nallows the LLM to process speech inputs in the same way as text, enabling\nspeech summarization by simply prompting the LLM. Unlike prior approaches, our\nmethod is able to summarize spoken content from any arbitrary domain, and it\ncan produce summaries in different styles by varying the LLM prompting\nstrategy. Experiments demonstrate that our approach outperforms a cascade\nbaseline of speech recognition followed by LLM text processing.",
        "score": 2.006500482559204
      },
      {
        "title": "Controllable Speaking Styles Using a Large Language Model,",
        "abstract": "Reference-based Text-to-Speech (TTS) models can generate multiple,\nprosodically-different renditions of the same target text. Such models jointly\nlearn a latent acoustic space during training, which can be sampled from during\ninference. Controlling these models during inference typically requires finding\nan appropriate reference utterance, which is non-trivial.\n  Large generative language models (LLMs) have shown excellent performance in\nvarious language-related tasks. Given only a natural language query text (the\nprompt), such models can be used to solve specific, context-dependent tasks.\nRecent work in TTS has attempted similar prompt-based control of novel speaking\nstyle generation. Those methods do not require a reference utterance and can,\nunder ideal conditions, be controlled with only a prompt. But existing methods\ntypically require a prompt-labelled speech corpus for jointly training a\nprompt-conditioned encoder.\n  In contrast, we instead employ an LLM to directly suggest prosodic\nmodifications for a controllable TTS model, using contextual information\nprovided in the prompt. The prompt can be designed for a multitude of tasks.\nHere, we give two demonstrations: control of speaking style; prosody\nappropriate for a given dialogue context. The proposed method is rated most\nappropriate in 50% of cases vs. 31% for a baseline model.",
        "score": 1.9062334299087524
      },
      {
        "title": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond\n  Words,",
        "abstract": "Speech encompasses a wealth of information, including but not limited to\ncontent, paralinguistic, and environmental information. This comprehensive\nnature of speech significantly impacts communication and is crucial for\nhuman-computer interaction. Chat-Oriented Large Language Models (LLMs), known\nfor their general-purpose assistance capabilities, have evolved to handle\nmulti-modal inputs, including speech. Although these models can be adept at\nrecognizing and analyzing speech, they often fall short of generating\nappropriate responses. We argue that this is due to the lack of principles on\ntask definition and model development, which requires open-source datasets and\nmetrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a\nbenchmark dataset aimed at multidimensional evaluation of spoken dialogue\nunderstanding and generation. SD-Eval focuses on paralinguistic and\nenvironmental information and includes 7,303 utterances, amounting to 8.76\nhours of speech data. The data is aggregated from eight public datasets,\nrepresenting four perspectives: emotion, accent, age, and background sound. To\nassess the SD-Eval benchmark dataset, we implement three different models and\nconstruct a training set following a similar process as SD-Eval. The training\nset contains 1,052.72 hours of speech data and 724.4k utterances. We also\nconduct a comprehensive evaluation using objective evaluation methods (e.g.\nBLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated\nresponses. Models conditioned with paralinguistic and environmental information\noutperform their counterparts in both objective and subjective measures.\nMoreover, experiments demonstrate LLM-based metrics show a higher correlation\nwith human evaluation compared to traditional metrics. We open-source SD-Eval\nat https://github.com/amphionspace/SD-Eval.",
        "score": 1.7872310876846313
      },
      {
        "title": "SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization,",
        "abstract": "Multi-turn dialogues are characterized by their extended length and the\npresence of turn-taking conversations. Traditional language models often\noverlook the distinct features of these dialogues by treating them as regular\ntext. In this paper, we propose a speaker-enhanced pre-training method for long\ndialogue summarization, which leverages the inherent structure of multiple-turn\ndialogues. To support our study, we curate a diverse dataset that includes\ntranscripts from real-world scenarios, movie or TV show transcripts, and\ndialogues generated by a Large Language Model. We then perform a pre-training,\nwhich encompasses the detection of speaker changes, and masked utterance\ngeneration. Experimental results of fine-tuned models demonstrate that our\nmodel achieves state-of-the-art performance on downstream benchmarks with long\ncontext, surpassing baseline models and highlighting the effectiveness of our\napproach. Our findings highlight the importance of curating pre-training\ndatasets that exhibit diversity and variations in length distribution to ensure\neffective alignment with downstream datasets.",
        "score": 1.6568275690078735
      },
      {
        "title": "End-to-end Spoken Conversational Question Answering: Task, Dataset and\n  Model,",
        "abstract": "In spoken question answering, the systems are designed to answer questions\nfrom contiguous text spans within the related speech transcripts. However, the\nmost natural way that human seek or test their knowledge is via human\nconversations. Therefore, we propose a new Spoken Conversational Question\nAnswering task (SCQA), aiming at enabling the systems to model complex dialogue\nflows given the speech documents. In this task, our main objective is to build\nthe system to deal with conversational questions based on the audio recordings,\nand to explore the plausibility of providing more cues from different\nmodalities with systems in information gathering. To this end, instead of\ndirectly adopting automatically generated speech transcripts with highly noisy\ndata, we propose a novel unified data distillation approach, DDNet, which\neffectively ingests cross-modal information to achieve fine-grained\nrepresentations of the speech and language modalities. Moreover, we propose a\nsimple and novel mechanism, termed Dual Attention, by encouraging better\nalignments between audio and text to ease the process of knowledge transfer. To\nevaluate the capacity of SCQA systems in a dialogue-style interaction, we\nassemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with\nmore than 40k question-answer pairs from 4k conversations. The performance of\nthe existing state-of-the-art methods significantly degrade on our dataset,\nhence demonstrating the necessity of cross-modal information integration. Our\nexperimental results demonstrate that our proposed method achieves superior\nperformance in spoken conversational question answering tasks.",
        "score": 1.6020722389221191
      },
      {
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk,",
        "abstract": "Large language models (LLMs) are powerful dialogue agents, but specializing\nthem towards fulfilling a specific function can be challenging. Instructing\ntuning, i.e. tuning models on instruction and sample responses generated by\nhumans (Ouyang et al., 2022), has proven as an effective method to do so, yet\nrequires a number of data samples that a) might not be available or b) costly\nto generate. Furthermore, this cost increases when the goal is to make the LLM\nfollow a specific workflow within a dialogue instead of single instructions.\nInspired by the self-play technique in reinforcement learning and the use of\nLLMs to simulate human agents, we propose a more effective method for data\ncollection through LLMs engaging in a conversation in various roles. This\napproach generates a training data via \"self-talk\" of LLMs that can be refined\nand utilized for supervised fine-tuning. We introduce an automated way to\nmeasure the (partial) success of a dialogue. This metric is used to filter the\ngenerated conversational data that is fed back in LLM for training. Based on\nour automated and human evaluations of conversation quality, we demonstrate\nthat such self-talk data improves results. In addition, we examine the various\ncharacteristics that showcase the quality of generated dialogues and how they\ncan be connected to their potential utility as training data.",
        "score": 1.2454822063446045
      },
      {
        "title": "Natural language guidance of high-fidelity text-to-speech with synthetic\n  annotations,",
        "abstract": "Text-to-speech models trained on large-scale datasets have demonstrated\nimpressive in-context learning capabilities and naturalness. However, control\nof speaker identity and style in these models typically requires conditioning\non reference speech recordings, limiting creative applications. Alternatively,\nnatural language prompting of speaker identity and style has demonstrated\npromising results and provides an intuitive method of control. However,\nreliance on human-labeled descriptions prevents scaling to large datasets. Our\nwork bridges the gap between these two approaches. We propose a scalable method\nfor labeling various aspects of speaker identity, style, and recording\nconditions. We then apply this method to a 45k hour dataset, which we use to\ntrain a speech language model. Furthermore, we propose simple methods for\nincreasing audio fidelity, significantly outperforming recent work despite\nrelying entirely on found data. Our results demonstrate high-fidelity speech\ngeneration in a diverse range of accents, prosodic styles, channel conditions,\nand acoustic conditions, all accomplished with a single model and intuitive\nnatural language conditioning. Audio samples can be heard at\nhttps://text-description-to-speech.com/.",
        "score": 1.230925440788269
      },
      {
        "title": "StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized\n  Dialogue Generation,",
        "abstract": "Large Language Models (LLMs) demonstrate superior performance in generative\nscenarios and have attracted widespread attention. Among them, stylized\ndialogue generation is essential in the context of LLMs for building\nintelligent and engaging dialogue agent. However the ability of LLMs is\ndata-driven and limited by data bias, leading to poor performance on specific\ntasks. In particular, stylized dialogue generation suffers from a severe lack\nof supervised data. Furthermore, although many prompt-based methods have been\nproposed to accomplish specific tasks, their performance in complex real-world\nscenarios involving a wide variety of dialog styles further enhancement. In\nthis work, we first introduce a stylized dialogue dataset StyleEval with 38\nstyles by leveraging the generative power of LLMs comprehensively, which has\nbeen carefully constructed with rigorous human-led quality control. Based on\nthis, we propose the stylized dialogue framework StyleChat via\nrecitation-augmented memory strategy and multi-task style learning strategy to\npromote generalization ability. To evaluate the effectiveness of our approach,\nwe created a test benchmark that included both a generation task and a choice\ntask to comprehensively evaluate trained models and assess whether styles and\npreferences are remembered and understood. Experimental results show that our\nproposed framework StyleChat outperforms all the baselines and helps to break\nthe style boundary of LLMs.",
        "score": 1.1737943887710571
      },
      {
        "title": "Spoken Question Answering and Speech Continuation Using\n  Spectrogram-Powered LLM,",
        "abstract": "We present Spectron, a novel approach to adapting pre-trained large language\nmodels (LLMs) to perform spoken question answering (QA) and speech\ncontinuation. By endowing the LLM with a pre-trained speech encoder, our model\nbecomes able to take speech inputs and generate speech outputs. The entire\nsystem is trained end-to-end and operates directly on spectrograms, simplifying\nour architecture. Key to our approach is a training objective that jointly\nsupervises speech recognition, text continuation, and speech synthesis using\nonly paired speech-text pairs, enabling a `cross-modal' chain-of-thought within\na single decoding pass. Our method surpasses existing spoken language models in\nspeaker preservation and semantic coherence. Furthermore, the proposed model\nimproves upon direct initialization in retaining the knowledge of the original\nLLM as demonstrated through spoken QA datasets. We release our audio samples\n(https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset\n(https://github.com/google-research-datasets/LLAMA1-Test-Set).",
        "score": 1.1645960807800293
      },
      {
        "title": "StyleCap: Automatic Speaking-Style Captioning from Speech Based on\n  Speech and Language Self-supervised Learning Models,",
        "abstract": "We propose StyleCap, a method to generate natural language descriptions of\nspeaking styles appearing in speech. Although most of conventional techniques\nfor para-/non-linguistic information recognition focus on the category\nclassification or the intensity estimation of pre-defined labels, they cannot\nprovide the reasoning of the recognition result in an interpretable manner.\nStyleCap is a first step towards an end-to-end method for generating\nspeaking-style prompts from speech, i.e., automatic speaking-style captioning.\nStyleCap is trained with paired data of speech and natural language\ndescriptions. We train neural networks that convert a speech representation\nvector into prefix vectors that are fed into a large language model (LLM)-based\ntext decoder. We explore an appropriate text decoder and speech feature\nrepresentation suitable for this new task. The experimental results demonstrate\nthat our StyleCap leveraging richer LLMs for the text decoder, speech\nself-supervised learning (SSL) features, and sentence rephrasing augmentation\nimproves the accuracy and diversity of generated speaking-style captions.\nSamples of speaking-style captions generated by our StyleCap are publicly\navailable.",
        "score": 1.1243822574615479
      },
      {
        "title": "LLaSM: Large Language and Speech Model,",
        "abstract": "Multi-modal large language models have garnered significant interest\nrecently. Though, most of the works focus on vision-language multi-modal models\nproviding strong capabilities in following vision-and-language instructions.\nHowever, we claim that speech is also an important modality through which\nhumans interact with the world. Hence, it is crucial for a general-purpose\nassistant to be able to follow multi-modal speech-and-language instructions. In\nthis work, we propose Large Language and Speech Model (LLaSM). LLaSM is an\nend-to-end trained large multi-modal speech-language model with cross-modal\nconversational abilities, capable of following speech-and-language\ninstructions. Our early experiments show that LLaSM demonstrates a more\nconvenient and natural way for humans to interact with artificial intelligence.\nSpecifically, we also release a large Speech Instruction Following dataset\nLLaSM-Audio-Instructions. Code and demo are available at\nhttps://github.com/LinkSoul-AI/LLaSM and\nhttps://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions\ndataset is available at\nhttps://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.",
        "score": 0.9909853935241699
      },
      {
        "title": "TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer,",
        "abstract": "Text style is highly abstract, as it encompasses various aspects of a\nspeaker's characteristics, habits, logical thinking, and the content they\nexpress. However, previous text-style transfer tasks have primarily focused on\ndata-driven approaches, lacking in-depth analysis and research from the\nperspectives of linguistics and cognitive science. In this paper, we introduce\na novel task called Text Speech-Style Transfer (TSST). The main objective is to\nfurther explore topics related to human cognition, such as personality and\nemotion, based on the capabilities of existing LLMs. Considering the objective\nof our task and the distinctive characteristics of oral speech in real-life\nscenarios, we trained multi-dimension (i.e. filler words, vividness,\ninteractivity, emotionality) evaluation models for the TSST and validated their\ncorrelation with human assessments. We thoroughly analyze the performance of\nseveral large language models (LLMs) and identify areas where further\nimprovement is needed. Moreover, driven by our evaluation models, we have\nreleased a new corpus that improves the capabilities of LLMs in generating text\nwith speech-style characteristics. In summary, we present the TSST task, a new\nbenchmark for style transfer and emphasizing human-oriented evaluation,\nexploring and advancing the performance of current LLMs.",
        "score": 0.8496066331863403
      },
      {
        "title": "Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis\n  Using Linguistic and Prosodic Contexts of Dialogue History,",
        "abstract": "We propose an end-to-end empathetic dialogue speech synthesis (DSS) model\nthat considers both the linguistic and prosodic contexts of dialogue history.\nEmpathy is the active attempt by humans to get inside the interlocutor in\ndialogue, and empathetic DSS is a technology to implement this act in spoken\ndialogue systems. Our model is conditioned by the history of linguistic and\nprosody features for predicting appropriate dialogue context. As such, it can\nbe regarded as an extension of the conventional linguistic-feature-based\ndialogue history modeling. To train the empathetic DSS model effectively, we\ninvestigate 1) a self-supervised learning model pretrained with large speech\ncorpora, 2) a style-guided training using a prosody embedding of the current\nutterance to be predicted by the dialogue context embedding, 3) a cross-modal\nattention to combine text and speech modalities, and 4) a sentence-wise\nembedding to achieve fine-grained prosody modeling rather than utterance-wise\nmodeling. The evaluation results demonstrate that 1) simply considering\nprosodic contexts of the dialogue history does not improve the quality of\nspeech in empathetic DSS and 2) introducing style-guided training and\nsentence-wise embedding modeling achieves higher speech quality than that by\nthe conventional method.",
        "score": 0.6274914741516113
      },
      {
        "title": "LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts\n  for Text-to-Speech and Style Captioning,",
        "abstract": "We introduce LibriTTS-P, a new corpus based on LibriTTS-R that includes\nutterance-level descriptions (i.e., prompts) of speaking style and\nspeaker-level prompts of speaker characteristics. We employ a hybrid approach\nto construct prompt annotations: (1) manual annotations that capture human\nperceptions of speaker characteristics and (2) synthetic annotations on\nspeaking style. Compared to existing English prompt datasets, our corpus\nprovides more diverse prompt annotations for all speakers of LibriTTS-R.\nExperimental results for prompt-based controllable TTS demonstrate that the TTS\nmodel trained with LibriTTS-P achieves higher naturalness than the model using\nthe conventional dataset. Furthermore, the results for style captioning tasks\nshow that the model utilizing LibriTTS-P generates 2.5 times more accurate\nwords than the model using a conventional dataset. Our corpus, LibriTTS-P, is\navailable at https://github.com/line/LibriTTS-P.",
        "score": 0.3909211754798889
      },
      {
        "title": "Augmenting text for spoken language understanding with Large Language\n  Models,",
        "abstract": "Spoken semantic parsing (SSP) involves generating machine-comprehensible\nparses from input speech. Training robust models for existing application\ndomains represented in training data or extending to new domains requires\ncorresponding triplets of speech-transcript-semantic parse data, which is\nexpensive to obtain. In this paper, we address this challenge by examining\nmethods that can use transcript-semantic parse data (unpaired text) without\ncorresponding speech. First, when unpaired text is drawn from existing textual\ncorpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are compared as ways\nto generate speech representations for unpaired text. Experiments on the STOP\ndataset show that unpaired text from existing and new domains improves\nperformance by 2% and 30% in absolute Exact Match (EM) respectively. Second, we\nconsider the setting when unpaired text is not available in existing textual\ncorpora. We propose to prompt Large Language Models (LLMs) to generate unpaired\ntext for existing and new domains. Experiments show that examples and words\nthat co-occur with intents can be used to generate unpaired text with Llama\n2.0. Using the generated text with JAT and TTS for spoken semantic parsing\nimproves EM on STOP by 1.4% and 2.6% absolute for existing and new domains\nrespectively.",
        "score": -0.272297203540802
      },
      {
        "title": "PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech\n  Using Natural Language Descriptions,",
        "abstract": "We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system\nthat allows control over speaker identity using natural language descriptions.\nTo control speaker identity within the prompt-based TTS framework, we introduce\nthe concept of speaker prompt, which describes voice characteristics (e.g.,\ngender-neutral, young, old, and muffled) designed to be approximately\nindependent of speaking style. Since there is no large-scale dataset containing\nspeaker prompts, we first construct a dataset based on the LibriTTS-R corpus\nwith manually annotated speaker prompts. We then employ a diffusion-based\nacoustic model with mixture density networks to model diverse speaker factors\nin the training data. Unlike previous studies that rely on style prompts\ndescribing only a limited aspect of speaker individuality, such as pitch,\nspeaking speed, and energy, our method utilizes an additional speaker prompt to\neffectively learn the mapping from natural language descriptions to the\nacoustic features of diverse speakers. Our subjective evaluation results show\nthat the proposed method can better control speaker characteristics than the\nmethods without the speaker prompt. Audio samples are available at\nhttps://reppy4620.github.io/demo.promptttspp/.",
        "score": -0.3470436632633209
      },
      {
        "title": "DisfluencySpeech -- Single-Speaker Conversational Speech Dataset with\n  Paralanguage,",
        "abstract": "Laughing, sighing, stuttering, and other forms of paralanguage do not\ncontribute any direct lexical meaning to speech, but they provide crucial\npropositional context that aids semantic and pragmatic processes such as irony.\nIt is thus important for artificial social agents to both understand and be\nable to generate speech with semantically-important paralanguage. Most speech\ndatasets do not include transcribed non-lexical speech sounds and disfluencies,\nwhile those that do are typically multi-speaker datasets where each speaker\nprovides relatively little audio. This makes it challenging to train\nconversational Text-to-Speech (TTS) synthesis models that include such\nparalinguistic components.\n  We thus present DisfluencySpeech, a studio-quality labeled English speech\ndataset with paralanguage. A single speaker recreates nearly 10 hours of\nexpressive utterances from the Switchboard-1 Telephone Speech Corpus\n(Switchboard), simulating realistic informal conversations. To aid the\ndevelopment of a TTS model that is able to predictively synthesise paralanguage\nfrom text without such components, we provide three different transcripts at\ndifferent levels of information removal (removal of non-speech events, removal\nof non-sentence elements, and removal of false starts), as well as benchmark\nTTS models trained on each of these levels.",
        "score": -0.5027960538864136
      },
      {
        "title": "M2-CTTS: End-to-End Multi-scale Multi-modal Conversational\n  Text-to-Speech Synthesis,",
        "abstract": "Conversational text-to-speech (TTS) aims to synthesize speech with proper\nprosody of reply based on the historical conversation. However, it is still a\nchallenge to comprehensively model the conversation, and a majority of\nconversational TTS systems only focus on extracting global information and omit\nlocal prosody features, which contain important fine-grained information like\nkeywords and emphasis. Moreover, it is insufficient to only consider the\ntextual features, and acoustic features also contain various prosody\ninformation. Hence, we propose M2-CTTS, an end-to-end multi-scale multi-modal\nconversational text-to-speech system, aiming to comprehensively utilize\nhistorical conversation and enhance prosodic expression. More specifically, we\ndesign a textual context module and an acoustic context module with both\ncoarse-grained and fine-grained modeling. Experimental results demonstrate that\nour model mixed with fine-grained context information and additionally\nconsidering acoustic features achieves better prosody performance and\nnaturalness in CMOS tests.",
        "score": -0.5700416564941406
      },
      {
        "title": "Lexical Entrainment for Conversational Systems,",
        "abstract": "Conversational agents have become ubiquitous in assisting with daily tasks,\nand are expected to possess human-like features. One such feature is lexical\nentrainment (LE), a phenomenon in which speakers in human-human conversations\ntend to naturally and subconsciously align their lexical choices with those of\ntheir interlocutors, leading to more successful and engaging conversations. As\nan example, if a digital assistant replies 'Your appointment for Jinling Noodle\nPub is at 7 pm' to the question 'When is my reservation for Jinling Noodle Bar\ntoday?', it may feel as though the assistant is trying to correct the speaker,\nwhereas a response of 'Your reservation for Jinling Noodle Bar is at 7 pm'\nwould likely be perceived as more positive. This highlights the importance of\nLE in establishing a shared terminology for maximum clarity and reducing\nambiguity in conversations. However, we demonstrate in this work that current\nresponse generation models do not adequately address this crucial humanlike\nphenomenon. To address this, we propose a new dataset, named MULTIWOZ-ENTR, and\na measure for LE for conversational systems. Additionally, we suggest a way to\nexplicitly integrate LE into conversational systems with two new tasks, a LE\nextraction task and a LE generation task. We also present two baseline\napproaches for the LE extraction task, which aim to detect LE expressions from\ndialogue contexts.",
        "score": -0.6029044389724731
      },
      {
        "title": "Audio Dialogues: Dialogues dataset for audio and music understanding,",
        "abstract": "Existing datasets for audio understanding primarily focus on single-turn\ninteractions (i.e. audio captioning, audio question answering) for describing\naudio in natural language, thus limiting understanding audio via interactive\ndialogue. To address this gap, we introduce Audio Dialogues: a multi-turn\ndialogue dataset containing 163.8k samples for general audio sounds and music.\nIn addition to dialogues, Audio Dialogues also has question-answer pairs to\nunderstand and compare multiple input audios together. Audio Dialogues\nleverages a prompting-based approach and caption annotations from existing\ndatasets to generate multi-turn dialogues using a Large Language Model (LLM).\nWe evaluate existing audio-augmented large language models on our proposed\ndataset to demonstrate the complexity and applicability of Audio Dialogues. Our\ncode for generating the dataset will be made publicly available. Detailed\nprompts and generated dialogues can be found on the demo website\nhttps://audiodialogues.github.io/.",
        "score": -0.6428468227386475
      },
      {
        "title": "The Conversational Short-phrase Speaker Diarization (CSSD) Task:\n  Dataset, Evaluation Metric and Baselines,",
        "abstract": "The conversation scenario is one of the most important and most challenging\nscenarios for speech processing technologies because people in conversation\nrespond to each other in a casual style. Detecting the speech activities of\neach person in a conversation is vital to downstream tasks, like natural\nlanguage processing, machine translation, etc. People refer to the detection\ntechnology of \"who speak when\" as speaker diarization (SD). Traditionally,\ndiarization error rate (DER) has been used as the standard evaluation metric of\nSD systems for a long time. However, DER fails to give enough importance to\nshort conversational phrases, which are short but important on the semantic\nlevel. Also, a carefully and accurately manually-annotated testing dataset\nsuitable for evaluating the conversational SD technologies is still unavailable\nin the speech community. In this paper, we design and describe the\nConversational Short-phrases Speaker Diarization (CSSD) task, which consists of\ntraining and testing datasets, evaluation metric and baselines. In the dataset\naspect, despite the previously open-sourced 180-hour conversational\nMagicData-RAMC dataset, we prepare an individual 20-hour conversational speech\ntest dataset with carefully and artificially verified speakers timestamps\nannotations for the CSSD task. In the metric aspect, we design the new\nconversational DER (CDER) evaluation metric, which calculates the SD accuracy\nat the utterance level. In the baseline aspect, we adopt a commonly used\nmethod: Variational Bayes HMM x-vector system, as the baseline of the CSSD\ntask. Our evaluation metric is publicly available at\nhttps://github.com/SpeechClub/CDER_Metric.",
        "score": -1.5889726877212524
      },
      {
        "title": "Do Prosody Transfer Models Transfer Prosody?,",
        "abstract": "Some recent models for Text-to-Speech synthesis aim to transfer the prosody\nof a reference utterance to the generated target synthetic speech. This is done\nby using a learned embedding of the reference utterance, which is used to\ncondition speech generation. During training, the reference utterance is\nidentical to the target utterance. Yet, during synthesis, these models are\noften used to transfer prosody from a reference that differs from the text or\nspeaker being synthesized.\n  To address this inconsistency, we propose to use a different, but\nprosodically-related, utterance during training too. We believe this should\nencourage the model to learn to transfer only those characteristics that the\nreference and target have in common. If prosody transfer methods do indeed\ntransfer prosody they should be able to be trained in the way we propose.\nHowever, results show that a model trained under these conditions performs\nsignificantly worse than one trained using the target utterance as a reference.\nTo explain this, we hypothesize that prosody transfer models do not learn a\ntransferable representation of prosody, but rather an utterance-level\nrepresentation which is highly dependent on both the reference speaker and\nreference text.",
        "score": -2.14815092086792
      }
    ]
  },
  {
    "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of\n  Text Watermark for Large Language Models",
    "abstract": "  Text watermarking technology aims to tag and identify content produced by\nlarge language models (LLMs) to prevent misuse. In this study, we introduce the\nconcept of cross-lingual consistency in text watermarking, which assesses the\nability of text watermarks to maintain their effectiveness after being\ntranslated into other languages. Preliminary empirical results from two LLMs\nand three watermarking methods reveal that current text watermarking\ntechnologies lack consistency when texts are translated into various languages.\nBased on this observation, we propose a Cross-lingual Watermark Removal Attack\n(CWRA) to bypass watermarking by first obtaining a response from an LLM in a\npivot language, which is then translated into the target language. CWRA can\neffectively remove watermarks, decreasing the AUCs to a random-guessing level\nwithout performance loss. Furthermore, we analyze two key factors that\ncontribute to the cross-lingual consistency in text watermarking and propose\nX-SIR as a defense method against CWRA. Code: https://github.com/zwhe99/X-SIR.\n",
    "related_paper_titles": [
      "A Survey of Text Watermarking in the Era of Large Language Models",
      "An Entropy-based Text Watermarking Detection Method",
      "MarkLLM: An Open-Source Toolkit for LLM Watermarking"
    ],
    "related_paper_abstract": [
      "  Text watermarking algorithms are crucial for protecting the copyright of\ntextual content. Historically, their capabilities and application scenarios\nwere limited. However, recent advancements in large language models (LLMs) have\nrevolutionized these techniques. LLMs not only enhance text watermarking\nalgorithms with their advanced abilities but also create a need for employing\nthese algorithms to protect their own copyrights or prevent potential misuse.\nThis paper conducts a comprehensive survey of the current state of text\nwatermarking technology, covering four main aspects: (1) an overview and\ncomparison of different text watermarking techniques; (2) evaluation methods\nfor text watermarking algorithms, including their detectability, impact on text\nor LLM quality, robustness under target or untargeted attacks; (3) potential\napplication scenarios for text watermarking technology; (4) current challenges\nand future directions for text watermarking. This survey aims to provide\nresearchers with a thorough understanding of text watermarking technology in\nthe era of LLM, thereby promoting its further advancement.\n",
      "  Text watermarking algorithms for large language models (LLMs) can effectively\nidentify machine-generated texts by embedding and detecting hidden features in\nthe text. Although the current text watermarking algorithms perform well in\nmost high-entropy scenarios, its performance in low-entropy scenarios still\nneeds to be improved. In this work, we opine that the influence of token\nentropy should be fully considered in the watermark detection process, $i.e.$,\nthe weight of each token during watermark detection should be customized\naccording to its entropy, rather than setting the weights of all tokens to the\nsame value as in previous methods. Specifically, we propose\n\\textbf{E}ntropy-based Text \\textbf{W}atermarking \\textbf{D}etection\n(\\textbf{EWD}) that gives higher-entropy tokens higher influence weights during\nwatermark detection, so as to better reflect the degree of watermarking.\nFurthermore, the proposed detection process is training-free and fully\nautomated. From the experiments, we demonstrate that our EWD can achieve better\ndetection performance in low-entropy scenarios, and our method is also general\nand can be applied to texts with different entropy distributions. Our code and\ndata is available\\footnote{\\url{https://github.com/luyijian3/EWD}}.\nAdditionally, our algorithm could be accessed through MarkLLM\n\\cite{pan2024markllm}\\footnote{\\url{https://github.com/THU-BPM/MarkLLM}}.\n",
      "  LLM watermarking, which embeds imperceptible yet algorithmically detectable\nsignals in model outputs to identify LLM-generated text, has become crucial in\nmitigating the potential misuse of large language models. However, the\nabundance of LLM watermarking algorithms, their intricate mechanisms, and the\ncomplex evaluation procedures and perspectives pose challenges for researchers\nand the community to easily experiment with, understand, and assess the latest\nadvancements. To address these issues, we introduce MarkLLM, an open-source\ntoolkit for LLM watermarking. MarkLLM offers a unified and extensible framework\nfor implementing LLM watermarking algorithms, while providing user-friendly\ninterfaces to ensure ease of access. Furthermore, it enhances understanding by\nsupporting automatic visualization of the underlying mechanisms of these\nalgorithms. For evaluation, MarkLLM offers a comprehensive suite of 12 tools\nspanning three perspectives, along with two types of automated evaluation\npipelines. Through MarkLLM, we aim to support researchers while improving the\ncomprehension and involvement of the general public in LLM watermarking\ntechnology, fostering consensus and driving further advancements in research\nand application. Our code is available at https://github.com/THU-BPM/MarkLLM.\n"
    ],
    "entities": [
      "DPO",
      "RLHF",
      "Human Feedback",
      "Code",
      "ToM",
      "KV",
      "Retrieval",
      "Verilog",
      "SQL",
      "MATH",
      "MMLU",
      "SLMs",
      "Wikipedia",
      "Direct Preference",
      "NER",
      "GSM8K",
      "Knowledge Graphs",
      "IFT",
      "Mind",
      "Generative",
      "AGI",
      "GenAI",
      "WSIs",
      "Automated",
      "Beyond",
      "BEV",
      "ANN",
      "Large Language Models Large",
      "Model",
      "PLMs"
    ],
    "retrieved_papers": [
      {
        "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of\n  Text Watermark for Large Language Models,",
        "abstract": "Text watermarking technology aims to tag and identify content produced by\nlarge language models (LLMs) to prevent misuse. In this study, we introduce the\nconcept of cross-lingual consistency in text watermarking, which assesses the\nability of text watermarks to maintain their effectiveness after being\ntranslated into other languages. Preliminary empirical results from two LLMs\nand three watermarking methods reveal that current text watermarking\ntechnologies lack consistency when texts are translated into various languages.\nBased on this observation, we propose a Cross-lingual Watermark Removal Attack\n(CWRA) to bypass watermarking by first obtaining a response from an LLM in a\npivot language, which is then translated into the target language. CWRA can\neffectively remove watermarks, decreasing the AUCs to a random-guessing level\nwithout performance loss. Furthermore, we analyze two key factors that\ncontribute to the cross-lingual consistency in text watermarking and propose\nX-SIR as a defense method against CWRA. Code: https://github.com/zwhe99/X-SIR.",
        "score": 7.189775466918945
      },
      {
        "title": "Necessary and Sufficient Watermark for Large Language Models,",
        "abstract": "In recent years, large language models (LLMs) have achieved remarkable\nperformances in various NLP tasks. They can generate texts that are\nindistinguishable from those written by humans. Such remarkable performance of\nLLMs increases their risk of being used for malicious purposes, such as\ngenerating fake news articles. Therefore, it is necessary to develop methods\nfor distinguishing texts written by LLMs from those written by humans.\nWatermarking is one of the most powerful methods for achieving this. Although\nexisting watermarking methods have successfully detected texts generated by\nLLMs, they significantly degrade the quality of the generated texts. In this\nstudy, we propose the Necessary and Sufficient Watermark (NS-Watermark) for\ninserting watermarks into generated texts without degrading the text quality.\nMore specifically, we derive minimum constraints required to be imposed on the\ngenerated texts to distinguish whether LLMs or humans write the texts. Then, we\nformulate the NS-Watermark as a constrained optimization problem and propose an\nefficient algorithm to solve it. Through the experiments, we demonstrate that\nthe NS-Watermark can generate more natural texts than existing watermarking\nmethods and distinguish more accurately between texts written by LLMs and those\nwritten by humans. Especially in machine translation tasks, the NS-Watermark\ncan outperform the existing watermarking method by up to 30 BLEU scores.",
        "score": 4.27582311630249
      },
      {
        "title": "On Evaluating The Performance of Watermarked Machine-Generated Texts\n  Under Adversarial Attacks,",
        "abstract": "Large Language Models (LLMs) excel in various applications, including text\ngeneration and complex tasks. However, the misuse of LLMs raises concerns about\nthe authenticity and ethical implications of the content they produce, such as\ndeepfake news, academic fraud, and copyright infringement. Watermarking\ntechniques, which embed identifiable markers in machine-generated text, offer a\npromising solution to these issues by allowing for content verification and\norigin tracing. Unfortunately, the robustness of current LLM watermarking\nschemes under potential watermark removal attacks has not been comprehensively\nexplored.\n  In this paper, to fill this gap, we first systematically comb the mainstream\nwatermarking schemes and removal attacks on machine-generated texts, and then\nwe categorize them into pre-text (before text generation) and post-text (after\ntext generation) classes so that we can conduct diversified analyses. In our\nexperiments, we evaluate eight watermarks (five pre-text, three post-text) and\ntwelve attacks (two pre-text, ten post-text) across 87 scenarios. Evaluation\nresults indicate that (1) KGW and Exponential watermarks offer high text\nquality and watermark retention but remain vulnerable to most attacks; (2)\nPost-text attacks are found to be more efficient and practical than pre-text\nattacks; (3) Pre-text watermarks are generally more imperceptible, as they do\nnot alter text fluency, unlike post-text watermarks; (4) Additionally, combined\nattack methods can significantly increase effectiveness, highlighting the need\nfor more robust watermarking solutions. Our study underscores the\nvulnerabilities of current techniques and the necessity for developing more\nresilient schemes.",
        "score": 3.8797805309295654
      },
      {
        "title": "Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality,",
        "abstract": "With the widespread adoption of Large Language Models (LLMs), concerns about\npotential misuse have emerged. To this end, watermarking has been adapted to\nLLM, enabling a simple and effective way to detect and monitor generated text.\nHowever, while the existing methods can differentiate between watermarked and\nunwatermarked text with high accuracy, they often face a trade-off between the\nquality of the generated text and the effectiveness of the watermarking\nprocess. In this work, we present a novel type of LLM watermark, Sparse\nWatermark, which aims to mitigate this trade-off by applying watermarks to a\nsmall subset of generated tokens distributed across the text. The key strategy\ninvolves anchoring watermarked tokens to words that have specific\nPart-of-Speech (POS) tags. Our experimental results demonstrate that the\nproposed watermarking scheme achieves high detectability while generating text\nthat outperforms previous LLM watermarking methods in quality across various\ntasks",
        "score": 3.5039541721343994
      },
      {
        "title": "Adaptive Text Watermark for Large Language Models,",
        "abstract": "The advancement of Large Language Models (LLMs) has led to increasing\nconcerns about the misuse of AI-generated text, and watermarking for\nLLM-generated text has emerged as a potential solution. However, it is\nchallenging to generate high-quality watermarked text while maintaining strong\nsecurity, robustness, and the ability to detect watermarks without prior\nknowledge of the prompt or model. This paper proposes an adaptive watermarking\nstrategy to address this problem. To improve the text quality and maintain\nrobustness, we adaptively add watermarking to token distributions with high\nentropy measured using an auxiliary model and keep the low entropy token\ndistributions untouched. For the sake of security and to further minimize the\nwatermark's impact on text quality, instead of using a fixed green/red list\ngenerated from a random secret key, which can be vulnerable to decryption and\nforgery, we adaptively scale up the output logits in proportion based on the\nsemantic embedding of previously generated text using a well designed semantic\nmapping model. Our experiments involving various LLMs demonstrate that our\napproach achieves comparable robustness performance to existing watermark\nmethods. Additionally, the text generated by our method has perplexity\ncomparable to that of \\emph{un-watermarked} LLMs while maintaining security\neven under various attacks.",
        "score": 3.351813793182373
      },
      {
        "title": "Three Bricks to Consolidate Watermarks for Large Language Models,",
        "abstract": "The task of discerning between generated and natural texts is increasingly\nchallenging. In this context, watermarking emerges as a promising technique for\nascribing generated text to a specific model. It alters the sampling generation\nprocess so as to leave an invisible trace in the generated output, facilitating\nlater detection. This research consolidates watermarks for large language\nmodels based on three theoretical and empirical considerations. First, we\nintroduce new statistical tests that offer robust theoretical guarantees which\nremain valid even at low false-positive rates (less than 10$^{\\text{-6}}$).\nSecond, we compare the effectiveness of watermarks using classical benchmarks\nin the field of natural language processing, gaining insights into their\nreal-world applicability. Third, we develop advanced detection schemes for\nscenarios where access to the LLM is available, as well as multi-bit\nwatermarking.",
        "score": 3.2118215560913086
      },
      {
        "title": "A Survey of Text Watermarking in the Era of Large Language Models,",
        "abstract": "Text watermarking algorithms play a crucial role in the copyright protection\nof textual content, yet their capabilities and application scenarios have been\nlimited historically. The recent developments in large language models (LLMs)\nhave opened new opportunities for the advancement of text watermarking\ntechniques. LLMs not only enhance the capabilities of text watermarking\nalgorithms through their text understanding and generation abilities but also\nnecessitate the use of text watermarking algorithms for their own copyright\nprotection. This paper conducts a comprehensive survey of the current state of\ntext watermarking technology, covering four main aspects: (1) an overview and\ncomparison of different text watermarking techniques; (2) evaluation methods\nfor text watermarking algorithms, including their success rates, impact on text\nquality, robustness, and unforgeability; (3) potential application scenarios\nfor text watermarking technology; (4) current challenges and future directions\nfor development. This survey aims to provide researchers with a thorough\nunderstanding of text watermarking technology, thereby promoting its further\nadvancement.",
        "score": 3.096672296524048
      },
      {
        "title": "A Resilient and Accessible Distribution-Preserving Watermark for Large\n  Language Models,",
        "abstract": "Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models. A challenge in the domain lies in preserving the distribution\nof original generated content after watermarking. Our research extends and\nimproves upon existing watermarking framework, placing emphasis on the\nimportance of a \\textbf{Di}stribution-\\textbf{P}reserving (DiP) watermark.\nContrary to the current strategies, our proposed DiPmark simultaneously\npreserves the original token distribution during watermarking\n(distribution-preserving), is detectable without access to the language model\nAPI and prompts (accessible), and is provably robust to moderate changes of\ntokens (resilient). DiPmark operates by selecting a random set of tokens prior\nto the generation of a word, then modifying the token distribution through a\ndistribution-preserving reweight function to enhance the probability of these\nselected tokens during the sampling process. Extensive empirical evaluation on\nvarious language models and tasks demonstrates our approach's\ndistribution-preserving property, accessibility, and resilience, making it a\neffective solution for watermarking tasks that demand impeccable quality\npreservation.",
        "score": 3.0615243911743164
      },
      {
        "title": "Towards Codable Watermarking for Injecting Multi-bits Information to\n  LLMs,",
        "abstract": "As large language models (LLMs) generate texts with increasing fluency and\nrealism, there is a growing need to identify the source of texts to prevent the\nabuse of LLMs. Text watermarking techniques have proven reliable in\ndistinguishing whether a text is generated by LLMs by injecting hidden\npatterns. However, we argue that existing LLM watermarking methods are\nencoding-inefficient and cannot flexibly meet the diverse information encoding\nneeds (such as encoding model version, generation time, user id, etc.). In this\nwork, we conduct the first systematic study on the topic of Codable Text\nWatermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit\ncustomizable information. First of all, we study the taxonomy of LLM\nwatermarking technologies and give a mathematical formulation for CTWL.\nAdditionally, we provide a comprehensive evaluation system for CTWL: (1)\nwatermarking success rate, (2) robustness against various corruptions, (3)\ncoding rate of payload information, (4) encoding and decoding efficiency, (5)\nimpacts on the quality of the generated text. To meet the requirements of these\nnon-Pareto-improving metrics, we follow the most prominent vocabulary\npartition-based watermarking direction, and devise an advanced CTWL method\nnamed Balance-Marking. The core idea of our method is to use a proxy language\nmodel to split the vocabulary into probability-balanced parts, thereby\neffectively maintaining the quality of the watermarked text. Our code is\navailable at https://github.com/lancopku/codable-watermarking-for-llm.",
        "score": 2.9937798976898193
      },
      {
        "title": "WaterPool: A Watermark Mitigating Trade-offs among Imperceptibility,\n  Efficacy and Robustness,",
        "abstract": "With the increasing use of large language models (LLMs) in daily life,\nconcerns have emerged regarding their potential misuse and societal impact.\nWatermarking is proposed to trace the usage of specific models by injecting\npatterns into their generated texts. An ideal watermark should produce outputs\nthat are nearly indistinguishable from those of the original LLM\n(imperceptibility), while ensuring a high detection rate (efficacy), even when\nthe text is partially altered (robustness). Despite many methods having been\nproposed, none have simultaneously achieved all three properties, revealing an\ninherent trade-off. This paper utilizes a key-centered scheme to unify existing\nwatermarking techniques by decomposing a watermark into two distinct modules: a\nkey module and a mark module. Through this decomposition, we demonstrate for\nthe first time that the key module significantly contributes to the trade-off\nissues observed in prior methods. Specifically, this reflects the conflict\nbetween the scale of the key sampling space during generation and the\ncomplexity of key restoration during detection. To this end, we introduce\n\\textbf{WaterPool}, a simple yet effective key module that preserves a complete\nkey sampling space required by imperceptibility while utilizing semantics-based\nsearch to improve the key restoration process. WaterPool can integrate with\nmost watermarks, acting as a plug-in. Our experiments with three well-known\nwatermarking techniques show that WaterPool significantly enhances their\nperformance, achieving near-optimal imperceptibility and markedly improving\nefficacy and robustness (+12.73\\% for KGW, +20.27\\% for EXP, +7.27\\% for ITS).",
        "score": 2.986734628677368
      },
      {
        "title": "New Evaluation Metrics Capture Quality Degradation due to LLM\n  Watermarking,",
        "abstract": "With the increasing use of large-language models (LLMs) like ChatGPT,\nwatermarking has emerged as a promising approach for tracing machine-generated\ncontent. However, research on LLM watermarking often relies on simple\nperplexity or diversity-based measures to assess the quality of watermarked\ntext, which can mask important limitations in watermarking. Here we introduce\ntwo new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1)\nevaluation by LLM-judger with specific guidelines; and 2) binary classification\non text embeddings to distinguish between watermarked and unwatermarked text.\nWe apply these methods to characterize the effectiveness of current\nwatermarking techniques. Our experiments, conducted across various datasets,\nreveal that current watermarking methods are detectable by even simple\nclassifiers, challenging the notion of watermarking subtlety. We also found,\nthrough the LLM judger, that watermarking impacts text quality, especially in\ndegrading the coherence and depth of the response. Our findings underscore the\ntrade-off between watermark robustness and text quality and highlight the\nimportance of having more informative metrics to assess watermarking quality.",
        "score": 2.972522258758545
      },
      {
        "title": "A Watermark for Low-entropy and Unbiased Generation in Large Language\n  Models,",
        "abstract": "Recent advancements in large language models (LLMs) have highlighted the risk\nof misuse, raising concerns about accurately detecting LLM-generated content. A\nviable solution for the detection problem is to inject imperceptible\nidentifiers into LLMs, known as watermarks. Previous work demonstrates that\nunbiased watermarks ensure unforgeability and preserve text quality by\nmaintaining the expectation of the LLM output probability distribution.\nHowever, previous unbiased watermarking methods are impractical for local\ndeployment because they rely on accesses to white-box LLMs and input prompts\nduring detection. Moreover, these methods fail to provide statistical\nguarantees for the type II error of watermark detection. This study proposes\nthe Sampling One Then Accepting (STA-1) method, an unbiased watermark that does\nnot require access to LLMs nor prompts during detection and has statistical\nguarantees for the type II error. Moreover, we propose a novel tradeoff between\nwatermark strength and text quality in unbiased watermarks. We show that in\nlow-entropy scenarios, unbiased watermarks face a tradeoff between watermark\nstrength and the risk of unsatisfactory outputs. Experimental results on\nlow-entropy and high-entropy datasets demonstrate that STA-1 achieves text\nquality and watermark strength comparable to existing unbiased watermarks, with\na low risk of unsatisfactory outputs. Implementation codes for this study are\navailable online.",
        "score": 2.9250473976135254
      },
      {
        "title": "Lost in Overlap: Exploring Watermark Collision in LLMs,",
        "abstract": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread use of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks like question answering and paraphrasing. This study focuses on dual\nwatermark collisions, where two watermarks are present simultaneously in the\nsame text. The research demonstrates that watermark collision poses a threat to\ndetection performance for detectors of both upstream and downstream watermark\nalgorithms.",
        "score": 2.688180923461914
      },
      {
        "title": "Mark My Words: Analyzing and Evaluating Language Model Watermarks,",
        "abstract": "The capabilities of large language models have grown significantly in recent\nyears and so too have concerns about their misuse. In this context, the ability\nto distinguish machine-generated text from human-authored content becomes\nimportant. Prior works have proposed numerous schemes to watermark text, which\nwould benefit from a systematic evaluation framework. This work focuses on text\nwatermarking techniques - as opposed to image watermarks - and proposes\nMARKMYWORDS, a comprehensive benchmark for them under different tasks as well\nas practical attacks. We focus on three main metrics: quality, size (e.g. the\nnumber of tokens needed to detect a watermark), and tamper-resistance. Current\nwatermarking techniques are good enough to be deployed: Kirchenbauer et al. [1]\ncan watermark Llama2-7B-chat with no perceivable loss in quality, the watermark\ncan be detected with fewer than 100 tokens, and the scheme offers good\ntamper-resistance to simple attacks. We argue that watermark\nindistinguishability, a criteria emphasized in some prior works, is too strong\na requirement: schemes that slightly modify logit distributions outperform\ntheir indistinguishable counterparts with no noticeable loss in generation\nquality. We publicly release our benchmark\n(https://github.com/wagner-group/MarkMyWords)",
        "score": 2.6611084938049316
      },
      {
        "title": "Provable Robust Watermarking for AI-Generated Text,",
        "abstract": "We study the problem of watermarking large language models (LLMs) generated\ntext -- one of the most promising approaches for addressing the safety\nchallenges of LLM usage. In this paper, we propose a rigorous theoretical\nframework to quantify the effectiveness and robustness of LLM watermarks. We\npropose a robust and high-quality watermark method, Unigram-Watermark, by\nextending an existing approach with a simplified fixed grouping strategy. We\nprove that our watermark method enjoys guaranteed generation quality,\ncorrectness in watermark detection, and is robust against text editing and\nparaphrasing. Experiments on three varying LLMs and two datasets verify that\nour Unigram-Watermark achieves superior detection accuracy and comparable\ngeneration quality in perplexity, thus promoting the responsible use of LLMs.\nCode is available at https://github.com/XuandongZhao/Unigram-Watermark.",
        "score": 2.62740421295166
      },
      {
        "title": "A Semantic Invariant Robust Watermark for Large Language Models,",
        "abstract": "Watermark algorithms for large language models (LLMs) have achieved extremely\nhigh accuracy in detecting text generated by LLMs. Such algorithms typically\ninvolve adding extra watermark logits to the LLM's logits at each generation\nstep. However, prior algorithms face a trade-off between attack robustness and\nsecurity robustness. This is because the watermark logits for a token are\ndetermined by a certain number of preceding tokens; a small number leads to low\nsecurity robustness, while a large number results in insufficient attack\nrobustness. In this work, we propose a semantic invariant watermarking method\nfor LLMs that provides both attack robustness and security robustness. The\nwatermark logits in our work are determined by the semantics of all preceding\ntokens. Specifically, we utilize another embedding LLM to generate semantic\nembeddings for all preceding tokens, and then these semantic embeddings are\ntransformed into the watermark logits through our trained watermark model.\nSubsequent analyses and experiments demonstrated the attack robustness of our\nmethod in semantically invariant settings: synonym substitution and text\nparaphrasing settings. Finally, we also show that our watermark possesses\nadequate security robustness. Our code and data are available at\n\\href{https://github.com/THU-BPM/Robust_Watermark}{https://github.com/THU-BPM/Robust\\_Watermark}.\nAdditionally, our algorithm could also be accessed through MarkLLM\n\\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.",
        "score": 2.6181254386901855
      },
      {
        "title": "Watermarking Language Models with Error Correcting Codes,",
        "abstract": "Recent progress in large language models enables the creation of realistic\nmachine-generated content. Watermarking is a promising approach to distinguish\nmachine-generated text from human text, embedding statistical signals in the\noutput that are ideally undetectable to humans. We propose a watermarking\nframework that encodes such signals through an error correcting code. Our\nmethod, termed robust binary code (RBC) watermark, introduces no distortion\ncompared to the original probability distribution, and no noticeable\ndegradation in quality. We evaluate our watermark on base and instruction\nfine-tuned models and find our watermark is robust to edits, deletions, and\ntranslations. We provide an information-theoretic perspective on watermarking,\na powerful statistical test for detection and for generating p-values, and\ntheoretical guarantees. Our empirical findings suggest our watermark is fast,\npowerful, and robust, comparing favorably to the state-of-the-art.",
        "score": 2.602210521697998
      },
      {
        "title": "Duwak: Dual Watermarks in Large Language Models,",
        "abstract": "As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.",
        "score": 2.5811240673065186
      },
      {
        "title": "On the Reliability of Watermarks for Large Language Models,",
        "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood\nthe internet with spam, social media bots, and valueless content. Watermarking\nis a simple and effective strategy for mitigating such harms by enabling the\ndetection and documentation of LLM-generated text. Yet a crucial question\nremains: How reliable is watermarking in realistic settings in the wild? There,\nwatermarked text may be modified to suit a user's needs, or entirely rewritten\nto avoid detection. We study the robustness of watermarked text after it is\nre-written by humans, paraphrased by a non-watermarked LLM, or mixed into a\nlonger hand-written document. We find that watermarks remain detectable even\nafter human and machine paraphrasing. While these attacks dilute the strength\nof the watermark, paraphrases are statistically likely to leak n-grams or even\nlonger fragments of the original text, resulting in high-confidence detections\nwhen enough tokens are observed. For example, after strong human paraphrasing\nthe watermark is detectable after observing 800 tokens on average, when setting\na 1e-5 false positive rate. We also consider a range of new detection schemes\nthat are sensitive to short spans of watermarked text embedded inside a large\ndocument, and we compare the robustness of watermarking to other kinds of\ndetectors.",
        "score": 2.555471897125244
      },
      {
        "title": "Topic-based Watermarks for LLM-Generated Text,",
        "abstract": "Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a \"topic-based watermarking\nalgorithm\" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.",
        "score": 2.5171315670013428
      },
      {
        "title": "On the Learnability of Watermarks for Language Models,",
        "abstract": "Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which can mitigate harms and misuses of language models.\nExisting watermarking strategies operate by altering the decoder of an existing\nlanguage model. In this paper, we ask whether language models can directly\nlearn to generate watermarked text, which would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, enabling\nwatermarking for open models, where users can control the decoding procedure.\nSecond, if watermarking is used to determine the provenance of generated text,\nan adversary can hurt the reputation of a victim model by spoofing its\nwatermark and generating damaging watermarked text. To investigate the\nlearnability of watermarks, we propose watermark distillation, which trains a\nstudent model to behave like a teacher model that uses decoding-based\nwatermarking. We test our approach on three decoding-based watermarking\nstrategies and various hyperparameter settings, finding that models can learn\nto generate watermarked text with high detectability. We also find limitations\nto learnability, including the loss of watermarking capabilities under\nfine-tuning on normal text and high sample complexity when learning\nlow-distortion watermarks.",
        "score": 2.478499174118042
      },
      {
        "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for\n  Identifying Machine Generated Text,",
        "abstract": "Potential harms of Large Language Models such as mass misinformation and\nplagiarism can be partially mitigated if there exists a reliable way to detect\nmachine generated text. In this paper, we propose a new watermarking method to\ndetect machine-generated texts. Our method embeds a unique pattern within the\ngenerated text, ensuring that while the content remains coherent and natural to\nhuman readers, it carries distinct markers that can be identified\nalgorithmically. Specifically, we intervene with the token sampling process in\na way which enables us to trace back our token choices during the detection\nphase. We show how watermarking affects textual quality and compare our\nproposed method with a state-of-the-art watermarking method in terms of\nrobustness and detectability. Through extensive experiments, we demonstrate the\neffectiveness of our watermarking scheme in distinguishing between watermarked\nand non-watermarked text, achieving high detection rates while maintaining\ntextual quality.",
        "score": 2.3839948177337646
      },
      {
        "title": "From Intentions to Techniques: A Comprehensive Taxonomy and Challenges\n  in Text Watermarking for Large Language Models,",
        "abstract": "With the rapid growth of Large Language Models (LLMs), safeguarding textual\ncontent against unauthorized use is crucial. Text watermarking offers a vital\nsolution, protecting both - LLM-generated and plain text sources. This paper\npresents a unified overview of different perspectives behind designing\nwatermarking techniques, through a comprehensive survey of the research\nliterature. Our work has two key advantages, (1) we analyze research based on\nthe specific intentions behind different watermarking techniques, evaluation\ndatasets used, watermarking addition, and removal methods to construct a\ncohesive taxonomy. (2) We highlight the gaps and open challenges in text\nwatermarking to promote research in protecting text authorship. This extensive\ncoverage and detailed analysis sets our work apart, offering valuable insights\ninto the evolving landscape of text watermarking in language models.",
        "score": 2.3789801597595215
      },
      {
        "title": "MarkLLM: An Open-Source Toolkit for LLM Watermarking,",
        "abstract": "LLM watermarking, which embeds imperceptible yet algorithmically detectable\nsignals in model outputs to identify LLM-generated text, has become crucial in\nmitigating the potential misuse of large language models. However, the\nabundance of LLM watermarking algorithms, their intricate mechanisms, and the\ncomplex evaluation procedures and perspectives pose challenges for researchers\nand the community to easily experiment with, understand, and assess the latest\nadvancements. To address these issues, we introduce MarkLLM, an open-source\ntoolkit for LLM watermarking. MarkLLM offers a unified and extensible framework\nfor implementing LLM watermarking algorithms, while providing user-friendly\ninterfaces to ensure ease of access. Furthermore, it enhances understanding by\nsupporting automatic visualization of the underlying mechanisms of these\nalgorithms. For evaluation, MarkLLM offers a comprehensive suite of 12 tools\nspanning three perspectives, along with two types of automated evaluation\npipelines. Through MarkLLM, we aim to support researchers while improving the\ncomprehension and involvement of the general public in LLM watermarking\ntechnology, fostering consensus and driving further advancements in research\nand application. Our code is available at https://github.com/THU-BPM/MarkLLM.",
        "score": 2.3649086952209473
      },
      {
        "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off,",
        "abstract": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite.",
        "score": 2.3646810054779053
      },
      {
        "title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models,",
        "abstract": "Recently, text watermarking algorithms for large language models (LLMs) have\nbeen proposed to mitigate the potential harms of text generated by LLMs,\nincluding fake news and copyright issues. However, current watermark detection\nalgorithms require the secret key used in the watermark generation process,\nmaking them susceptible to security breaches and counterfeiting during public\ndetection. To address this limitation, we propose an unforgeable publicly\nverifiable watermark algorithm named UPV that uses two different neural\nnetworks for watermark generation and detection, instead of using the same key\nat both stages. Meanwhile, the token embedding parameters are shared between\nthe generation and detection networks, which makes the detection network\nachieve a high accuracy very efficiently. Experiments demonstrate that our\nalgorithm attains high detection accuracy and computational efficiency through\nneural networks. Subsequent analysis confirms the high complexity involved in\nforging the watermark from the detection network. Our code is available at\n\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable\\_watermark}.\nAdditionally, our algorithm could also be accessed through MarkLLM\n\\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.",
        "score": 2.299328327178955
      },
      {
        "title": "PostMark: A Robust Blackbox Watermark for Large Language Models,",
        "abstract": "The most effective techniques to detect LLM-generated text rely on inserting\na detectable signature -- or watermark -- during the model's decoding process.\nMost existing watermarking methods require access to the underlying LLM's\nlogits, which LLM API providers are loath to share due to fears of model\ndistillation. As such, these watermarks must be implemented independently by\neach LLM provider. In this paper, we develop PostMark, a modular post-hoc\nwatermarking procedure in which an input-dependent set of words (determined via\na semantic embedding) is inserted into the text after the decoding process has\ncompleted. Critically, PostMark does not require logit access, which means it\ncan be implemented by a third party. We also show that PostMark is more robust\nto paraphrasing attacks than existing watermarking methods: our experiments\ncover eight baseline algorithms, five base LLMs, and three datasets. Finally,\nwe evaluate the impact of PostMark on text quality using both automated and\nhuman assessments, highlighting the trade-off between quality and robustness to\nparaphrasing. We release our code, outputs, and annotations at\nhttps://github.com/lilakk/PostMark.",
        "score": 2.07718563079834
      },
      {
        "title": "Robust Distortion-free Watermarks for Language Models,",
        "abstract": "We propose a methodology for planting watermarks in text from an\nautoregressive language model that are robust to perturbations without changing\nthe distribution over text up to a certain maximum generation budget. We\ngenerate watermarked text by mapping a sequence of random numbers -- which we\ncompute using a randomized watermark key -- to a sample from the language\nmodel. To detect watermarked text, any party who knows the key can align the\ntext to the random number sequence. We instantiate our watermark methodology\nwith two sampling schemes: inverse transform sampling and exponential minimum\nsampling. We apply these watermarks to three language models -- OPT-1.3B,\nLLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power\nand robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B\nand LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq\n0.01$) from $35$ tokens even after corrupting between $40$-$50\\%$ of the tokens\nvia random edits (i.e., substitutions, insertions or deletions). For the\nAlpaca-7B model, we conduct a case study on the feasibility of watermarking\nresponses to typical user instructions. Due to the lower entropy of the\nresponses, detection is more difficult: around $25\\%$ of the responses -- whose\nmedian length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and\nthe watermark is also less robust to certain automated paraphrasing attacks we\nimplement.",
        "score": 2.060652494430542
      },
      {
        "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules,",
        "abstract": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.",
        "score": 2.0258126258850098
      },
      {
        "title": "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service\n  Copyright Protection,",
        "abstract": "Embedding as a Service (EaaS) has become a widely adopted solution, which\noffers feature extraction capabilities for addressing various downstream tasks\nin Natural Language Processing (NLP). Prior studies have shown that EaaS can be\nprone to model extraction attacks; nevertheless, this concern could be\nmitigated by adding backdoor watermarks to the text embeddings and subsequently\nverifying the attack models post-publication. Through the analysis of the\nrecent watermarking strategy for EaaS, EmbMarker, we design a novel CSE\n(Clustering, Selection, Elimination) attack that removes the backdoor watermark\nwhile maintaining the high utility of embeddings, indicating that the previous\nwatermarking approach can be breached. In response to this new threat, we\npropose a new protocol to make the removal of watermarks more challenging by\nincorporating multiple possible watermark directions. Our defense approach,\nWARDEN, notably increases the stealthiness of watermarks and has been\nempirically shown to be effective against CSE attack.",
        "score": 1.7336512804031372
      }
    ]
  },
  {
    "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
    "abstract": "  We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.\n",
    "related_paper_titles": [],
    "related_paper_abstract": [],
    "entities": [
      "LLM",
      "LLMs",
      "Large Language Models",
      "RAG",
      "Large",
      "Large Language Models Large Language Models",
      "Persian",
      "Large Language Model",
      "DPO",
      "Large Language",
      "Mistral",
      "SFT",
      "LLaMA",
      "CoT",
      "ChatGPT",
      "ICL",
      "Language Models",
      "GPT",
      "Llama",
      "RLHF",
      "OpenAI",
      "Retrieval Augmented",
      "Shapley",
      "CAD",
      "SSMs",
      "Human Feedback",
      "ToM",
      "Gemini",
      "NLP",
      "Adam"
    ],
    "retrieved_papers": [
      {
        "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?,",
        "abstract": "We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.",
        "score": 5.979147434234619
      },
      {
        "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures during\n  Inference in Transformer-Based Language Models,",
        "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing\ninformation from diverse sources. Large Language Models (LLMs) struggle to\nperform such reasoning consistently. Here we propose an approach to pinpoint\nand rectify multi-hop reasoning failures through targeted memory injections on\nLLM attention heads. First, we analyze the per-layer activations of GPT-2\nmodels in response to single and multi-hop prompts. We then propose a mechanism\nthat allows users to inject pertinent prompt-specific information, which we\nrefer to as \"memories,\" at critical LLM locations during inference. By thus\nenabling the LLM to incorporate additional relevant information during\ninference, we enhance the quality of multi-hop prompt completions. We show\nempirically that a simple, efficient, and targeted memory injection into a key\nattention layer can often increase the probability of the desired next token in\nmulti-hop tasks, by up to 424%.",
        "score": 3.7426483631134033
      },
      {
        "title": "Noisy Exemplars Make Large Language Models More Robust: A\n  Domain-Agnostic Behavioral Analysis,",
        "abstract": "Recent advances in prompt engineering enable large language models (LLMs) to\nsolve multi-hop logical reasoning problems with impressive accuracy. However,\nthere is little existing work investigating the robustness of LLMs with\nfew-shot prompting techniques. Therefore, we introduce a systematic approach to\ntest the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic\nperturbations. We include perturbations at multiple levels of abstractions\n(e.g. lexical perturbations such as typos, and semantic perturbations such as\nthe inclusion of intermediate reasoning steps in the questions) to conduct\nbehavioral analysis on the LLMs. Throughout our experiments, we find that\nmodels are more sensitive to certain perturbations such as replacing words with\ntheir synonyms. We also demonstrate that increasing the proportion of perturbed\nexemplars in the prompts improves the robustness of few-shot prompting methods.",
        "score": 3.6346356868743896
      },
      {
        "title": "MoreHopQA: More Than Multi-hop Reasoning,",
        "abstract": "Most existing multi-hop datasets are extractive answer datasets, where the\nanswers to the questions can be extracted directly from the provided context.\nThis often leads models to use heuristics or shortcuts instead of performing\ntrue multi-hop reasoning. In this paper, we propose a new multi-hop dataset,\nMoreHopQA, which shifts from extractive to generative answers. Our dataset is\ncreated by utilizing three existing multi-hop datasets: HotpotQA,\n2WikiMultihopQA, and MuSiQue. Instead of relying solely on factual reasoning,\nwe enhance the existing multi-hop questions by adding another layer of\nquestioning that involves one, two, or all three of the following types of\nreasoning: commonsense, arithmetic, and symbolic. Our dataset is created\nthrough a semi-automated process, resulting in a dataset with 1,118 samples\nthat have undergone human verification. We then use our dataset to evaluate\nfive different large language models: Mistral 7B, Gemma 7B, Llama 3 (8B and\n70B), and GPT-4. We also design various cases to analyze the reasoning steps in\nthe question-answering process. Our results show that models perform well on\ninitial multi-hop questions but struggle with our extended questions,\nindicating that our dataset is more challenging than previous ones. Our\nanalysis of question decomposition reveals that although models can correctly\nanswer questions, only a portion - 38.7% for GPT-4 and 33.4% for Llama3-70B -\nachieve perfect reasoning, where all corresponding sub-questions are answered\ncorrectly. Evaluation code and data are available at\nhttps://github.com/Alab-NII/morehopqa",
        "score": 3.2033236026763916
      },
      {
        "title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large\n  Language Models,",
        "abstract": "Recent work has showcased the powerful capability of large language models\n(LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs\nin combining these two capabilities into reasoning through multi-hop facts has\nnot been widely explored. This paper systematically investigates the\npossibilities for LLMs to utilize shortcuts based on direct connections between\nthe initial and terminal entities of multi-hop knowledge. We first explore the\nexistence of factual shortcuts through Knowledge Neurons, revealing that: (i)\nthe strength of factual shortcuts is highly correlated with the frequency of\nco-occurrence of initial and terminal entities in the pre-training corpora;\n(ii) few-shot prompting leverage more shortcuts in answering multi-hop\nquestions compared to chain-of-thought prompting. Then, we analyze the risks\nposed by factual shortcuts from the perspective of multi-hop knowledge editing.\nAnalysis shows that approximately 20% of the failures are attributed to\nshortcuts, and the initial and terminal entities in these failure instances\nusually have higher co-occurrences in the pre-training corpus. Finally, we\npropose erasing shortcut neurons to mitigate the associated risks and find that\nthis approach significantly reduces failures in multiple-hop knowledge editing\ncaused by shortcuts.",
        "score": 3.070425271987915
      },
      {
        "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on\n  Multi-Hop Queries,",
        "abstract": "Large language models (LLMs) can solve complex multi-step problems, but\nlittle is known about how these computations are implemented internally.\nMotivated by this, we study how LLMs answer multi-hop queries such as \"The\nspouse of the performer of Imagine is\". These queries require two information\nextraction steps: a latent one for resolving the first hop (\"the performer of\nImagine\") into the bridge entity (John Lennon), and one for resolving the\nsecond hop (\"the spouse of John Lennon\") into the target entity (Yoko Ono).\nUnderstanding how the latent step is computed internally is key to\nunderstanding the overall computation. By carefully analyzing the internal\ncomputations of transformer-based LLMs, we discover that the bridge entity is\nresolved in the early layers of the model. Then, only after this resolution,\nthe two-hop query is solved in the later layers. Because the second hop\ncommences in later layers, there could be cases where these layers no longer\nencode the necessary knowledge for correctly predicting the answer. Motivated\nby this, we propose a novel \"back-patching\" analysis method whereby a hidden\nrepresentation from a later layer is patched back to an earlier layer. We find\nthat in up to 57% of previously incorrect cases there exists a back-patch that\nresults in the correct generation of the answer, showing that the later layers\nindeed sometimes lack the needed functionality. Overall our methods and\nfindings open further opportunities for understanding and improving latent\nreasoning in transformer-based LLMs.",
        "score": 2.9711720943450928
      },
      {
        "title": "Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with\n  Knowledge Graphs,",
        "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities when\nprompted to generate chain-of-thought (CoT) explanations alongside answers.\nHowever, previous research on evaluating LLMs has solely focused on answer\naccuracy, neglecting the correctness of the generated CoT. In this paper, we\ndelve deeper into the CoT reasoning capabilities of LLMs in multi-hop question\nanswering by utilizing knowledge graphs (KGs). We propose a novel\ndiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledge\nof reasoning and the accuracy of the generated CoT. Through experiments\nconducted on 5 different families of LLMs across 2 multi-hop question-answering\ndatasets, we find that LLMs possess sufficient knowledge to perform reasoning.\nHowever, there exists a significant disparity between answer accuracy and\nfaithfulness of the CoT reasoning generated by LLMs, indicating that they often\narrive at correct answers through incorrect reasoning.",
        "score": 2.9169204235076904
      },
      {
        "title": "Specializing Smaller Language Models towards Multi-Step Reasoning,",
        "abstract": "The surprising ability of Large Language Models (LLMs) to perform well on\ncomplex reasoning with only few-shot chain-of-thought prompts is believed to\nemerge only in very large-scale models (100+ billion parameters). We show that\nsuch abilities can, in fact, be distilled down from GPT-3.5 ($\\ge$ 175B) to T5\nvariants ($\\le$ 11B). We propose model specialization, to specialize the\nmodel's ability towards a target task. The hypothesis is that large models\n(commonly viewed as larger than 100B) have strong modeling power, but are\nspread on a large spectrum of tasks. Small models (commonly viewed as smaller\nthan 10B) have limited model capacity, but if we concentrate their capacity on\na specific target task, the model can achieve a decent improved performance. We\nuse multi-step math reasoning as our testbed because it is a very typical\nemergent ability. We show two important aspects of model abilities: (1). there\nexists a very complex balance/ tradeoff between language models'\nmulti-dimensional abilities; (2). by paying the price of decreased generic\nability, we can clearly lift up the scaling curve of models smaller than 10B\ntowards a specialized multi-step math reasoning ability. We further give\ncomprehensive discussions about important design choices for better\ngeneralization, including the tuning data format, the start model checkpoint,\nand a new model selection method. We hope our practice and discoveries can\nserve as an important attempt towards specialized smaller models in the new\nresearch paradigm set by LLMs.",
        "score": 2.7284324169158936
      },
      {
        "title": "Stress Testing Chain-of-Thought Prompting for Large Language Models,",
        "abstract": "This report examines the effectiveness of Chain-of-Thought (CoT) prompting in\nimproving the multi-step reasoning abilities of large language models (LLMs).\nInspired by previous studies \\cite{Min2022RethinkingWork}, we analyze the\nimpact of three types of CoT prompt perturbations, namely CoT order, CoT\nvalues, and CoT operators on the performance of GPT-3 on various tasks. Our\nfindings show that incorrect CoT prompting leads to poor performance on\naccuracy metrics. Correct values in the CoT is crucial for predicting correct\nanswers. Moreover, incorrect demonstrations, where the CoT operators or the CoT\norder are wrong, do not affect the performance as drastically when compared to\nthe value based perturbations. This research deepens our understanding of CoT\nprompting and opens some new questions regarding the capability of LLMs to\nlearn reasoning in context.",
        "score": 2.5687520503997803
      },
      {
        "title": "Measuring and Narrowing the Compositionality Gap in Language Models,",
        "abstract": "We investigate the ability of language models to perform compositional\nreasoning tasks where the overall solution depends on correctly composing the\nanswers to sub-problems. We measure how often models can correctly answer all\nsub-problems but not generate the overall solution, a ratio we call the\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\nanswers that require composing multiple facts unlikely to have been observed\ntogether during pretraining. In the GPT-3 family of models, as model size\nincreases we show that the single-hop question answering performance improves\nfaster than the multi-hop performance does, therefore the compositionality gap\ndoes not decrease. This surprising result suggests that while more powerful\nmodels memorize and recall more factual knowledge, they show no corresponding\nimprovement in their ability to perform this kind of compositional reasoning.\n  We then demonstrate how elicitive prompting (such as chain of thought)\nnarrows the compositionality gap by reasoning explicitly. We present a new\nmethod, self-ask, that further improves on chain of thought. In our method, the\nmodel explicitly asks itself (and answers) follow-up questions before answering\nthe initial question. We finally show that self-ask's structured prompting lets\nus easily plug in a search engine to answer the follow-up questions, which\nadditionally improves accuracy.",
        "score": 2.3084070682525635
      },
      {
        "title": "Distributional reasoning in LLMs: Parallel reasoning processes in\n  multi-hop reasoning,",
        "abstract": "Large language models (LLMs) have shown an impressive ability to perform\ntasks believed to require thought processes. When the model does not document\nan explicit thought process, it becomes difficult to understand the processes\noccurring within its hidden layers and to determine if these processes can be\nreferred to as reasoning. We introduce a novel and interpretable analysis of\ninternal multi-hop reasoning processes in LLMs. We demonstrate that the\nprediction process for compositional reasoning questions can be modeled using a\nsimple linear transformation between two semantic category spaces. We show that\nduring inference, the middle layers of the network generate highly\ninterpretable embeddings that represent a set of potential intermediate answers\nfor the multi-hop question. We use statistical analyses to show that a\ncorresponding subset of tokens is activated in the model's output, implying the\nexistence of parallel reasoning paths. These observations hold true even when\nthe model lacks the necessary knowledge to solve the task. Our findings can\nhelp uncover the strategies that LLMs use to solve reasoning tasks, offering\ninsights into the types of thought processes that can emerge from artificial\nintelligence. Finally, we also discuss the implication of cognitive modeling of\nthese results.",
        "score": 2.1887922286987305
      },
      {
        "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go\n  without Hallucination?,",
        "abstract": "Despite the recent advancement in large language models (LLMs) and their high\nperformances across numerous benchmarks, recent research has unveiled that LLMs\nsuffer from hallucinations and unfaithful reasoning. This work studies a\nspecific type of hallucination induced by semantic associations. Specifically,\nwe investigate to what extent LLMs take shortcuts from certain keyword/entity\nbiases in the prompt instead of following the correct reasoning path. To\nquantify this phenomenon, we propose a novel probing method and benchmark\ncalled EureQA. We start from questions that LLMs will answer correctly with\nutmost certainty, and mask the important entity with evidence sentence\nrecursively, asking models to find masked entities according to a chain of\nevidence before answering the question.\n  During the construction of the evidence, we purposefully replace semantic\nclues (entities) that may lead to the correct answer with distractor clues\n(evidence) that will not directly lead to the correct answer but require a\nchain-like reasoning process. We evaluate if models can follow the correct\nreasoning chain instead of short-cutting through distractor clues. We find that\nexisting LLMs lack the necessary capabilities to follow correct reasoning paths\nand resist the attempt of greedy shortcuts. We show that the distractor\nsemantic associations often lead to model hallucination, which is strong\nevidence that questions the validity of current LLM reasoning.",
        "score": 2.0503485202789307
      },
      {
        "title": "Large Language Models are In-Context Semantic Reasoners rather than\n  Symbolic Reasoners,",
        "abstract": "The emergent few-shot reasoning capabilities of Large Language Models (LLMs)\nhave excited the natural language and machine learning community over recent\nyears. Despite of numerous successful applications, the underlying mechanism of\nsuch in-context capabilities still remains unclear. In this work, we\nhypothesize that the learned \\textit{semantics} of language tokens do the most\nheavy lifting during the reasoning process. Different from human's symbolic\nreasoning process, the semantic representations of LLMs could create strong\nconnections among tokens, thus composing a superficial logical chain. To test\nour hypothesis, we decouple semantics from the language reasoning process and\nevaluate three kinds of reasoning abilities, i.e., deduction, induction and\nabduction. Our findings reveal that semantics play a vital role in LLMs'\nin-context reasoning -- LLMs perform significantly better when semantics are\nconsistent with commonsense but struggle to solve symbolic or\ncounter-commonsense reasoning tasks by leveraging in-context new knowledge. The\nsurprising observations question whether modern LLMs have mastered the\ninductive, deductive and abductive reasoning abilities as in human\nintelligence, and motivate research on unveiling the magic existing within the\nblack-box LLMs. On the whole, our analysis provides a novel perspective on the\nrole of semantics in developing and evaluating language models' reasoning\nabilities. Code is available at {\\url{https://github.com/XiaojuanTang/ICSR}}.",
        "score": 2.011462688446045
      },
      {
        "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning\n  in Large Language Models,",
        "abstract": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving\nrationales, has impressively unlocked the reasoning potential of large language\nmodels (LLMs). Yet, the standard CoT is less effective in problems demanding\nmultiple reasoning steps. This limitation arises from the complex reasoning\nprocess in multi-step problems: later stages often depend on the results of\nseveral steps earlier, not just the results of the immediately preceding step.\nSuch complexities suggest the reasoning process is naturally represented as a\ngraph. The almost linear and straightforward structure of CoT prompting,\nhowever, struggles to capture this complex reasoning graph. To address this\nchallenge, we propose Residual Connection Prompting (RESPROMPT), a new\nprompting strategy that advances multi-step reasoning in LLMs. Our key idea is\nto reconstruct the reasoning graph within prompts. We achieve this by\nintegrating necessary connections-links present in the reasoning graph but\nmissing in the linear CoT flow-into the prompts. Termed \"residual connections\",\nthese links are pivotal in morphing the linear CoT structure into a graph\nrepresentation, effectively capturing the complex reasoning graphs inherent in\nmulti-step problems. We evaluate RESPROMPT on six benchmarks across three\ndiverse domains: math, sequential, and commonsense reasoning. For the\nopen-sourced LLaMA family of models, RESPROMPT yields a significant average\nreasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B.\nBreakdown analysis further highlights RESPROMPT particularly excels in complex\nmulti-step reasoning: for questions demanding at least five reasoning steps,\nRESPROMPT outperforms the best CoT based benchmarks by a remarkable average\nimprovement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive\nablation studies and analyses, we pinpoint how to most effectively build\nresidual connections.",
        "score": 2.005594491958618
      },
      {
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of\n  What Matters,",
        "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.",
        "score": 1.8728951215744019
      },
      {
        "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning\n  Capabilities of Language Models,",
        "abstract": "Recent work has shown that language models (LMs) have strong multi-step\n(i.e., procedural) reasoning capabilities. However, it is unclear whether LMs\nperform these tasks by cheating with answers memorized from pretraining corpus,\nor, via a multi-step reasoning mechanism. In this paper, we try to answer this\nquestion by exploring a mechanistic interpretation of LMs for multi-step\nreasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a\nreasoning tree resembling the correct reasoning process within it. We test this\nhypothesis by introducing a new probing approach (called MechanisticProbe) that\nrecovers the reasoning tree from the model's attention patterns. We use our\nprobe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element),\nand LLaMA on two simple language-based reasoning tasks (ProofWriter & AI2\nReasoning Challenge). We show that MechanisticProbe is able to detect the\ninformation of the reasoning tree from the model's attentions for most\nexamples, suggesting that the LM indeed is going through a process of\nmulti-step reasoning within its architecture in many cases.",
        "score": 1.863356351852417
      },
      {
        "title": "Measuring Faithfulness in Chain-of-Thought Reasoning,",
        "abstract": "Large language models (LLMs) perform better when they produce step-by-step,\n\"Chain-of-Thought\" (CoT) reasoning before answering a question, but it is\nunclear if the stated reasoning is a faithful explanation of the model's actual\nreasoning (i.e., its process for answering the question). We investigate\nhypotheses for how CoT reasoning may be unfaithful, by examining how the model\npredictions change when we intervene on the CoT (e.g., by adding mistakes or\nparaphrasing it). Models show large variation across tasks in how strongly they\ncondition on the CoT when predicting their answer, sometimes relying heavily on\nthe CoT and other times primarily ignoring it. CoT's performance boost does not\nseem to come from CoT's added test-time compute alone or from information\nencoded via the particular phrasing of the CoT. As models become larger and\nmore capable, they produce less faithful reasoning on most tasks we study.\nOverall, our results suggest that CoT can be faithful if the circumstances such\nas the model size and task are carefully chosen.",
        "score": 1.7633196115493774
      },
      {
        "title": "Chain-of-Thought Reasoning Without Prompting,",
        "abstract": "In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding effectively\nelicits reasoning capabilities from language models, which were previously\nobscured by standard greedy decoding.",
        "score": 1.6600711345672607
      },
      {
        "title": "Towards Faithful Chain-of-Thought: Large Language Models are Bridging\n  Reasoners,",
        "abstract": "Large language models (LLMs) suffer from serious unfaithful chain-of-thought\n(CoT) issues. Previous work attempts to measure and explain it but lacks\nin-depth analysis within CoTs and does not consider the interactions among all\nreasoning components jointly. In this paper, we first study the CoT\nfaithfulness issue at the granularity of CoT steps, identify two reasoning\nparadigms: centralized reasoning and distributed reasoning, and find their\nrelationship with faithfulness. Subsequently, we conduct a joint analysis of\nthe causal relevance among the context, CoT, and answer during reasoning. The\nresult proves that, when the LLM predicts answers, it can recall correct\ninformation missing in the CoT from the context, leading to unfaithfulness\nissues. Finally, we propose the inferential bridging method to mitigate this\nissue, in which we use the attribution method to recall information as hints\nfor CoT generation and filter out noisy CoTs based on their semantic\nconsistency and attribution scores. Extensive experiments demonstrate that our\napproach effectively alleviates the unfaithful CoT problem.",
        "score": 1.6443040370941162
      },
      {
        "title": "How to think step-by-step: A mechanistic understanding of\n  chain-of-thought reasoning,",
        "abstract": "Despite superior reasoning prowess demonstrated by Large Language Models\n(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails\naround the internal mechanisms of the models that facilitate CoT generation.\nThis work investigates the neural sub-structures within LLMs that manifest CoT\nreasoning from a mechanistic point of view. From an analysis of Llama-2 7B\napplied to multistep reasoning over fictional ontologies, we demonstrate that\nLLMs deploy multiple parallel pathways of answer generation for step-by-step\nreasoning. These parallel pathways provide sequential answers from the input\nquestion context as well as the generated CoT. We observe a functional rift in\nthe middle layers of the LLM. Token representations in the initial half remain\nstrongly biased towards the pretraining prior, with the in-context prior taking\nover in the later half. This internal phase shift manifests in different\nfunctional components: attention heads that write the answer token appear in\nthe later half, attention heads that move information along ontological\nrelationships appear in the initial half, and so on. To the best of our\nknowledge, this is the first attempt towards mechanistic investigation of CoT\nreasoning in LLMs.",
        "score": 1.521405816078186
      },
      {
        "title": "Triggering Multi-Hop Reasoning for Question Answering in Language Models\n  using Soft Prompts and Random Walks,",
        "abstract": "Despite readily memorizing world knowledge about entities, pre-trained\nlanguage models (LMs) struggle to compose together two or more facts to perform\nmulti-hop reasoning in question-answering tasks. In this work, we propose\ntechniques that improve upon this limitation by relying on random walks over\nstructured knowledge graphs. Specifically, we use soft prompts to guide LMs to\nchain together their encoded knowledge by learning to map multi-hop questions\nto random walk paths that lead to the answer. Applying our methods on two T5\nLMs shows substantial improvements over standard tuning approaches in answering\nquestions that require 2-hop reasoning.",
        "score": 1.5106253623962402
      },
      {
        "title": "The Impact of Reasoning Step Length on Large Language Models,",
        "abstract": "Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models",
        "score": 1.3565256595611572
      },
      {
        "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,",
        "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex\nreasoning tasks, which aims at designing a simple prompt like ``Let's think\nstep by step'' or multiple in-context exemplars with well-designed rationales\nto elicit Large Language Models (LLMs) to generate intermediate reasoning\nsteps. However, the generated rationales often come with mistakes, making\nunfactual and unfaithful reasoning chains. To mitigate this brittleness, we\npropose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting\nLLMs to generate explicit pieces of knowledge evidence in the form of structure\ntriple. This is inspired by our human behaviors, i.e., we can draw a mind map\nor knowledge map as the reasoning evidence in the brain before answering a\ncomplex question. Benefiting from CoK, we additionally introduce a\nF^2-Verification method to estimate the reliability of the reasoning chains in\nterms of factuality and faithfulness. For the unreliable response, the wrong\nevidence can be indicated to prompt the LLM to rethink. Extensive experiments\ndemonstrate that our method can further improve the performance of commonsense,\nfactual, symbolic, and arithmetic reasoning tasks.",
        "score": 1.3542481660842896
      },
      {
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,",
        "abstract": "We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.",
        "score": 1.1803812980651855
      },
      {
        "title": "A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning,",
        "abstract": "Chain-of-Thought (CoT) holds a significant place in augmenting the reasoning\nperformance for large language models (LLMs). While some studies focus on\nimproving CoT accuracy through methods like retrieval enhancement, yet a\nrigorous explanation for why CoT achieves such success remains unclear. In this\npaper, we analyze CoT methods under two different settings by asking the\nfollowing questions: (1) For zero-shot CoT, why does prompting the model with\n\"let's think step by step\" significantly impact its outputs? (2) For few-shot\nCoT, why does providing examples before questioning the model could\nsubstantially improve its reasoning ability? To answer these questions, we\nconduct a top-down explainable analysis from the Hopfieldian view and propose a\nRead-and-Control approach for controlling the accuracy of CoT. Through\nextensive experiments on seven datasets for three different tasks, we\ndemonstrate that our framework can decipher the inner workings of CoT, provide\nreasoning error localization, and control to come up with the correct reasoning\npath.",
        "score": 1.0809552669525146
      },
      {
        "title": "Can Large Language Models put 2 and 2 together? Probing for Entailed\n  Arithmetical Relationships,",
        "abstract": "Two major areas of interest in the era of Large Language Models regard\nquestions of what do LLMs know, and if and how they may be able to reason (or\nrather, approximately reason). Since to date these lines of work progressed\nlargely in parallel (with notable exceptions), we are interested in\ninvestigating the intersection: probing for reasoning about the implicitly-held\nknowledge. Suspecting the performance to be lacking in this area, we use a very\nsimple set-up of comparisons between cardinalities associated with elements of\nvarious subjects (e.g. the number of legs a bird has versus the number of\nwheels on a tricycle). We empirically demonstrate that although LLMs make\nsteady progress in knowledge acquisition and (pseudo)reasoning with each new\nGPT release, their capabilities are limited to statistical inference only. It\nis difficult to argue that pure statistical learning can cope with the\ncombinatorial explosion inherent in many commonsense reasoning tasks,\nespecially once arithmetical notions are involved. Further, we argue that\nbigger is not always better and chasing purely statistical improvements is\nflawed at the core, since it only exacerbates the dangerous conflation of the\nproduction of correct answers with genuine reasoning ability.",
        "score": 0.7472838759422302
      },
      {
        "title": "Conditional and Modal Reasoning in Large Language Models,",
        "abstract": "The reasoning abilities of large language models (LLMs) are the topic of a\ngrowing body of research in AI and cognitive science. In this paper, we probe\nthe extent to which twenty-five LLMs are able to distinguish logically correct\ninferences from logically fallacious ones. We focus on inference patterns\ninvolving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and\nepistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These\ninferences have been of special interest to logicians, philosophers, and\nlinguists, since they play a central role in the fundamental human ability to\nreason about distal possibilities. Assessing LLMs on these inferences is thus\nhighly relevant to the question of how much the reasoning abilities of LLMs\nmatch those of humans. Among the LLMs we tested, all but the GPT-4 model family\noften make basic mistakes with conditionals, though zero-shot chain-of-thought\nprompting helps them make fewer mistakes. Moreover, even the GPT-4 family\ndisplays logically inconsistent judgments across inference patterns involving\nepistemic modals, and almost all models give answers to certain complex\nconditional inferences widely discussed in the literature that do not match\nhuman judgments. These results highlight gaps in basic logical reasoning in\ntoday's LLMs.",
        "score": 0.6683321595191956
      },
      {
        "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought,",
        "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world\nknowledge, they have been shown incapable of recalling these knowledge to solve\ntasks requiring complex & multi-step reasoning. Similar to how humans develop a\n\"chain of thought\" for these tasks, how can we equip PLMs with such abilities?\nIn this work, we explore an iterative prompting framework, a new prompting\nparadigm which progressively elicits relevant knowledge from PLMs for\nmulti-step inference. We identify key limitations of existing prompting\nmethods, namely they are either restricted to queries with a single\nidentifiable relation/predicate, or being agnostic to input contexts, which\nmakes it difficult to capture variabilities across different inference steps.\nWe propose an iterative context-aware prompter, which addresses these\nlimitations by learning to dynamically synthesize prompts conditioned on the\ncurrent step's contexts. Experiments on three datasets involving multi-step\nreasoning show the effectiveness of the iterative scheme and the context-aware\nprompter design.",
        "score": 0.624279260635376
      },
      {
        "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language\n  Models,",
        "abstract": "We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide reasoning, LLMs significantly improve their abilities in following a\ncorrect reasoning path towards the solution. We conduct experiments of\nStep-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe\nsubstantial performance gains on various challenging reasoning-intensive tasks\nincluding STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back\nPrompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\nand 11% respectively, TimeQA by 27%, and MuSiQue by 7%.",
        "score": 0.4355182647705078
      },
      {
        "title": "Understanding Reasoning Ability of Language Models From the Perspective\n  of Reasoning Paths Aggregation,",
        "abstract": "Pre-trained language models (LMs) are able to perform complex reasoning\nwithout explicit fine-tuning. To understand how pre-training with a next-token\nprediction objective contributes to the emergence of such reasoning capability,\nwe propose that we can view an LM as deriving new conclusions by aggregating\nindirect reasoning paths seen at pre-training time. We found this perspective\neffective in two important cases of reasoning: logic reasoning with knowledge\ngraphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, we\nformalize the reasoning paths as random walk paths on the knowledge/reasoning\ngraphs. Analyses of learned LM distributions suggest that a weighted sum of\nrelevant random walk path probabilities is a reasonable way to explain how LMs\nreason. Experiments and analysis on multiple KG and CoT datasets reveal the\neffect of training on random walk paths and suggest that augmenting unlabeled\nrandom walk reasoning paths can improve real-world multi-step reasoning\nperformance. code: https://github.com/WANGXinyiLinda/LM_random_walk",
        "score": 0.12205114960670471
      }
    ]
  },
  {
    "title": "Sora Detector: A Unified Hallucination Detection for Large Text-to-Video\n  Models",
    "abstract": "  The rapid advancement in text-to-video (T2V) generative models has enabled\nthe synthesis of high-fidelity video content guided by textual descriptions.\nDespite this significant progress, these models are often susceptible to\nhallucination, generating contents that contradict the input text, which poses\na challenge to their reliability and practical deployment. To address this\ncritical issue, we introduce the SoraDetector, a novel unified framework\ndesigned to detect hallucinations across diverse large T2V models, including\nthe cutting-edge Sora model. Our framework is built upon a comprehensive\nanalysis of hallucination phenomena, categorizing them based on their\nmanifestation in the video content. Leveraging the state-of-the-art keyframe\nextraction techniques and multimodal large language models, SoraDetector first\nevaluates the consistency between extracted video content summary and textual\nprompts, then constructs static and dynamic knowledge graphs (KGs) from frames\nto detect hallucination both in single frames and across frames. Sora Detector\nprovides a robust and quantifiable measure of consistency, static and dynamic\nhallucination. In addition, we have developed the Sora Detector Agent to\nautomate the hallucination detection process and generate a complete video\nquality report for each input video. Lastly, we present a novel meta-evaluation\nbenchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of\nadvancements in T2V hallucination detection. Through extensive experiments on\nvideos generated by Sora and other large T2V models, we demonstrate the\nefficacy of our approach in accurately detecting hallucinations. The code and\ndataset can be accessed via GitHub.\n",
    "related_paper_titles": [
      "Detecting and Preventing Hallucinations in Large Vision Language Models",
      "MagicVideo: Efficient Video Generation With Latent Diffusion Models",
      "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models\n  via Over-Trust Penalty and Retrospection-Allocation"
    ],
    "related_paper_abstract": [
      "  Instruction tuned Large Vision Language Models (LVLMs) have significantly\nadvanced in generalizing across a diverse set of multi-modal tasks, especially\nfor Visual Question Answering (VQA). However, generating detailed responses\nthat are visually grounded is still a challenging task for these models. We\nfind that even the current state-of-the-art LVLMs (InstructBLIP) still contain\na staggering 30 percent of the hallucinatory text in the form of non-existent\nobjects, unfaithful descriptions, and inaccurate relationships. To address\nthis, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion\nDataset that can be used to train and benchmark models for hallucination\ndetection and prevention. M-HalDetect consists of 16k fine-grained annotations\non VQA examples, making it the first comprehensive multi-modal hallucination\ndetection dataset for detailed image descriptions. Unlike previous work that\nonly consider object hallucination, we additionally annotate both entity\ndescriptions and relationships that are unfaithful. To demonstrate the\npotential of this dataset for hallucination prevention, we optimize\nInstructBLIP through our novel Fine-grained Direct Preference Optimization\n(FDPO). We also train fine-grained multi-modal reward models from InstructBLIP\nand evaluate their effectiveness with best-of-n rejection sampling. We perform\nhuman evaluation on both FDPO and rejection sampling, and find that they reduce\nhallucination rates in InstructBLIP by 41% and 55% respectively. We also find\nthat our reward model generalizes to other multi-modal models, reducing\nhallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has\nstrong correlation with human evaluated accuracy scores.\n",
      "  We present an efficient text-to-video generation framework based on latent\ndiffusion models, termed MagicVideo. MagicVideo can generate smooth video clips\nthat are concordant with the given text descriptions. Due to a novel and\nefficient 3D U-Net design and modeling video distributions in a low-dimensional\nspace, MagicVideo can synthesize video clips with 256x256 spatial resolution on\na single GPU card, which takes around 64x fewer computations than the Video\nDiffusion Models (VDM) in terms of FLOPs. In specific, unlike existing works\nthat directly train video models in the RGB space, we use a pre-trained VAE to\nmap video clips into a low-dimensional latent space and learn the distribution\nof videos' latent codes via a diffusion model. Besides, we introduce two new\ndesigns to adapt the U-Net denoiser trained on image tasks to video data: a\nframe-wise lightweight adaptor for the image-to-video distribution adjustment\nand a directed temporal attention module to capture temporal dependencies\nacross frames. Thus, we can exploit the informative weights of convolution\noperators from a text-to-image model for accelerating video training. To\nameliorate the pixel dithering in the generated videos, we also propose a novel\nVideoVAE auto-encoder for better RGB reconstruction. We conduct extensive\nexperiments and demonstrate that MagicVideo can generate high-quality video\nclips with either realistic or imaginary content. Refer to\n\\url{https://magicvideo.github.io/#} for more examples.\n",
      "  Hallucination, posed as a pervasive challenge of multi-modal large language\nmodels (MLLMs), has significantly impeded their real-world usage that demands\nprecise judgment. Existing methods mitigate this issue with either training\nwith specific designed data or inferencing with external knowledge from other\nsources, incurring inevitable additional costs. In this paper, we present\nOPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a\nRetrospection-Allocation strategy, serving as a nearly free lunch to alleviate\nthe hallucination issue without additional data, knowledge, or training. Our\napproach begins with an interesting observation that, most hallucinations are\nclosely tied to the knowledge aggregation patterns manifested in the\nself-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a\nfew summary tokens, but not all the previous tokens. Such partial over-trust\ninclination results in the neglecting of image tokens and describes the image\ncontent with hallucination. Based on the observation, OPERA introduces a\npenalty term on the model logits during the beam-search decoding to mitigate\nthe over-trust issue, along with a rollback strategy that retrospects the\npresence of summary tokens in the previously generated tokens, and re-allocate\nthe token selection if necessary. With extensive experiments, OPERA shows\nsignificant hallucination-mitigating performance on different MLLMs and\nmetrics, proving its effectiveness and generality. Our code is available at:\nhttps://github.com/shikiw/OPERA.\n"
    ],
    "entities": [
      "LVLMs",
      "LMMs",
      "VLMs",
      "KGE",
      "ICL",
      "KGC",
      "VQA",
      "Language Model",
      "ToM",
      "KGs",
      "IFT",
      "PEFT",
      "Dynamic",
      "LMM",
      "Italian",
      "SNNs",
      "MoE",
      "Verilog",
      "YOLOv8",
      "Retrieval Augmented",
      "QA",
      "EEG",
      "VLM",
      "Spanish",
      "Bayesian",
      "MATH",
      "PPO",
      "Mind",
      "Indian",
      "LLaMA"
    ],
    "retrieved_papers": [
      {
        "title": "Sora Detector: A Unified Hallucination Detection for Large Text-to-Video\n  Models,",
        "abstract": "The rapid advancement in text-to-video (T2V) generative models has enabled\nthe synthesis of high-fidelity video content guided by textual descriptions.\nDespite this significant progress, these models are often susceptible to\nhallucination, generating contents that contradict the input text, which poses\na challenge to their reliability and practical deployment. To address this\ncritical issue, we introduce the SoraDetector, a novel unified framework\ndesigned to detect hallucinations across diverse large T2V models, including\nthe cutting-edge Sora model. Our framework is built upon a comprehensive\nanalysis of hallucination phenomena, categorizing them based on their\nmanifestation in the video content. Leveraging the state-of-the-art keyframe\nextraction techniques and multimodal large language models, SoraDetector first\nevaluates the consistency between extracted video content summary and textual\nprompts, then constructs static and dynamic knowledge graphs (KGs) from frames\nto detect hallucination both in single frames and across frames. Sora Detector\nprovides a robust and quantifiable measure of consistency, static and dynamic\nhallucination. In addition, we have developed the Sora Detector Agent to\nautomate the hallucination detection process and generate a complete video\nquality report for each input video. Lastly, we present a novel meta-evaluation\nbenchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of\nadvancements in T2V hallucination detection. Through extensive experiments on\nvideos generated by Sora and other large T2V models, we demonstrate the\nefficacy of our approach in accurately detecting hallucinations. The code and\ndataset can be accessed via GitHub.",
        "score": 5.854030609130859
      },
      {
        "title": "Unveiling Hallucination in Text, Image, Video, and Audio Foundation\n  Models: A Comprehensive Survey,",
        "abstract": "The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.",
        "score": 2.4092459678649902
      },
      {
        "title": "Prompt-Consistency Image Generation (PCIG): A Unified Framework\n  Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models,",
        "abstract": "The rapid advancement of Text-to-Image(T2I) generative models has enabled the\nsynthesis of high-quality images guided by textual descriptions. Despite this\nsignificant progress, these models are often susceptible in generating contents\nthat contradict the input text, which poses a challenge to their reliability\nand practical deployment. To address this problem, we introduce a novel\ndiffusion-based framework to significantly enhance the alignment of generated\nimages with their corresponding descriptions, addressing the inconsistency\nbetween visual output and textual input. Our framework is built upon a\ncomprehensive analysis of inconsistency phenomena, categorizing them based on\ntheir manifestation in the image. Leveraging a state-of-the-art large language\nmodule, we first extract objects and construct a knowledge graph to predict the\nlocations of these objects in potentially generated images. We then integrate a\nstate-of-the-art controllable image generation model with a visual text\ngeneration module to generate an image that is consistent with the original\nprompt, guided by the predicted object locations. Through extensive experiments\non an advanced multimodal hallucination benchmark, we demonstrate the efficacy\nof our approach in accurately generating the images without the inconsistency\nwith the original prompt. The code can be accessed via\nhttps://github.com/TruthAI-Lab/PCIG.",
        "score": 2.1361489295959473
      },
      {
        "title": "Unified Hallucination Detection for Multimodal Large Language Models,",
        "abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.",
        "score": 1.7339075803756714
      },
      {
        "title": "Zero-Shot Multi-task Hallucination Detection,",
        "abstract": "In recent studies, the extensive utilization of large language models has\nunderscored the importance of robust evaluation methodologies for assessing\ntext generation quality and relevance to specific tasks. This has revealed a\nprevalent issue known as hallucination, an emergent condition in the model\nwhere generated text lacks faithfulness to the source and deviates from the\nevaluation criteria. In this study, we formally define hallucination and\npropose a framework for its quantitative detection in a zero-shot setting,\nleveraging our definition and the assumption that model outputs entail task and\nsample specific inputs. In detecting hallucinations, our solution achieves an\naccuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting.\nNotably, our solution maintains computational efficiency, requiring far less\ncomputational resources than other SOTA approaches, aligning with the trend\ntowards lightweight and compressed models.",
        "score": 1.7223525047302246
      },
      {
        "title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in\n  Large Video-Language Models,",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have extended\ntheir capabilities to video understanding. Yet, these models are often plagued\nby \"hallucinations\", where irrelevant or nonsensical content is generated,\ndeviating from the actual video context. This work introduces VideoHallucer,\nthe first comprehensive benchmark for hallucination detection in large\nvideo-language models (LVLMs). VideoHallucer categorizes hallucinations into\ntwo main types: intrinsic and extrinsic, offering further subcategories for\ndetailed analysis, including object-relation, temporal, semantic detail,\nextrinsic factual, and extrinsic non-factual hallucinations. We adopt an\nadversarial binary VideoQA method for comprehensive evaluation, where pairs of\nbasic and hallucinated questions are crafted strategically. By evaluating\neleven LVLMs on VideoHallucer, we reveal that i) the majority of current models\nexhibit significant issues with hallucinations; ii) while scaling datasets and\nparameters improves models' ability to detect basic visual cues and\ncounterfactuals, it provides limited benefit for detecting extrinsic factual\nhallucinations; iii) existing models are more adept at detecting facts than\nidentifying hallucinations. As a byproduct, these analyses further instruct the\ndevelopment of our self-PEP framework, achieving an average of 5.38%\nimprovement in hallucination resistance across all model architectures.",
        "score": 1.6031336784362793
      },
      {
        "title": "Detecting and Preventing Hallucinations in Large Vision Language Models,",
        "abstract": "Instruction tuned Large Vision Language Models (LVLMs) have significantly\nadvanced in generalizing across a diverse set of multi-modal tasks, especially\nfor Visual Question Answering (VQA). However, generating detailed responses\nthat are visually grounded is still a challenging task for these models. We\nfind that even the current state-of-the-art LVLMs (InstructBLIP) still contain\na staggering 30 percent of the hallucinatory text in the form of non-existent\nobjects, unfaithful descriptions, and inaccurate relationships. To address\nthis, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion\nDataset that can be used to train and benchmark models for hallucination\ndetection and prevention. M-HalDetect consists of 16k fine-grained annotations\non VQA examples, making it the first comprehensive multi-modal hallucination\ndetection dataset for detailed image descriptions. Unlike previous work that\nonly consider object hallucination, we additionally annotate both entity\ndescriptions and relationships that are unfaithful. To demonstrate the\npotential of this dataset for hallucination prevention, we optimize\nInstructBLIP through our novel Fine-grained Direct Preference Optimization\n(FDPO). We also train fine-grained multi-modal reward models from InstructBLIP\nand evaluate their effectiveness with best-of-n rejection sampling. We perform\nhuman evaluation on both FDPO and rejection sampling, and find that they reduce\nhallucination rates in InstructBLIP by 41% and 55% respectively. We also find\nthat our reward model generalizes to other multi-modal models, reducing\nhallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has\nstrong correlation with human evaluated accuracy scores.",
        "score": 0.9296653866767883
      },
      {
        "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in\n  Multimodal Large Language Models,",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the comprehension of multimedia content, bringing\ntogether diverse modalities such as text, images, and videos. However, a\ncritical challenge faced by these models, especially when processing video\ninputs, is the occurrence of hallucinations - erroneous perceptions or\ninterpretations, particularly at the event level. This study introduces an\ninnovative method to address event-level hallucinations in MLLMs, focusing on\nspecific temporal understanding in video content. Our approach leverages a\nnovel framework that extracts and utilizes event-specific information from both\nthe event query and the provided video to refine MLLMs' response. We propose a\nunique mechanism that decomposes on-demand event queries into iconic actions.\nSubsequently, we employ models like CLIP and BLIP2 to predict specific\ntimestamps for event occurrences. Our evaluation, conducted using the\nCharades-STA dataset, demonstrates a significant reduction in temporal\nhallucinations and an improvement in the quality of event-related responses.\nThis research not only provides a new perspective in addressing a critical\nlimitation of MLLMs but also contributes a quantitatively measurable method for\nevaluating MLLMs in the context of temporal-related questions.",
        "score": 0.7232528328895569
      },
      {
        "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language\n  Models,",
        "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare\napplications, including medical visual question answering and imaging report\ngeneration. While these models inherit the robust capabilities of foundational\nLarge Language Models (LLMs), they also inherit susceptibility to\nhallucinations-a significant concern in high-stakes medical contexts where the\nmargin for error is minimal. However, currently, there are no dedicated methods\nor benchmarks for hallucination detection and evaluation in the medical field.\nTo bridge this gap, we introduce Med-HallMark, the first benchmark specifically\ndesigned for hallucination detection and evaluation within the medical\nmultimodal domain. This benchmark provides multi-tasking hallucination support,\nmultifaceted hallucination data, and hierarchical hallucination categorization.\nFurthermore, we propose the MediHall Score, a new medical evaluative metric\ndesigned to assess LVLMs' hallucinations through a hierarchical scoring system\nthat considers the severity and type of hallucination, thereby enabling a\ngranular assessment of potential clinical impacts. We also present\nMediHallDetector, a novel Medical LVLM engineered for precise hallucination\ndetection, which employs multitask training for hallucination detection.\nThrough extensive experimental evaluations, we establish baselines for popular\nLVLMs using our benchmark. The findings indicate that MediHall Score provides a\nmore nuanced understanding of hallucination impacts compared to traditional\nmetrics and demonstrate the enhanced performance of MediHallDetector. We hope\nthis work can significantly improve the reliability of LVLMs in medical\napplications. All resources of this work will be released soon.",
        "score": 0.6361469030380249
      },
      {
        "title": "WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text\n  and Image Inputs,",
        "abstract": "Several text-to-video diffusion models have demonstrated commendable\ncapabilities in synthesizing high-quality video content. However, it remains a\nformidable challenge pertaining to maintaining temporal consistency and\nensuring action smoothness throughout the generated sequences. In this paper,\nwe present an innovative video generation AI agent that harnesses the power of\nSora-inspired multimodal learning to build skilled world models framework based\non textual prompts and accompanying images. The framework includes two parts:\nprompt enhancer and full video translation. The first part employs the\ncapabilities of ChatGPT to meticulously distill and proactively construct\nprecise prompts for each subsequent step, thereby guaranteeing the utmost\naccuracy in prompt communication and accurate execution in following model\noperations. The second part employ compatible with existing advanced diffusion\ntechniques to expansively generate and refine the key frame at the conclusion\nof a video. Then we can expertly harness the power of leading and trailing key\nframes to craft videos with enhanced temporal consistency and action\nsmoothness. The experimental results confirm that our method has strong\neffectiveness and novelty in constructing world models from text and image\ninputs over the other methods.",
        "score": 0.3516313433647156
      },
      {
        "title": "MetaToken: Detecting Hallucination in Image Descriptions by Meta\n  Classification,",
        "abstract": "Large Vision Language Models (LVLMs) have shown remarkable capabilities in\nmultimodal tasks like visual question answering or image captioning. However,\ninconsistencies between the visual information and the generated text, a\nphenomenon referred to as hallucinations, remain an unsolved problem with\nregard to the trustworthiness of LVLMs. To address this problem, recent works\nproposed to incorporate computationally costly Large (Vision) Language Models\nin order to detect hallucinations on a sentence- or subsentence-level. In this\nwork, we introduce MetaToken, a lightweight binary classifier to detect\nhallucinations on the token-level at negligible cost. Based on a statistical\nanalysis, we reveal key factors of hallucinations in LVLMs which have been\noverseen in previous works. MetaToken can be applied to any open-source LVLM\nwithout any knowledge about ground truth data providing a reliable detection of\nhallucinations. We evaluate our method on four state-of-the-art LVLMs\ndemonstrating the effectiveness of our approach.",
        "score": 0.26376843452453613
      },
      {
        "title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision\n  Language Models in the Medical Context,",
        "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior\nperformance in various tasks on natural image and text data, which inspires a\nlarge amount of studies for LVLMs fine-tuning and training. Despite their\nadvancements, there has been scant research on the robustness of these models\nagainst hallucination when fine-tuned on smaller datasets. In this study, we\nintroduce a new benchmark dataset, the Medical Visual Hallucination Test\n(MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH\ncomprises five tasks to evaluate hallucinations in LVLMs within the medical\ncontext, which includes tasks for comprehensive understanding of textual and\nvisual input, as well as long textual response generation. Our extensive\nexperiments with both general and medical LVLMs reveal that, although medical\nLVLMs demonstrate promising performance on standard medical tasks, they are\nparticularly susceptible to hallucinations, often more so than the general\nmodels, raising significant concerns about the reliability of these\ndomain-specific models. For medical LVLMs to be truly valuable in real-world\napplications, they must not only accurately integrate medical knowledge but\nalso maintain robust reasoning abilities to prevent hallucination. Our work\npaves the way for future evaluations of these studies.",
        "score": 0.250863641500473
      },
      {
        "title": "Hallucination of Multimodal Large Language Models: A Survey,",
        "abstract": "This survey presents a comprehensive analysis of the phenomenon of\nhallucination in multimodal large language models (MLLMs), also known as Large\nVision-Language Models (LVLMs), which have demonstrated significant\nadvancements and remarkable abilities in multimodal tasks. Despite these\npromising developments, MLLMs often generate outputs that are inconsistent with\nthe visual content, a challenge known as hallucination, which poses substantial\nobstacles to their practical deployment and raises concerns regarding their\nreliability in real-world applications. This problem has attracted increasing\nattention, prompting efforts to detect and mitigate such inaccuracies. We\nreview recent advances in identifying, evaluating, and mitigating these\nhallucinations, offering a detailed overview of the underlying causes,\nevaluation benchmarks, metrics, and strategies developed to address this issue.\nAdditionally, we analyze the current challenges and limitations, formulating\nopen questions that delineate potential pathways for future research. By\ndrawing the granular classification and landscapes of hallucination causes,\nevaluation benchmarks, and mitigation methods, this survey aims to deepen the\nunderstanding of hallucinations in MLLMs and inspire further advancements in\nthe field. Through our thorough and in-depth review, we contribute to the\nongoing dialogue on enhancing the robustness and reliability of MLLMs,\nproviding valuable insights and resources for researchers and practitioners\nalike. Resources are available at:\nhttps://github.com/showlab/Awesome-MLLM-Hallucination.",
        "score": 0.2388756424188614
      },
      {
        "title": "Detecting and Mitigating Hallucination in Large Vision Language Models\n  via Fine-Grained AI Feedback,",
        "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown\nnotable capabilities on a range of multi-modal tasks, but still face the\nhallucination phenomena where the generated texts do not align with the given\ncontexts, significantly restricting the usages of LVLMs. Most previous work\ndetects and mitigates hallucination at the coarse-grained level or requires\nexpensive annotation (e.g., labeling by proprietary models or human experts).\nTo address these issues, we propose detecting and mitigating hallucinations in\nLVLMs via fine-grained AI feedback. The basic idea is that we generate a\nsmall-size sentence-level hallucination annotation dataset by proprietary\nmodels, whereby we train a hallucination detection model which can perform\nsentence-level hallucination detection, covering primary hallucination types\n(i.e., object, attribute, and relationship). Then, we propose a\ndetect-then-rewrite pipeline to automatically construct preference dataset for\ntraining hallucination mitigating model. Furthermore, we propose\ndifferentiating the severity of hallucinations, and introducing a Hallucination\nSeverity-Aware Direct Preference Optimization (HSA-DPO) for mitigating\nhallucination in LVLMs by incorporating the severity of hallucinations into\npreference learning. Extensive experiments demonstrate the effectiveness of our\nmethod.",
        "score": 0.1554236114025116
      },
      {
        "title": "Seeing is Believing: Mitigating Hallucination in Large Vision-Language\n  Models via CLIP-Guided Decoding,",
        "abstract": "Large Vision-Language Models (LVLMs) are susceptible to object\nhallucinations, an issue in which their generated text contains non-existent\nobjects, greatly limiting their reliability and practicality. Current\napproaches often rely on the model's token likelihoods or other internal\ninformation, instruction tuning on additional datasets, or incorporating\ncomplex external tools. We first perform empirical analysis on sentence-level\nLVLM hallucination, finding that CLIP similarity to the image acts as a\nstronger and more robust indicator of hallucination compared to token\nlikelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)\napproach, a straightforward but effective training-free approach to reduce\nobject hallucination at decoding time. CGD uses CLIP to guide the model's\ndecoding process by enhancing visual grounding of generated text with the\nimage. Experiments demonstrate that CGD effectively mitigates object\nhallucination across multiple LVLM families while preserving the utility of\ntext generation. Codes are available at\nhttps://github.com/d-ailin/CLIP-Guided-Decoding.",
        "score": -0.005998872220516205
      },
      {
        "title": "Sora as an AGI World Model? A Complete Survey on Text-to-Video\n  Generation,",
        "abstract": "The evolution of video generation from text, starting with animating MNIST\nnumbers to simulating the physical world with Sora, has progressed at a\nbreakneck speed over the past seven years. While often seen as a superficial\nexpansion of the predecessor text-to-image generation model, text-to-video\ngeneration models are developed upon carefully engineered constituents. Here,\nwe systematically discuss these elements consisting of but not limited to core\nbuilding blocks (vision, language, and temporal) and supporting features from\nthe perspective of their contributions to achieving a world model. We employ\nthe PRISMA framework to curate 97 impactful research articles from renowned\nscientific databases primarily studying video synthesis using text conditions.\nUpon minute exploration of these manuscripts, we observe that text-to-video\ngeneration involves more intricate technologies beyond the plain extension of\ntext-to-image generation. Our additional review into the shortcomings of\nSora-generated videos pinpoints the call for more in-depth studies in various\nenabling aspects of video generation such as dataset, evaluation metric,\nefficient architecture, and human-controlled generation. Finally, we conclude\nthat the study of the text-to-video generation may still be in its infancy,\nrequiring contribution from the cross-discipline research community towards its\nadvancement as the first step to realize artificial general intelligence (AGI).",
        "score": -0.11148979514837265
      },
      {
        "title": "HallE-Control: Controlling Object Hallucination in Large Multimodal\n  Models,",
        "abstract": "Current Large Multimodal Models (LMMs) achieve remarkable progress, yet there\nremains significant uncertainty regarding their ability to accurately apprehend\nvisual details, that is, in performing detailed captioning. To address this, we\nintroduce $\\textit{CCEval}$, a GPT-4 assisted evaluation method for detailed\ncaptioning. Interestingly, while LMMs demonstrate minimal object existence\nhallucination in existing VQA benchmarks, our proposed evaluation reveals\ncontinued susceptibility to such hallucinations. In this paper, we make the\nfirst attempt to investigate such hallucination from different aspects,\nincluding image resolution, the language decoder size, and instruction data\namount, quality, granularity. Our findings underscore the unwarranted inference\nwhen the language description includes details at a finer object granularity\nthan what the vision module can ground or verify, thus inducing hallucination.\nTo control such hallucinations, we further attribute the reliability of\ncaptioning to contextual knowledge (involving only contextually grounded\nobjects) and parametric knowledge (containing inferred objects by the model).\nThus, we introduce $\\textit{HallE-Control}$, a controllable LMM in terms of\n$\\textbf{Hall}$ucination in object $\\textbf{E}$xistence. HallE-Control can\ncondition the captioning to shift between (i) exclusively depicting contextual\nknowledge for grounded objects and (ii) blending it with parametric knowledge\nto imagine inferred objects. Our method reduces hallucination by 44% compared\nto LLaVA$_{7B}$ and maintains the object coverage.",
        "score": -0.31974536180496216
      },
      {
        "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large\n  Vision-Language Models with Caption Rewrites,",
        "abstract": "Large language models (LLMs) have shown remarkable performance in natural\nlanguage processing (NLP) tasks. To comprehend and execute diverse human\ninstructions over image data, instruction-tuned large vision-language models\n(LVLMs) have been introduced. However, LVLMs may suffer from different types of\nobject hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained\nobject hallucinations only (i.e., generated objects non-existent in the input\nimage). The fine-grained object attributes and behaviors non-existent in the\nimage may still be generated but not measured by the current evaluation\nmethods. In this paper, we thus focus on reducing fine-grained hallucinations\nof LVLMs. We propose \\textit{ReCaption}, a framework that consists of two\ncomponents: rewriting captions using ChatGPT and fine-tuning the\ninstruction-tuned LVLMs on the rewritten captions. We also propose a\nfine-grained probing-based evaluation method named \\textit{Fine-Grained Object\nHallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate\nthat ReCaption effectively reduces fine-grained object hallucination for\ndifferent LVLM options and improves their text generation quality. The code can\nbe found at https://github.com/Anonymousanoy/FOHE.",
        "score": -0.4291975498199463
      },
      {
        "title": "Hallucination Mitigation Prompts Long-term Video Understanding,",
        "abstract": "Recently, multimodal large language models have made significant advancements\nin video understanding tasks. However, their ability to understand unprocessed\nlong videos is very limited, primarily due to the difficulty in supporting the\nenormous memory overhead. Although existing methods achieve a balance between\nmemory and information by aggregating frames, they inevitably introduce the\nsevere hallucination issue. To address this issue, this paper constructs a\ncomprehensive hallucination mitigation pipeline based on existing MLLMs.\nSpecifically, we use the CLIP Score to guide the frame sampling process with\nquestions, selecting key frames relevant to the question. Then, We inject\nquestion information into the queries of the image Q-former to obtain more\nimportant visual features. Finally, during the answer generation stage, we\nutilize chain-of-thought and in-context learning techniques to explicitly\ncontrol the generation of answers. It is worth mentioning that for the\nbreakpoint mode, we found that image understanding models achieved better\nresults than video understanding models. Therefore, we aggregated the answers\nfrom both types of models using a comparison mechanism. Ultimately, We achieved\n84.2\\% and 62.9\\% for the global and breakpoint modes respectively on the\nMovieChat dataset, surpassing the official baseline model by 29.1\\% and 24.1\\%.\nMoreover the proposed method won the third place in the CVPR LOVEU 2024\nLong-Term Video Question Answering Challenge. The code is avaiable at\nhttps://github.com/lntzm/CVPR24Track-LongVideo",
        "score": -0.4482406675815582
      },
      {
        "title": "Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens,",
        "abstract": "Recent advances in large video-language models have displayed promising\noutcomes in video comprehension. Current approaches straightforwardly convert\nvideo into language tokens and employ large language models for multi-modal\ntasks. However, this method often leads to the generation of irrelevant\ncontent, commonly known as \"hallucination\", as the length of the text increases\nand the impact of the video diminishes. To address this problem, we propose\nVista-LLaMA, a novel framework that maintains the consistent distance between\nall visual tokens and any language tokens, irrespective of the generated text\nlength. Vista-LLaMA omits relative position encoding when determining attention\nweights between visual and text tokens, retaining the position encoding for\ntext and text tokens. This amplifies the effect of visual tokens on text\ngeneration, especially when the relative distance is longer between visual and\ntext tokens. The proposed attention mechanism significantly reduces the chance\nof producing irrelevant text related to the video content. Furthermore, we\npresent a sequential visual projector that projects the current video frame\ninto tokens of language space with the assistance of the previous frame. This\napproach not only captures the temporal relationship within the video, but also\nallows less visual tokens to encompass the entire video. Our approach\nsignificantly outperforms various previous methods (e.g., Video-ChatGPT,\nMovieChat) on four challenging open-ended video question answering benchmarks.\nWe reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot\nMSRVTT-QA, setting a new state-of-the-art performance. This project is\navailable at https://jinxxian.github.io/Vista-LLaMA.",
        "score": -0.4717235565185547
      },
      {
        "title": "Direct Preference Optimization of Video Large Multimodal Models from\n  Language Model Reward,",
        "abstract": "Preference modeling techniques, such as direct preference optimization (DPO),\nhas shown effective in enhancing the generalization abilities of large language\nmodel (LLM). However, in tasks involving video instruction-following, providing\ninformative feedback, especially for detecting hallucinations in generated\nresponses, remains a significant challenge. Previous studies have explored\nusing large large multimodal models (LMMs) as reward models to guide preference\nmodeling, but their ability to accurately assess the factuality of generated\nresponses compared to corresponding videos has not been conclusively\nestablished. This paper introduces a novel framework that utilizes detailed\nvideo captions as a proxy of video content, enabling language models to\nincorporate this information as supporting evidence for scoring video Question\nAnswering (QA) predictions. Our approach demonstrates robust alignment with\nOpenAI GPT-4V model's reward mechanism, which directly takes video frames as\ninput. Furthermore, we show that applying this tailored reward through DPO\nsignificantly improves the performance of video LMMs on video QA tasks.",
        "score": -0.5929229259490967
      },
      {
        "title": "On the Audio Hallucinations in Large Audio-Video Language Models,",
        "abstract": "Large audio-video language models can generate descriptions for both video\nand audio. However, they sometimes ignore audio content, producing audio\ndescriptions solely reliant on visual information. This paper refers to this as\naudio hallucinations and analyzes them in large audio-video language models. We\ngather 1,000 sentences by inquiring about audio information and annotate them\nwhether they contain hallucinations. If a sentence is hallucinated, we also\ncategorize the type of hallucination. The results reveal that 332 sentences are\nhallucinated with distinct trends observed in nouns and verbs for each\nhallucination type. Based on this, we tackle a task of audio hallucination\nclassification using pre-trained audio-text models in the zero-shot and\nfine-tuning settings. Our experimental results reveal that the zero-shot models\nachieve higher performance (52.2% in F1) than the random (40.3%) and the\nfine-tuning models achieve 87.9%, outperforming the zero-shot models.",
        "score": -0.6472208499908447
      },
      {
        "title": "TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation,",
        "abstract": "Recent advances in diffusion-based generative modeling have led to the\ndevelopment of text-to-video (T2V) models that can generate high-quality videos\nconditioned on a text prompt. Most of these T2V models often produce\nsingle-scene video clips that depict an entity performing a particular action\n(e.g., 'a red panda climbing a tree'). However, it is pertinent to generate\nmulti-scene videos since they are ubiquitous in the real-world (e.g., 'a red\npanda climbing a tree' followed by 'the red panda sleeps on the top of the\ntree'). To generate multi-scene videos from a pretrained T2V model, we\nintroduce Time-Aligned Captions (TALC) framework. Specifically, we enhance the\ntext-conditioning mechanism in the T2V architecture to recognize the temporal\nalignment between the video scenes and scene descriptions. As a result, we show\nthat the pretrained T2V model can generate multi-scene videos that adhere to\nthe multi-scene text descriptions and be visually consistent (e.g., w.r.t\nentity and background). Our TALC-finetuned model outperforms the baseline\nmethods on multi-scene video-text data by 15.5 points on aggregated score,\naveraging visual consistency and text adherence using human evaluation. The\nproject website is https://talc-mst2v.github.io/.",
        "score": -0.7830883264541626
      },
      {
        "title": "M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced\n  Video-grounded Dialogue Generation,",
        "abstract": "Video-grounded dialogue generation (VDG) requires the system to generate a\nfluent and accurate answer based on multimodal knowledge. However, the\ndifficulty in multimodal knowledge utilization brings serious hallucinations to\nVDG models in practice. Although previous works mitigate the hallucination in a\nvariety of ways, they hardly take notice of the importance of the multimodal\nknowledge anchor answer tokens. In this paper, we reveal via perplexity that\ndifferent VDG models experience varying hallucinations and exhibit diverse\nanchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive\nmultimodal knowledge anchor enhancement framework for hallucination reduction.\nFurthermore, we introduce the counterfactual effect for more accurate anchor\ntoken detection. The experimental results on three popular benchmarks exhibit\nthe superiority of our approach over state-of-the-art methods, demonstrating\nits effectiveness in reducing hallucinations.",
        "score": -1.1203469038009644
      },
      {
        "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation\n  Framework for Large Vision Language Models,",
        "abstract": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.",
        "score": -1.2761098146438599
      },
      {
        "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models,",
        "abstract": "Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.",
        "score": -1.6330515146255493
      },
      {
        "title": "A Survey on Hallucination in Large Vision-Language Models,",
        "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.",
        "score": -1.6484863758087158
      },
      {
        "title": "Thinking Hallucination for Video Captioning,",
        "abstract": "With the advent of rich visual representations and pre-trained language\nmodels, video captioning has seen continuous improvement over time. Despite the\nperformance improvement, video captioning models are prone to hallucination.\nHallucination refers to the generation of highly pathological descriptions that\nare detached from the source material. In video captioning, there are two kinds\nof hallucination: object and action hallucination. Instead of endeavoring to\nlearn better representations of a video, in this work, we investigate the\nfundamental sources of the hallucination problem. We identify three main\nfactors: (i) inadequate visual features extracted from pre-trained models, (ii)\nimproper influences of source and target contexts during multi-modal fusion,\nand (iii) exposure bias in the training strategy. To alleviate these problems,\nwe propose two robust solutions: (a) the introduction of auxiliary heads\ntrained in multi-label settings on top of the extracted visual features and (b)\nthe addition of context gates, which dynamically select the features during\nfusion. The standard evaluation metrics for video captioning measures\nsimilarity with ground truth captions and do not adequately capture object and\naction relevance. To this end, we propose a new metric, COAHA (caption object\nand action hallucination assessment), which assesses the degree of\nhallucination. Our method achieves state-of-the-art performance on the\nMSR-Video to Text (MSR-VTT) and the Microsoft Research Video Description Corpus\n(MSVD) datasets, especially by a massive margin in CIDEr score.",
        "score": -1.649269700050354
      },
      {
        "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language\n  Models,",
        "abstract": "Large vision-language models (LVLMs) have shown remarkable abilities in\nunderstanding visual information with human languages. However, LVLMs still\nsuffer from object hallucination, which is the problem of generating\ndescriptions that include objects that do not actually exist in the images.\nThis can negatively impact many vision-language tasks, such as visual\nsummarization and reasoning. To address this issue, we propose a simple yet\npowerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify\nobject hallucination in LVLMs by reconstructing less hallucinatory\ndescriptions. LURE is grounded in a rigorous statistical analysis of the key\nfactors underlying object hallucination, including co-occurrence (the frequent\nappearance of certain objects alongside others in images), uncertainty (objects\nwith higher uncertainty during LVLM decoding), and object position\n(hallucination often appears in the later part of the generated text). LURE can\nalso be seamlessly integrated with any LVLMs. We evaluate LURE on six\nopen-source LVLMs, achieving a 23% improvement in general object hallucination\nevaluation metrics over the previous best approach. In both GPT and human\nevaluations, LURE consistently ranks at the top. Our data and code are\navailable at https://github.com/YiyangZhou/LURE.",
        "score": -2.000828504562378
      },
      {
        "title": "Mitigating Hallucination in Visual Language Models with Visual\n  Supervision,",
        "abstract": "Large vision-language models (LVLMs) suffer from hallucination a lot,\ngenerating responses that apparently contradict to the image content\noccasionally. The key problem lies in its weak ability to comprehend detailed\ncontent in a multi-modal context, which can be mainly attributed to two factors\nin training data and loss function. The vision instruction dataset primarily\nfocuses on global description, and the auto-regressive loss function favors\ntext modeling rather than image understanding. In this paper, we bring more\ndetailed vision annotations and more discriminative vision models to facilitate\nthe training of LVLMs, so that they can generate more precise responses without\nencounter hallucination. On one hand, we generate image-text pairs with\ndetailed relationship annotations in panoptic scene graph dataset (PSG). These\nconversations pay more attention on detailed facts in the image, encouraging\nthe model to answer questions based on multi-modal contexts. On the other hand,\nwe integrate SAM and mask prediction loss as auxiliary supervision, forcing the\nLVLMs to have the capacity to identify context-related objects, so that they\ncan generate more accurate responses, mitigating hallucination. Moreover, to\nprovide a deeper evaluation on the hallucination in LVLMs, we propose a new\nbenchmark, RAH-Bench. It divides vision hallucination into three different\ntypes that contradicts the image with wrong categories, attributes or\nrelations, and introduces False Positive Rate as detailed sub-metric for each\ntype. In this benchmark, our approach demonstrates an +8.4% enhancement\ncompared to original LLaVA and achieves widespread performance improvements\nacross other models.",
        "score": -2.3151817321777344
      }
    ]
  },
  {
    "title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in\n  Large Language Model Collaborations via Debate",
    "abstract": "  Large Language Models (LLMs) have shown exceptional results on current\nbenchmarks when working individually. The advancement in their capabilities,\nalong with a reduction in parameter size and inference times, has facilitated\nthe use of these models as agents, enabling interactions among multiple models\nto execute complex tasks. Such collaborations offer several advantages,\nincluding the use of specialized models (e.g. coding), improved confidence\nthrough multiple computations, and enhanced divergent thinking, leading to more\ndiverse outputs. Thus, the collaborative use of language models is expected to\ngrow significantly in the coming years. In this work, we evaluate the behavior\nof a network of models collaborating through debate under the influence of an\nadversary. We introduce pertinent metrics to assess the adversary's\neffectiveness, focusing on system accuracy and model agreement. Our findings\nhighlight the importance of a model's persuasive ability in influencing others.\nAdditionally, we explore inference-time methods to generate more compelling\narguments and evaluate the potential of prompt-based mitigation as a defensive\nstrategy.\n",
    "related_paper_titles": [],
    "related_paper_abstract": [],
    "entities": [
      "Large Language Models",
      "Language Models",
      "Retrieval Augmented",
      "SQL",
      "OOD",
      "GenAI",
      "Large Language Models Large",
      "Large Language Models Large Language Models",
      "KV",
      "Multimodal",
      "XAI",
      "RAG",
      "ChatGPT",
      "Direct Preference",
      "HumanEval",
      "EHR",
      "ICL",
      "MCTS",
      "Copilot",
      "Meta",
      "Korean",
      "Monte Carlo Tree Search",
      "LVLMs",
      "NLP",
      "TSF",
      "GAI",
      "DMs",
      "SFT",
      "Artificial",
      "QA"
    ],
    "retrieved_papers": [
      {
        "title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in\n  Large Language Model Collaborations via Debate,",
        "abstract": "Large Language Models (LLMs) have shown exceptional results on current\nbenchmarks when working individually. The advancement in their capabilities,\nalong with a reduction in parameter size and inference times, has facilitated\nthe use of these models as agents, enabling interactions among multiple models\nto execute complex tasks. Such collaborations offer several advantages,\nincluding the use of specialized models (e.g. coding), improved confidence\nthrough multiple computations, and enhanced divergent thinking, leading to more\ndiverse outputs. Thus, the collaborative use of language models is expected to\ngrow significantly in the coming years. In this work, we evaluate the behavior\nof a network of models collaborating through debate under the influence of an\nadversary. We introduce pertinent metrics to assess the adversary's\neffectiveness, focusing on system accuracy and model agreement. Our findings\nhighlight the importance of a model's persuasive ability in influencing others.\nAdditionally, we explore inference-time methods to generate more compelling\narguments and evaluate the potential of prompt-based mitigation as a defensive\nstrategy.",
        "score": 4.2686238288879395
      },
      {
        "title": "Survey of Vulnerabilities in Large Language Models Revealed by\n  Adversarial Attacks,",
        "abstract": "Large Language Models (LLMs) are swiftly advancing in architecture and\ncapability, and as they integrate more deeply into complex systems, the urgency\nto scrutinize their security properties grows. This paper surveys research in\nthe emerging interdisciplinary field of adversarial attacks on LLMs, a subfield\nof trustworthy ML, combining the perspectives of Natural Language Processing\nand Security. Prior work has shown that even safety-aligned LLMs (via\ninstruction tuning and reinforcement learning through human feedback) can be\nsusceptible to adversarial attacks, which exploit weaknesses and mislead AI\nsystems, as evidenced by the prevalence of `jailbreak' attacks on models like\nChatGPT and Bard. In this survey, we first provide an overview of large\nlanguage models, describe their safety alignment, and categorize existing\nresearch based on various learning structures: textual-only attacks,\nmulti-modal attacks, and additional attack methods specifically targeting\ncomplex systems, such as federated learning or multi-agent systems. We also\noffer comprehensive remarks on works that focus on the fundamental sources of\nvulnerabilities and potential defenses. To make this field more accessible to\nnewcomers, we present a systematic review of existing works, a structured\ntypology of adversarial attack concepts, and additional resources, including\nslides for presentations on related topics at the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (ACL'24).",
        "score": 3.7940549850463867
      },
      {
        "title": "Combating Adversarial Attacks with Multi-Agent Debate,",
        "abstract": "While state-of-the-art language models have achieved impressive results, they\nremain susceptible to inference-time adversarial attacks, such as adversarial\nprompts generated by red teams arXiv:2209.07858. One approach proposed to\nimprove the general quality of language model generations is multi-agent\ndebate, where language models self-evaluate through discussion and feedback\narXiv:2305.14325. We implement multi-agent debate between current\nstate-of-the-art language models and evaluate models' susceptibility to red\nteam attacks in both single- and multi-agent settings. We find that multi-agent\ndebate can reduce model toxicity when jailbroken or less capable models are\nforced to debate with non-jailbroken or more capable models. We also find\nmarginal improvements through the general usage of multi-agent interactions. We\nfurther perform adversarial prompt content classification via embedding\nclustering, and analyze the susceptibility of different models to different\ntypes of attack topics.",
        "score": 3.6461193561553955
      },
      {
        "title": "L-AutoDA: Leveraging Large Language Models for Automated Decision-based\n  Adversarial Attacks,",
        "abstract": "In the rapidly evolving field of machine learning, adversarial attacks\npresent a significant challenge to model robustness and security.\nDecision-based attacks, which only require feedback on the decision of a model\nrather than detailed probabilities or scores, are particularly insidious and\ndifficult to defend against. This work introduces L-AutoDA (Large Language\nModel-based Automated Decision-based Adversarial Attacks), a novel approach\nleveraging the generative capabilities of Large Language Models (LLMs) to\nautomate the design of these attacks. By iteratively interacting with LLMs in\nan evolutionary framework, L-AutoDA automatically designs competitive attack\nalgorithms efficiently without much human effort. We demonstrate the efficacy\nof L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline\nmethods in both success rate and computational efficiency. Our findings\nunderscore the potential of language models as tools for adversarial attack\ngeneration and highlight new avenues for the development of robust AI systems.",
        "score": 3.1835906505584717
      },
      {
        "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of\n  Language Models,",
        "abstract": "Large-scale pre-trained language models have achieved tremendous success\nacross a wide range of natural language understanding (NLU) tasks, even\nsurpassing human performance. However, recent studies reveal that the\nrobustness of these models can be challenged by carefully crafted textual\nadversarial examples. While several individual datasets have been proposed to\nevaluate model robustness, a principled and comprehensive benchmark is still\nmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task\nbenchmark to quantitatively and thoroughly explore and evaluate the\nvulnerabilities of modern large-scale language models under various types of\nadversarial attacks. In particular, we systematically apply 14 textual\nadversarial attack methods to GLUE tasks to construct AdvGLUE, which is further\nvalidated by humans for reliable annotations. Our findings are summarized as\nfollows. (i) Most existing adversarial attack algorithms are prone to\ngenerating invalid or ambiguous adversarial examples, with around 90% of them\neither changing the original semantic meanings or misleading human annotators\nas well. Therefore, we perform a careful filtering process to curate a\nhigh-quality benchmark. (ii) All the language models and robust training\nmethods we tested perform poorly on AdvGLUE, with scores lagging far behind the\nbenign accuracy. We hope our work will motivate the development of new\nadversarial attacks that are more stealthy and semantic-preserving, as well as\nnew robust language models against sophisticated adversarial attacks. AdvGLUE\nis available at https://adversarialglue.github.io.",
        "score": 2.6229848861694336
      },
      {
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language\n  Models,",
        "abstract": "Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.",
        "score": 2.4657418727874756
      },
      {
        "title": "Assessing Adversarial Robustness of Large Language Models: An Empirical\n  Study,",
        "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their robustness against adversarial attacks remains a critical concern. We\npresents a novel white-box style attack approach that exposes vulnerabilities\nin leading open-source LLMs, including Llama, OPT, and T5. We assess the impact\nof model size, structure, and fine-tuning strategies on their resistance to\nadversarial perturbations. Our comprehensive evaluation across five diverse\ntext classification tasks establishes a new benchmark for LLM robustness. The\nfindings of this study have far-reaching implications for the reliable\ndeployment of LLMs in real-world applications and contribute to the advancement\nof trustworthy AI systems.",
        "score": 2.3238229751586914
      },
      {
        "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large\n  Language Models,",
        "abstract": "Large Language Models (LLMs) have become a cornerstone in the field of\nNatural Language Processing (NLP), offering transformative capabilities in\nunderstanding and generating human-like text. However, with their rising\nprominence, the security and vulnerability aspects of these models have\ngarnered significant attention. This paper presents a comprehensive survey of\nthe various forms of attacks targeting LLMs, discussing the nature and\nmechanisms of these attacks, their potential impacts, and current defense\nstrategies. We delve into topics such as adversarial attacks that aim to\nmanipulate model outputs, data poisoning that affects model training, and\nprivacy concerns related to training data exploitation. The paper also explores\nthe effectiveness of different attack methodologies, the resilience of LLMs\nagainst these attacks, and the implications for model integrity and user trust.\nBy examining the latest research, we provide insights into the current\nlandscape of LLM vulnerabilities and defense mechanisms. Our objective is to\noffer a nuanced understanding of LLM attacks, foster awareness within the AI\ncommunity, and inspire robust solutions to mitigate these risks in future\ndevelopments.",
        "score": 2.1164534091949463
      },
      {
        "title": "Large Language Model Sentinel: Advancing Adversarial Robustness by LLM\n  Agent,",
        "abstract": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for defense, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.",
        "score": 2.094646453857422
      },
      {
        "title": "KGPA: Robustness Evaluation for Large Language Models via Cross-Domain\n  Knowledge Graphs,",
        "abstract": "Existing frameworks for assessing robustness of large language models (LLMs)\noverly depend on specific benchmarks, increasing costs and failing to evaluate\nperformance of LLMs in professional domains due to dataset limitations. This\npaper proposes a framework that systematically evaluates the robustness of LLMs\nunder adversarial attack scenarios by leveraging knowledge graphs (KGs). Our\nframework generates original prompts from the triplets of knowledge graphs and\ncreates adversarial prompts by poisoning, assessing the robustness of LLMs\nthrough the results of these adversarial attacks. We systematically evaluate\nthe effectiveness of this framework and its modules. Experiments show that\nadversarial robustness of the ChatGPT family ranks as GPT-4-turbo > GPT-4o >\nGPT-3.5-turbo, and the robustness of large language models is influenced by the\nprofessional domains in which they operate.",
        "score": 2.0582199096679688
      },
      {
        "title": "Adversarial Attacks and Defense for Conversation Entailment Task,",
        "abstract": "As the deployment of NLP systems in critical applications grows, ensuring the\nrobustness of large language models (LLMs) against adversarial attacks becomes\nincreasingly important. Large language models excel in various NLP tasks but\nremain vulnerable to low-cost adversarial attacks. Focusing on the domain of\nconversation entailment, where multi-turn dialogues serve as premises to verify\nhypotheses, we fine-tune a transformer model to accurately discern the\ntruthfulness of these hypotheses. Adversaries manipulate hypotheses through\nsynonym swapping, aiming to deceive the model into making incorrect\npredictions. To counteract these attacks, we implemented innovative fine-tuning\ntechniques and introduced an embedding perturbation loss method to\nsignificantly bolster the model's robustness. Our findings not only emphasize\nthe importance of defending against adversarial attacks in NLP but also\nhighlight the real-world implications, suggesting that enhancing model\nrobustness is critical for reliable NLP applications.",
        "score": 2.0438172817230225
      },
      {
        "title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language\n  Agents,",
        "abstract": "Language agents powered by large language models (LLMs) have seen exploding\ndevelopment. Their capability of using language as a vehicle for thought and\ncommunication lends an incredible level of flexibility and versatility. People\nhave quickly capitalized on this capability to connect LLMs to a wide range of\nexternal components and environments: databases, tools, the Internet, robotic\nembodiment, etc. Many believe an unprecedentedly powerful automation technology\nis emerging. However, new automation technologies come with new safety risks,\nespecially for intricate systems like language agents. There is a surprisingly\nlarge gap between the speed and scale of their development and deployment and\nour understanding of their safety risks. Are we building a house of cards? In\nthis position paper, we present the first systematic effort in mapping\nadversarial attacks against language agents. We first present a unified\nconceptual framework for agents with three major components: Perception, Brain,\nand Action. Under this framework, we present a comprehensive discussion and\npropose 12 potential attack scenarios against different components of an agent,\ncovering different attack strategies (e.g., input manipulation, adversarial\ndemonstrations, jailbreaking, backdoors). We also draw connections to\nsuccessful attack strategies previously applied to LLMs. We emphasize the\nurgency to gain a thorough understanding of language agent risks before their\nwidespread deployment.",
        "score": 1.956976294517517
      },
      {
        "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious\n  Demonstrations Shows their Vulnerabilities,",
        "abstract": "The rapid progress in open-source Large Language Models (LLMs) is\nsignificantly driving AI development forward. However, there is still a limited\nunderstanding of their trustworthiness. Deploying these models at scale without\nsufficient trustworthiness can pose significant risks, highlighting the need to\nuncover these issues promptly. In this work, we conduct an adversarial\nassessment of open-source LLMs on trustworthiness, scrutinizing them across\neight different aspects including toxicity, stereotypes, ethics, hallucination,\nfairness, sycophancy, privacy, and robustness against adversarial\ndemonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU)\nprompting strategy by incorporating carefully crafted malicious demonstrations\nfor trustworthiness attack. Our extensive experiments encompass recent and\nrepresentative series of open-source LLMs, including Vicuna, MPT, Falcon,\nMistral, and Llama 2. The empirical outcomes underscore the efficacy of our\nattack strategy across diverse aspects. More interestingly, our result analysis\nreveals that models with superior performance in general NLP tasks do not\nalways have greater trustworthiness; in fact, larger models can be more\nvulnerable to attacks. Additionally, models that have undergone instruction\ntuning, focusing on instruction following, tend to be more susceptible,\nalthough fine-tuning LLMs for safety alignment proves effective in mitigating\nadversarial trustworthiness attacks.",
        "score": 1.9435056447982788
      },
      {
        "title": "Dynamic Transformers Provide a False Sense of Efficiency,",
        "abstract": "Despite much success in natural language processing (NLP), pre-trained\nlanguage models typically lead to a high computational cost during inference.\nMulti-exit is a mainstream approach to address this issue by making a trade-off\nbetween efficiency and accuracy, where the saving of computation comes from an\nearly exit. However, whether such saving from early-exiting is robust remains\nunknown. Motivated by this, we first show that directly adapting existing\nadversarial attack approaches targeting model accuracy cannot significantly\nreduce inference efficiency. To this end, we propose a simple yet effective\nattacking framework, SAME, a novel slowdown attack framework on multi-exit\nmodels, which is specially tailored to reduce the efficiency of the multi-exit\nmodels. By leveraging the multi-exit models' design characteristics, we utilize\nall internal predictions to guide the adversarial sample generation instead of\nmerely considering the final prediction. Experiments on the GLUE benchmark show\nthat SAME can effectively diminish the efficiency gain of various multi-exit\nmodels by 80% on average, convincingly validating its effectiveness and\ngeneralization ability.",
        "score": 1.757768154144287
      },
      {
        "title": "Cooperation, Competition, and Maliciousness: LLM-Stakeholders\n  Interactive Negotiation,",
        "abstract": "There is an growing interest in using Large Language Models (LLMs) in\nmulti-agent systems to tackle interactive real-world tasks that require\neffective collaboration and assessing complex situations. Yet, we still have a\nlimited understanding of LLMs' communication and decision-making abilities in\nmulti-agent setups. The fundamental task of negotiation spans many key features\nof communication, such as cooperation, competition, and manipulation\npotentials. Thus, we propose using scorable negotiation to evaluate LLMs. We\ncreate a testbed of complex multi-agent, multi-issue, and semantically rich\nnegotiation games. To reach an agreement, agents must have strong arithmetic,\ninference, exploration, and planning capabilities while integrating them in a\ndynamic and multi-turn setup. We propose multiple metrics to rigorously\nquantify agents' performance and alignment with the assigned role. We provide\nprocedures to create new games and increase games' difficulty to have an\nevolving benchmark. Importantly, we evaluate critical safety aspects such as\nthe interaction dynamics between agents influenced by greedy and adversarial\nplayers. Our benchmark is highly challenging; GPT-3.5 and small models mostly\nfail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.",
        "score": 1.6368393898010254
      },
      {
        "title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models\n  on Adversarial Prompts,",
        "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptRobust, a\nrobustness benchmark designed to measure LLMs' resilience to adversarial\nprompts. This study uses a plethora of adversarial textual attacks targeting\nprompts across multiple levels: character, word, sentence, and semantic. The\nadversarial prompts, crafted to mimic plausible user errors like typos or\nsynonyms, aim to evaluate how slight deviations can affect LLM outcomes while\nmaintaining semantic integrity. These prompts are then employed in diverse\ntasks including sentiment analysis, natural language inference, reading\ncomprehension, machine translation, and math problem-solving. Our study\ngenerates 4,788 adversarial prompts, meticulously evaluated over 8 tasks and 13\ndatasets. Our findings demonstrate that contemporary LLMs are not robust to\nadversarial prompts. Furthermore, we present a comprehensive analysis to\nunderstand the mystery behind prompt robustness and its transferability. We\nthen offer insightful robustness analysis and pragmatic recommendations for\nprompt composition, beneficial to both researchers and everyday users.",
        "score": 1.62428879737854
      },
      {
        "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in\n  Cognition, Adaptability, Rationality and Collaboration,",
        "abstract": "Large Language Models (LLMs) have marked a significant advancement in the\nfield of natural language processing, demonstrating exceptional capabilities in\nreasoning, tool usage, and memory. As their applications extend into\nmulti-agent environments, a need has arisen for a comprehensive evaluation\nframework that captures their abilities in reasoning, planning, collaboration,\nand more. This work introduces a novel benchmarking framework specifically\ntailored to assess LLMs within multi-agent settings, providing quantitative\nmetrics to evaluate their judgment, reasoning, deception, self-awareness,\ncooperation, coordination, and rationality. We utilize games such as Chameleon\nand Undercover, alongside game theory scenarios like Cost Sharing, Multi-player\nPrisoner's Dilemma, and Public Good, to create diverse testing environments.\nOur framework is fortified with the Probabilistic Graphical Modeling (PGM)\nmethod, enhancing the LLMs' capabilities in navigating complex social and\ncognitive dimensions. The benchmark evaluates seven multi-agent systems powered\nby different LLMs, quantitatively highlighting a significant capability gap\nover threefold between the strongest, GPT-4, and the weakest, Llama-2-70B. It\nalso confirms that our PGM enhancement boosts the inherent abilities of all\nselected models by 50% on average. Our codes are released here\nhttps://github.com/cathyxl/MAgIC.",
        "score": 1.164690613746643
      },
      {
        "title": "Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent\n  Communities,",
        "abstract": "The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools.",
        "score": 1.0927691459655762
      },
      {
        "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures\n  and Contextual Information,",
        "abstract": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in\nvarious applications. However, these models are susceptible to adversarial\nprompt attacks, where attackers can carefully curate input strings that mislead\nLLMs into generating incorrect or undesired outputs. Previous work has revealed\nthat with relatively simple yet effective attacks based on discrete\noptimization, it is possible to generate adversarial prompts that bypass\nmoderation and alignment of the models. This vulnerability to adversarial\nprompts underscores a significant concern regarding the robustness and\nreliability of LLMs. Our work aims to address this concern by introducing a\nnovel approach to detecting adversarial prompts at a token level, leveraging\nthe LLM's capability to predict the next token's probability. We measure the\ndegree of the model's perplexity, where tokens predicted with high probability\nare considered normal, and those exhibiting high perplexity are flagged as\nadversarial. Additionaly, our method also integrates context understanding by\nincorporating neighboring token information to encourage the detection of\ncontiguous adversarial prompt sequences. To this end, we design two algorithms\nfor adversarial prompt detection: one based on optimization techniques and\nanother on Probabilistic Graphical Models (PGM). Both methods are equipped with\nefficient solving methods, ensuring efficient adversarial prompt detection. Our\ntoken-level detection result can be visualized as heatmap overlays on the text\nsequence, allowing for a clearer and more intuitive representation of which\npart of the text may contain adversarial prompts.",
        "score": 1.004570722579956
      },
      {
        "title": "Backdoor Activation Attack: Attack Large Language Models using\n  Activation Steering for Safety-Alignment,",
        "abstract": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are\nspecifically trained to ensure alignment, which refers to making models behave\nin accordance with human intentions. While these models have demonstrated\ncommendable results on various safety benchmarks, the vulnerability of their\nsafety alignment has not been extensively studied. This is particularly\ntroubling given the potential harm that LLMs can inflict. Existing attack\nmethods on LLMs often rely on poisoned training data or the injection of\nmalicious prompts. These approaches compromise the stealthiness and\ngeneralizability of the attacks, making them susceptible to detection.\nAdditionally, these models often demand substantial computational resources for\nimplementation, making them less practical for real-world applications.\nInspired by recent success in modifying model behavior through steering vectors\nwithout the need for optimization, and drawing on its effectiveness in\nred-teaming LLMs, we conducted experiments employing activation steering to\ntarget four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness\n- across a varied set of attack settings. To establish a universal attack\nstrategy applicable to diverse target alignments without depending on manual\nanalysis, we automatically select the intervention layer based on contrastive\nlayer search. Our experiment results show that activation attacks are highly\neffective and add little or no overhead to attack efficiency. Additionally, we\ndiscuss potential countermeasures against such activation attacks. Our code and\ndata are available at https://github.com/wang2226/Backdoor-Activation-Attack\nWarning: this paper contains content that can be offensive or upsetting.",
        "score": 0.8221297264099121
      },
      {
        "title": "Learning diverse attacks on large language models for robust red-teaming\n  and safety tuning,",
        "abstract": "Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches.",
        "score": 0.81081622838974
      },
      {
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming\n  and Robust Refusal,",
        "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.",
        "score": 0.7919361591339111
      },
      {
        "title": "Ignore Previous Prompt: Attack Techniques For Language Models,",
        "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation\nfor natural language tasks in large-scale customer-facing applications.\nHowever, studies that explore their vulnerabilities emerging from malicious\nuser interaction are scarce. By proposing PromptInject, a prosaic alignment\nframework for mask-based iterative adversarial prompt composition, we examine\nhow GPT-3, the most widely deployed language model in production, can be easily\nmisaligned by simple handcrafted inputs. In particular, we investigate two\ntypes of attacks -- goal hijacking and prompt leaking -- and demonstrate that\neven low-aptitude, but sufficiently ill-intentioned agents, can easily exploit\nGPT-3's stochastic nature, creating long-tail risks. The code for PromptInject\nis available at https://github.com/agencyenterprise/PromptInject.",
        "score": 0.4842507243156433
      },
      {
        "title": "Towards More Realistic Extraction Attacks: An Adversarial Perspective,",
        "abstract": "Language models are prone to memorizing large parts of their training data,\nmaking them vulnerable to extraction attacks. Existing research on these\nattacks remains limited in scope, often studying isolated trends rather than\nthe real-world interactions with these models. In this paper, we revisit\nextraction attacks from an adversarial perspective, exploiting the brittleness\nof language models. We find significant churn in extraction attack trends,\ni.e., even minor, unintuitive changes to the prompt, or targeting smaller\nmodels and older checkpoints, can exacerbate the risks of extraction by up to\n$2-4 \\times$. Moreover, relying solely on the widely accepted verbatim match\nunderestimates the extent of extracted information, and we provide various\nalternatives to more accurately capture the true risks of extraction. We\nconclude our discussion with data deduplication, a commonly suggested\nmitigation strategy, and find that while it addresses some memorization\nconcerns, it remains vulnerable to the same escalation of extraction risks\nagainst a real-world adversary. Our findings highlight the necessity of\nacknowledging an adversary's true capabilities to avoid underestimating\nextraction risks.",
        "score": 0.45662248134613037
      },
      {
        "title": "Securing Multi-turn Conversational Language Models Against Distributed\n  Backdoor Triggers,",
        "abstract": "The security of multi-turn conversational large language models (LLMs) is\nunderstudied despite it being one of the most popular LLM utilization.\nSpecifically, LLMs are vulnerable to data poisoning backdoor attacks, where an\nadversary manipulates the training data to cause the model to output malicious\nresponses to predefined triggers. Specific to the multi-turn dialogue setting,\nLLMs are at the risk of even more harmful and stealthy backdoor attacks where\nthe backdoor triggers may span across multiple utterances, giving lee-way to\ncontext-driven attacks. In this paper, we explore a novel distributed backdoor\ntrigger attack that serves to be an extra tool in an adversary's toolbox that\ncan interface with other single-turn attack strategies in a plug and play\nmanner. Results on two representative defense mechanisms indicate that\ndistributed backdoor triggers are robust against existing defense strategies\nwhich are designed for single-turn user-model interactions, motivating us to\npropose a new defense strategy for the multi-turn dialogue setting that is more\nchallenging. To this end, we also explore a novel contrastive decoding based\ndefense that is able to mitigate the backdoor with a low computational\ntradeoff.",
        "score": 0.2713647782802582
      },
      {
        "title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden\n  Harms and Biases,",
        "abstract": "Red-teaming has been a widely adopted way to evaluate the harmfulness of\nLarge Language Models (LLMs). It aims to jailbreak a model's safety behavior to\nmake it act as a helpful agent disregarding the harmfulness of the query.\nExisting methods are primarily based on input text-based red-teaming such as\nadversarial prompts, low-resource prompts, or contextualized prompts to\ncondition the model in a way to bypass its safe behavior. Bypassing the\nguardrails uncovers hidden harmful information and biases in the model that are\nleft untreated or newly introduced by its safety training. However,\nprompt-based attacks fail to provide such a diagnosis owing to their low attack\nsuccess rate, and applicability to specific models. In this paper, we present a\nnew perspective on LLM safety research i.e., parametric red-teaming through\nUnalignment. It simply (instruction) tunes the model parameters to break model\nguardrails that are not deeply rooted in the model's behavior. Unalignment\nusing as few as 100 examples can significantly bypass commonly referred to as\nCHATGPT, to the point where it responds with an 88% success rate to harmful\nqueries on two safety benchmark datasets. On open-source models such as\nVICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more\nthan 91%. On bias evaluations, Unalignment exposes inherent biases in\nsafety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's\nresponses are strongly biased and opinionated 64% of the time.",
        "score": 0.25116926431655884
      },
      {
        "title": "How do humans perceive adversarial text? A reality check on the validity\n  and naturalness of word-based adversarial attacks,",
        "abstract": "Natural Language Processing (NLP) models based on Machine Learning (ML) are\nsusceptible to adversarial attacks -- malicious algorithms that imperceptibly\nmodify input text to force models into making incorrect predictions. However,\nevaluations of these attacks ignore the property of imperceptibility or study\nit under limited settings. This entails that adversarial perturbations would\nnot pass any human quality gate and do not represent real threats to\nhuman-checked NLP systems. To bypass this limitation and enable proper\nassessment (and later, improvement) of NLP model robustness, we have surveyed\n378 human participants about the perceptibility of text adversarial examples\nproduced by state-of-the-art methods. Our results underline that existing text\nattacks are impractical in real-world scenarios where humans are involved. This\ncontrasts with previous smaller-scale human studies, which reported overly\noptimistic conclusions regarding attack success. Through our work, we hope to\nposition human perceptibility as a first-class success criterion for text\nattacks, and provide guidance for research to build effective attack algorithms\nand, in turn, design appropriate defence mechanisms.",
        "score": 0.15652720630168915
      },
      {
        "title": "garak: A Framework for Security Probing Large Language Models,",
        "abstract": "As Large Language Models (LLMs) are deployed and integrated into thousands of\napplications, the need for scalable evaluation of how models respond to\nadversarial attacks grows rapidly. However, LLM security is a moving target:\nmodels produce unpredictable output, are constantly updated, and the potential\nadversary is highly diverse: anyone with access to the internet and a decent\ncommand of natural language. Further, what constitutes a security weak in one\ncontext may not be an issue in a different context; one-fits-all guardrails\nremain theoretical. In this paper, we argue that it is time to rethink what\nconstitutes ``LLM security'', and pursue a holistic approach to LLM security\nevaluation, where exploration and discovery of issues are central. To this end,\nthis paper introduces garak (Generative AI Red-teaming and Assessment Kit), a\nframework which can be used to discover and identify vulnerabilities in a\ntarget LLM or dialog system. garak probes an LLM in a structured fashion to\ndiscover potential vulnerabilities. The outputs of the framework describe a\ntarget model's weaknesses, contribute to an informed discussion of what\ncomposes vulnerabilities in unique contexts, and can inform alignment and\npolicy discussions for LLM deployment.",
        "score": 0.05617658048868179
      },
      {
        "title": "Uncertainty is Fragile: Manipulating Uncertainty in Large Language\n  Models,",
        "abstract": "Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.",
        "score": -0.07295151799917221
      },
      {
        "title": "JAB: Joint Adversarial Prompting and Belief Augmentation,",
        "abstract": "With the recent surge of language models in different applications, attention\nto safety and robustness of these models has gained significant importance.\nHere we introduce a joint framework in which we simultaneously probe and\nimprove the robustness of a black-box target model via adversarial prompting\nand belief augmentation using iterative feedback loops. This framework utilizes\nan automated red teaming approach to probe the target model, along with a\nbelief augmenter to generate instructions for the target model to improve its\nrobustness to those adversarial probes. Importantly, the adversarial model and\nthe belief generator leverage the feedback from past interactions to improve\nthe effectiveness of the adversarial prompts and beliefs, respectively. In our\nexperiments, we demonstrate that such a framework can reduce toxic content\ngeneration both in dynamic cases where an adversary directly interacts with a\ntarget model and static cases where we use a static benchmark dataset to\nevaluate our model.",
        "score": -1.4304059743881226
      }
    ]
  },
  {
    "title": "Cross-Lingual Multi-Hop Knowledge Editing -- Benchmarks, Analysis and a\n  Simple Contrastive Learning based Approach",
    "abstract": "  Large language models are often expected to constantly adapt to new sources\nof knowledge and knowledge editing techniques aim to efficiently patch the\noutdated model knowledge, with minimal modification. Most prior works focus on\nmonolingual knowledge editing in English, even though new information can\nemerge in any language from any part of the world. We propose the Cross-Lingual\nMulti-Hop Knowledge Editing paradigm, for measuring and analyzing the\nperformance of various SoTA knowledge editing techniques in a cross-lingual\nsetup. Specifically, we create a parallel cross-lingual benchmark,\nCROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive\nanalysis over various knowledge editing techniques uncover significant gaps in\nperformance between the cross-lingual and English-centric setting. Following\nthis, we propose a significantly improved system for cross-lingual multi-hop\nknowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and\ngenerate knowledge editing framework, where a retriever is formulated to recall\nedited facts and support an LLM to adhere to knowledge edits. We develop\nlanguage-aware and hard-negative based contrastive objectives for improving the\ncross-lingual and fine-grained fact retrieval and verification process used in\nthis framework. Extensive experiments on three LLMs, eight languages, and two\ndatasets show CLEVER-CKE's significant gains of up to 30% over prior methods.\n",
    "related_paper_titles": [
      "LLaMA: Open and Efficient Foundation Language Models"
    ],
    "related_paper_abstract": [
      "  We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.\n"
    ],
    "entities": [
      "Large Language",
      "PEFT",
      "GPU",
      "Retrieval Augmented",
      "ToM",
      "SQL",
      "OOD",
      "API",
      "APIs",
      "Retrieval",
      "Large Language Models Large Language Models",
      "KV",
      "SLMs",
      "KG",
      "Knowledge",
      "AGI",
      "XAI",
      "MATH",
      "Verilog",
      "Mind",
      "CLIP",
      "EHR",
      "PTQ",
      "GPUs",
      "Thought",
      "MCTS",
      "AIGC",
      "PPO",
      "Automated",
      "Copilot"
    ],
    "retrieved_papers": [
      {
        "title": "Cross-Lingual Multi-Hop Knowledge Editing -- Benchmarks, Analysis and a\n  Simple Contrastive Learning based Approach,",
        "abstract": "Large language models are often expected to constantly adapt to new sources\nof knowledge and knowledge editing techniques aim to efficiently patch the\noutdated model knowledge, with minimal modification. Most prior works focus on\nmonolingual knowledge editing in English, even though new information can\nemerge in any language from any part of the world. We propose the Cross-Lingual\nMulti-Hop Knowledge Editing paradigm, for measuring and analyzing the\nperformance of various SoTA knowledge editing techniques in a cross-lingual\nsetup. Specifically, we create a parallel cross-lingual benchmark,\nCROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive\nanalysis over various knowledge editing techniques uncover significant gaps in\nperformance between the cross-lingual and English-centric setting. Following\nthis, we propose a significantly improved system for cross-lingual multi-hop\nknowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and\ngenerate knowledge editing framework, where a retriever is formulated to recall\nedited facts and support an LLM to adhere to knowledge edits. We develop\nlanguage-aware and hard-negative based contrastive objectives for improving the\ncross-lingual and fine-grained fact retrieval and verification process used in\nthis framework. Extensive experiments on three LLMs, eight languages, and two\ndatasets show CLEVER-CKE's significant gains of up to 30% over prior methods.",
        "score": 6.463914394378662
      },
      {
        "title": "Language Anisotropic Cross-Lingual Model Editing,",
        "abstract": "Multilingual pre-trained language models can learn task-specific abilities or\nmemorize facts across multiple languages but inevitably make undesired\npredictions with specific inputs. Under similar observation, model editing aims\nto post-hoc calibrate a model targeted to specific inputs with keeping the\nmodel's raw behavior. However, existing work only studies the monolingual\nscenario, which lacks the cross-lingual transferability to perform editing\nsimultaneously across languages. In this work, we focus on cross-lingual model\nediting. Firstly, we define the cross-lingual model editing task and\ncorresponding metrics, where an edit in one language propagates to the others.\nNext, we propose a framework to naturally adapt monolingual model editing\napproaches to the cross-lingual scenario using parallel corpus. Further, we\npropose language anisotropic editing to improve cross-lingual editing by\namplifying different subsets of parameters for each language. On the newly\ndefined cross-lingual model editing task, we empirically demonstrate the\nfailure of monolingual baselines in propagating the edit to multiple languages\nand the effectiveness of the proposed language anisotropic model editing. Our\ncode is publicly available at https://github.com/franklear/LiME.",
        "score": 4.204222679138184
      },
      {
        "title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context\n  Learning,",
        "abstract": "Large language models (LLMs) possess extensive parametric knowledge, but this\nknowledge is difficult to update with new information because retraining is\nvery expensive and infeasible for closed-source models. Knowledge editing (KE)\nhas emerged as a viable solution for updating the knowledge of LLMs without\ncompromising their overall performance. On-the-fly KE methods, inspired by\nin-context learning (ICL), have shown great promise and allow LLMs to be\ntreated as black boxes. In the past, KE was primarily employed in English\ncontexts, whereas the potential for cross-lingual KE in current English-centric\nLLMs has not been fully explored. To foster more research in this direction, we\nintroduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse\nlanguages across three KE task types. We also propose a gradient-free KE method\ncalled Multilingual In-context Knowledge Editing (MIKE) and evaluate it on\nBMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms\nof reliability, generality, locality, and portability, offering valuable\ninsights and a framework for future research in cross-lingual KE. Our code and\ndata are publicly accessible via the anonymous repository at\nhttps://anonymous.4open.science/r/MIKE.",
        "score": 3.8495426177978516
      },
      {
        "title": "MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model\n  Editing,",
        "abstract": "Large language models are known for encoding a vast amount of factual\nknowledge, but they often becomes outdated due to the ever-changing nature of\nexternal information. A promising solution to this challenge is the utilization\nof model editing methods to update the knowledge in an efficient manner.\nHowever, the majority of existing model editing techniques are limited to\nmonolingual frameworks, thus failing to address the crucial issue of\ncross-lingual knowledge synchronization for multilingual models. To tackle this\nproblem, we propose a simple yet effective method that trains multilingual\npatch neuron to store cross-lingual knowledge. It can be easily adapted to\nexisting approaches to enhance their cross-lingual editing capabilities. To\nevaluate our method, we conduct experiments using both the XNLI dataset and a\nself-constructed XFEVER dataset. Experimental results demonstrate that our\nproposed method achieves improved performance in cross-lingual editing tasks\nwithout requiring excessive modifications to the original methodology, thereby\nshowcasing its user-friendly characteristics. Codes will be released soon.",
        "score": 3.8037989139556885
      },
      {
        "title": "Cross-Lingual Knowledge Editing in Large Language Models,",
        "abstract": "Knowledge editing aims to change language models' performance on several\nspecial cases (i.e., editing scope) by infusing the corresponding expected\nknowledge into them. With the recent advancements in large language models\n(LLMs), knowledge editing has been shown as a promising technique to adapt LLMs\nto new knowledge without retraining from scratch. However, most of the previous\nstudies neglect the multi-lingual nature of some main-stream LLMs (e.g., LLaMA,\nChatGPT and GPT-4), and typically focus on monolingual scenarios, where LLMs\nare edited and evaluated in the same language. As a result, it is still unknown\nthe effect of source language editing on a different target language. In this\npaper, we aim to figure out this cross-lingual effect in knowledge editing.\nSpecifically, we first collect a large-scale cross-lingual synthetic dataset by\ntranslating ZsRE from English to Chinese. Then, we conduct English editing on\nvarious knowledge editing methods covering different paradigms, and evaluate\ntheir performance in Chinese, and vice versa. To give deeper analyses of the\ncross-lingual effect, the evaluation includes four aspects, i.e., reliability,\ngenerality, locality and portability. Furthermore, we analyze the inconsistent\nbehaviors of the edited models and discuss their specific challenges. Data and\ncodes are available at https://github.com/krystalan/Bi_ZsRE",
        "score": 3.7437984943389893
      },
      {
        "title": "MLaKE: Multilingual Knowledge Editing Benchmark for Large Language\n  Models,",
        "abstract": "The extensive utilization of large language models (LLMs) underscores the\ncrucial necessity for precise and contemporary knowledge embedded within their\nintrinsic parameters. Existing research on knowledge editing primarily\nconcentrates on monolingual scenarios, neglecting the complexities presented by\nmultilingual contexts and multi-hop reasoning. To address these challenges, our\nstudy introduces MLaKE (Multilingual Language Knowledge Editing), a novel\nbenchmark comprising 4072 multi-hop and 5360 single-hop questions designed to\nevaluate the adaptability of knowledge editing methods across five languages:\nEnglish, Chinese, Japanese, French, and German. MLaKE aggregates fact chains\nfrom Wikipedia across languages and utilizes LLMs to generate questions in both\nfree-form and multiple-choice. We evaluate the multilingual knowledge editing\ngeneralization capabilities of existing methods on MLaKE. Existing knowledge\nediting methods demonstrate higher success rates in English samples compared to\nother languages. However, their generalization capabilities are limited in\nmulti-language experiments. Notably, existing knowledge editing methods often\nshow relatively high generalization for languages within the same language\nfamily compared to languages from different language families. These results\nunderscore the imperative need for advancements in multilingual knowledge\nediting and we hope MLaKE can serve as a valuable resource for benchmarking and\nsolution development.",
        "score": 3.6461751461029053
      },
      {
        "title": "MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked\n  Low-Rank Adaptation,",
        "abstract": "Knowledge editing aims to adjust the knowledge within large language models\n(LLMs) to prevent their responses from becoming obsolete or inaccurate.\nHowever, existing works on knowledge editing are primarily conducted in a\nsingle language, which is inadequate for multilingual language models. In this\npaper, we focus on multilingual knowledge editing (MKE), which requires\npropagating updates across multiple languages. This necessity poses a\nsignificant challenge for the task. Furthermore, the limited availability of a\ncomprehensive dataset for MKE exacerbates this challenge, hindering progress in\nthis area. Hence, we introduce the Multilingual Knowledge Editing Benchmark\n(MKEB), a novel dataset comprising 12 languages and providing a complete\nevaluation framework. Additionally, we propose a method that enhances\nMultilingual knowledge Editing with neuron-Masked Low-Rank Adaptation (MEMLA).\nSpecifically, we identify two categories of knowledge neurons to improve\nediting precision. Moreover, we perform LoRA-based editing with neuron masks to\nefficiently modify parameters and facilitate the propagation of updates across\nmultiple languages. Experiments demonstrate that our method outperforms\nexisting baselines and significantly enhances the multi-hop reasoning\ncapability of the edited model, with minimal impact on its downstream task\nperformance. The dataset and code will be made publicly available.",
        "score": 2.9346160888671875
      },
      {
        "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions,",
        "abstract": "The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up\nto 175B) and outperforms previous model editors by a large margin.",
        "score": 2.83610200881958
      },
      {
        "title": "Stable Knowledge Editing in Large Language Models,",
        "abstract": "Efficient knowledge editing of large language models is crucial for replacing\nobsolete information or incorporating specialized knowledge on a large scale.\nHowever, previous methods implicitly assume that knowledge is localized and\nisolated within the model, an assumption that oversimplifies the interconnected\nnature of model knowledge. The premise of localization results in an incomplete\nknowledge editing, whereas an isolated assumption may impair both other\nknowledge and general abilities. It introduces instability to the performance\nof the knowledge editing method. To transcend these assumptions, we introduce\nStableKE, a method adopts a novel perspective based on knowledge augmentation\nrather than knowledge localization. To overcome the expense of human labeling,\nStableKE integrates two automated knowledge augmentation strategies: Semantic\nParaphrase Enhancement strategy, which diversifies knowledge descriptions to\nfacilitate the teaching of new information to the model, and Contextual\nDescription Enrichment strategy, expanding the surrounding knowledge to prevent\nthe forgetting of related information. StableKE surpasses other knowledge\nediting methods, demonstrating stability both edited knowledge and multi-hop\nknowledge, while also preserving unrelated knowledge and general abilities.\nMoreover, StableKE can edit knowledge on ChatGPT.",
        "score": 2.68002986907959
      },
      {
        "title": "A Comprehensive Study of Knowledge Editing for Large Language Models,",
        "abstract": "Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.",
        "score": 2.541208267211914
      },
      {
        "title": "Learning to Edit: Aligning LLMs with Knowledge Editing,",
        "abstract": "Knowledge editing techniques, aiming to efficiently modify a minor proportion\nof knowledge in large language models (LLMs) without negatively impacting\nperformance across other inputs, have garnered widespread attention. However,\nexisting methods predominantly rely on memorizing the updated knowledge,\nimpeding LLMs from effectively combining the new knowledge with their inherent\nknowledge when answering questions. To this end, we propose a Learning to Edit\n(LTE) framework, focusing on teaching LLMs to apply updated knowledge into\ninput questions, inspired by the philosophy of \"Teach a man to fish.\" LTE\nfeatures a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on\na meticulously curated parallel dataset to make reliable, in-scope edits while\npreserving out-of-scope information and linguistic proficiency; and (ii) the\nInference Phase, which employs a retrieval-based mechanism for real-time and\nmass knowledge editing. By comparing our approach with seven advanced baselines\nacross four popular knowledge editing benchmarks and two LLM architectures, we\ndemonstrate LTE's superiority in knowledge editing performance, robustness in\nboth batch and sequential editing, minimal interference on general tasks, and\nrapid editing speeds. The data and code are available at\nhttps://github.com/YJiangcm/LTE.",
        "score": 2.259631395339966
      },
      {
        "title": "Time Sensitive Knowledge Editing through Efficient Finetuning,",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capability in\ndifferent tasks and are bringing transformative changes to many domains.\nHowever, keeping the knowledge in LLMs up-to-date remains a challenge once\npretraining is complete. It is thus essential to design effective methods to\nboth update obsolete knowledge and induce new knowledge into LLMs. Existing\nlocate-and-edit knowledge editing (KE) method suffers from two limitations.\nFirst, the post-edit LLMs by such methods generally have poor capability in\nanswering complex queries that require multi-hop reasoning. Second, the long\nrun-time of such locate-and-edit methods to perform knowledge edits make it\ninfeasible for large scale KE in practice. In this paper, we explore\nParameter-Efficient Fine-Tuning (PEFT) techniques as an alternative for KE. We\ncurate a more comprehensive temporal KE dataset with both knowledge update and\nknowledge injection examples for KE performance benchmarking. We further probe\nthe effect of fine-tuning on a range of layers in an LLM for the multi-hop QA\ntask. We find that PEFT performs better than locate-and-edit techniques for\ntime-sensitive knowledge edits.",
        "score": 1.9297304153442383
      },
      {
        "title": "InstructEdit: Instruction-based Knowledge Editing for Large Language\n  Models,",
        "abstract": "Knowledge editing for large language models can offer an efficient solution\nto alter a model's behavior without negatively impacting the overall\nperformance. However, the current approaches encounter issues with limited\ngeneralizability across tasks, necessitating one distinct editor for each task,\nsignificantly hindering the broader applications. To address this, we take the\nfirst step to analyze the multi-task generalization issue in knowledge editing.\nSpecifically, we develop an instruction-based editing technique, termed\nInstructEdit, which facilitates the editor's adaptation to various task\nperformances simultaneously using simple instructions. With only one unified\neditor for each LLM, we empirically demonstrate that InstructEdit can improve\nthe editor's control, leading to an average 14.86% increase in Reliability in\nmulti-task editing setting. Furthermore, experiments involving holdout unseen\ntask illustrate that InstructEdit consistently surpass previous strong\nbaselines. To further investigate the underlying mechanisms of\ninstruction-based knowledge editing, we analyze the principal components of the\nediting gradient directions, which unveils that instructions can help control\noptimization direction with stronger OOD generalization. Code and datasets are\navailable in https://github.com/zjunlp/EasyEdit.",
        "score": 1.875933051109314
      },
      {
        "title": "Assessing Knowledge Editing in Language Models via Relation Perspective,",
        "abstract": "Knowledge Editing (KE) for modifying factual knowledge in Large Language\nModels (LLMs) has been receiving increasing attention. However, existing\nknowledge editing methods are entity-centric, and it is unclear whether this\napproach is suitable for a relation-centric perspective. To address this gap,\nthis paper constructs a new benchmark named RaKE, which focuses on Relation\nbased Knowledge Editing. In this paper, we establish a suite of innovative\nmetrics for evaluation and conduct comprehensive experiments involving various\nknowledge editing baselines. We notice that existing knowledge editing methods\nexhibit the potential difficulty in their ability to edit relations. Therefore,\nwe further explore the role of relations in factual triplets within the\ntransformer. Our research results confirm that knowledge related to relations\nis not only stored in the FFN network but also in the attention layers. This\nprovides experimental support for future relation-based knowledge editing\nmethods.",
        "score": 1.7433303594589233
      },
      {
        "title": "Propagation and Pitfalls: Reasoning-based Assessment of Knowledge\n  Editing through Counterfactual Tasks,",
        "abstract": "Current approaches of knowledge editing struggle to effectively propagate\nupdates to interconnected facts. In this work, we delve into the barriers that\nhinder the appropriate propagation of updated knowledge within these models for\naccurate reasoning. To support our analysis, we introduce a novel\nreasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing\ndataset) -- which covers six common reasoning schemes in real world. We conduct\na thorough analysis of existing knowledge editing techniques, including input\naugmentation, finetuning, and locate-and-edit. We found that all model editing\nmethods show notably low performance on this dataset, especially in certain\nreasoning schemes. Our analysis over the chain-of-thought generation of edited\nmodels further uncover key reasons behind the inadequacy of existing knowledge\nediting methods from a reasoning standpoint, involving aspects on fact-wise\nediting, fact recall ability, and coherence in generation. We will make our\nbenchmark publicly available.",
        "score": 1.7161006927490234
      },
      {
        "title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of\n  Knowledge Editing in Large Language Models,",
        "abstract": "Knowledge editing is a rising technique for efficiently updating factual\nknowledge in Large Language Models (LLMs) with minimal alteration of\nparameters. However, recent studies have identified concerning side effects,\nsuch as knowledge distortion and the deterioration of general abilities, that\nhave emerged after editing. This survey presents a comprehensive study of these\nside effects, providing a unified view of the challenges associated with\nknowledge editing in LLMs. We discuss related works and summarize potential\nresearch directions to overcome these limitations. Our work highlights the\nlimitations of current knowledge editing methods, emphasizing the need for\ndeeper understanding of inner knowledge structures of LLMs and improved\nknowledge editing methods. To foster future research, we have released the\ncomplementary materials such as paper collection publicly at\nhttps://github.com/MiuLab/EditLLM-Survey",
        "score": 1.6516010761260986
      },
      {
        "title": "Knowledge Graph Enhanced Large Language Model Editing,",
        "abstract": "Large language models (LLMs) are pivotal in advancing natural language\nprocessing (NLP) tasks, yet their efficacy is hampered by inaccuracies and\noutdated knowledge. Model editing emerges as a promising solution to address\nthese challenges. However, existing editing methods struggle to track and\nincorporate changes in knowledge associated with edits, which limits the\ngeneralization ability of postedit LLMs in processing edited knowledge. To\ntackle these problems, we propose a novel model editing method that leverages\nknowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we\nfirst utilize a knowledge graph augmentation module to uncover associated\nknowledge that has changed due to editing, obtaining its internal\nrepresentations within LLMs. This approach allows knowledge alterations within\nLLMs to be reflected through an external graph structure. Subsequently, we\ndesign a graph-based knowledge edit module to integrate structured knowledge\ninto the model editing. This ensures that the updated parameters reflect not\nonly the modifications of the edited knowledge but also the changes in other\nassociated knowledge resulting from the editing process. Comprehensive\nexperiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME\nsignificantly improves the generalization capabilities of post-edit LLMs in\nemploying edited knowledge.",
        "score": 1.6414306163787842
      },
      {
        "title": "Retrieval-augmented Multilingual Knowledge Editing,",
        "abstract": "Knowledge represented in Large Language Models (LLMs) is quite often\nincorrect and can also become obsolete over time. Updating knowledge via\nfine-tuning is computationally resource-hungry and not reliable, and so\nknowledge editing (KE) has developed as an effective and economical alternative\nto inject new knowledge or to fix factual errors in LLMs. Although there has\nbeen considerable interest in this area, current KE research exclusively\nfocuses on the monolingual setting, typically in English. However, what happens\nif the new knowledge is supplied in one language, but we would like to query\nthe LLM in a different language? To address the problem of multilingual\nknowledge editing, we propose Retrieval-augmented Multilingual Knowledge Editor\n(ReMaKE) to update new knowledge in LLMs. ReMaKE can perform model-agnostic\nknowledge editing in multilingual settings. ReMaKE concatenates the new\nknowledge retrieved from a multilingual knowledge base with prompts. Our\nexperimental results show that ReMaKE outperforms baseline knowledge editing\nmethods by a significant margin and is the first KE method to work in a\nmultilingual setting. We provide our multilingual knowledge editing dataset\n(MzsRE) in 12 languages, which along with code, and additional project\ninformation is available at https://github.com/Vicky-Wil/ReMaKE.",
        "score": 1.572399616241455
      },
      {
        "title": "Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs,",
        "abstract": "Large language models (LLMs) possess a wealth of knowledge encoded in their\nparameters. However, this knowledge may become outdated or unsuitable over\ntime. As a result, there has been a growing interest in knowledge editing for\nLLMs and evaluating its effectiveness. Existing studies primarily focus on\nknowledge editing using factual triplets, which not only incur high costs for\ncollection but also struggle to express complex facts. Furthermore, these\nstudies are often limited in their evaluation perspectives. In this paper, we\npropose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs.\nThis benchmark includes an evaluation framework and a corresponding dataset.\nUnder our framework, we first ask the LLM to perform knowledge editing using\nraw documents, which provides a more convenient and universal approach compared\nto using factual triplets. We then evaluate the updated LLM from multiple\nperspectives. In addition to assessing the effectiveness of knowledge editing\nand the retention of unrelated knowledge from conventional studies, we further\ntest the LLM's ability in two aspects: 1) Reasoning with the altered knowledge,\naiming for the LLM to genuinely learn the altered knowledge instead of simply\nmemorizing it. 2) Cross-lingual knowledge transfer, where the LLM updated with\nraw documents in one language should be capable of handling queries from\nanother language. To facilitate further research, we construct and release the\ncorresponding dataset. Using this benchmark, we investigate the effectiveness\nof several commonly-used knowledge editing methods. Experimental results\nindicate that the current methods for knowledge editing using raw documents are\nnot effective in yielding satisfactory results, particularly when it comes to\nreasoning with altered knowledge and cross-lingual knowledge transfer.",
        "score": 1.5282543897628784
      },
      {
        "title": "Updating Language Models with Unstructured Facts: Towards Practical\n  Knowledge Editing,",
        "abstract": "Knowledge editing aims to inject knowledge updates into language models to\nkeep them correct and up-to-date. However, its current evaluation strategies\nare notably impractical: they solely update with well-curated structured facts\n(triplets with subjects, relations, and objects), whereas real-world knowledge\nupdates commonly emerge in unstructured texts like news articles. In this\npaper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It\nevaluates editing performance directly using unstructured texts as knowledge\nupdates, termed unstructured facts. Hence UKE avoids the laborious construction\nof structured facts and enables efficient and responsive knowledge editing,\nbecoming a more practical benchmark. We conduct extensive experiments on newly\nbuilt datasets and demonstrate that UKE poses a significant challenge to\nstate-of-the-art knowledge editing methods, resulting in their critical\nperformance declines. We further show that this challenge persists even if we\nextract triplets as structured facts. Our analysis discloses key insights to\nmotivate future research in UKE for more practical knowledge editing.",
        "score": 1.5026359558105469
      },
      {
        "title": "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models,",
        "abstract": "Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners from applying knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub,\nalong with Google Colab tutorials and comprehensive documentation for beginners\nto get started. Besides, we present an online system for real-time knowledge\nediting, and a demo video.",
        "score": 1.4340355396270752
      },
      {
        "title": "Event-level Knowledge Editing,",
        "abstract": "Knowledge editing aims at updating knowledge of large language models (LLMs)\nto prevent them from becoming outdated. Existing work edits LLMs at the level\nof factual knowledge triplets. However, natural knowledge updates in the real\nworld come from the occurrences of new events rather than direct changes in\nfactual triplets. In this paper, we propose a new task setting: event-level\nknowledge editing, which directly edits new events into LLMs and improves over\nconventional triplet-level editing on (1) Efficiency. A single event edit leads\nto updates in multiple entailed knowledge triplets. (2) Completeness. Beyond\nupdating factual knowledge, event-level editing also requires considering the\nevent influences and updating LLMs' knowledge about future trends. We construct\na high-quality event-level editing benchmark ELKEN, consisting of 1,515 event\nedits, 6,449 questions about factual knowledge, and 10,150 questions about\nfuture tendencies. We systematically evaluate the performance of various\nknowledge editing methods and LLMs on this benchmark. We find that ELKEN poses\nsignificant challenges to existing knowledge editing approaches. Our codes and\ndataset are publicly released to facilitate further research.",
        "score": 1.3942080736160278
      },
      {
        "title": "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons,",
        "abstract": "Multilingual knowledge editing (MKE) aims to simultaneously revise factual\nknowledge across multilingual languages within large language models (LLMs).\nHowever, most existing MKE methods just adapt existing monolingual editing\nmethods to multilingual scenarios, overlooking the deep semantic connections of\nthe same factual knowledge between different languages, thereby limiting edit\nperformance. To address this issue, we first investigate how LLMs represent\nmultilingual factual knowledge and discover that the same factual knowledge in\ndifferent languages generally activates a shared set of neurons, which we call\nlanguage-agnostic factual neurons. These neurons represent the semantic\nconnections between multilingual knowledge and are mainly located in certain\nlayers. Inspired by this finding, we propose a new MKE method by locating and\nmodifying Language-Agnostic Factual Neurons (LAFN) to simultaneously edit\nmultilingual knowledge. Specifically, we first generate a set of paraphrases\nfor each multilingual knowledge to be edited to precisely locate the\ncorresponding language-agnostic factual neurons. Then we optimize the update\nvalues for modifying these located neurons to achieve simultaneous modification\nof the same factual knowledge in multiple languages. Experimental results on\nBi-ZsRE and MzsRE benchmarks demonstrate that our method outperforms existing\nMKE methods and achieves remarkable edit performance, indicating the importance\nof considering the semantic connections among multilingual knowledge.",
        "score": 1.0547517538070679
      },
      {
        "title": "Editing Conceptual Knowledge for Large Language Models,",
        "abstract": "Recently, there has been a growing interest in knowledge editing for Large\nLanguage Models (LLMs). Current approaches and evaluations merely explore the\ninstance-level editing, while whether LLMs possess the capability to modify\nconcepts remains unclear. This paper pioneers the investigation of editing\nconceptual knowledge for LLMs, by constructing a novel benchmark dataset\nConceptEdit and establishing a suite of new metrics for evaluation. The\nexperimental results reveal that, although existing editing methods can\nefficiently modify concept-level definition to some extent, they also have the\npotential to distort the related instantial knowledge in LLMs, leading to poor\nperformance. We anticipate this can inspire further progress in better\nunderstanding LLMs. Our project homepage is available at\nhttps://zjunlp.github.io/project/ConceptEdit.",
        "score": 1.0259984731674194
      },
      {
        "title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?,",
        "abstract": "As large language models (LLMs) are widely deployed, targeted editing of\ntheir knowledge has become a critical challenge. Recently, advancements in\nmodel editing techniques, such as Rank-One Model Editing (ROME), have paved the\nway for updating LLMs with new knowledge. However, the efficacy of these\nmethods varies across different types of knowledge. This study investigates the\ncapability of knowledge editing methods to incorporate new knowledge with\nvarying degrees of \"perplexingness\", a term we use to describe the initial\ndifficulty LLMs have in understanding new concepts. We begin by quantifying the\n\"perplexingness\" of target knowledge using pre-edit conditional probabilities,\nand assess the efficacy of edits through post-edit conditional probabilities.\nUtilizing the widely-used CounterFact dataset, we find significant negative\ncorrelations between the \"perplexingness\" of the new knowledge and the edit\nefficacy across all 12 scenarios. To dive deeper into this phenomenon, we\nintroduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym\npairs across diverse categories. Our analysis reveal that more abstract\nconcepts (hypernyms) tend to be more perplexing than their specific\ncounterparts (hyponyms). Further exploration into the influence of knowledge\nhierarchy on editing outcomes indicates that knowledge positioned at higher\nhierarchical levels is more challenging to modify in some scenarios. Our\nresearch highlights a previously overlooked aspect of LLM editing: the variable\nefficacy of editing methods in handling perplexing knowledge. By revealing how\nhierarchical relationships can influence editing outcomes, our findings offer\nnew insights into the challenges of updating LLMs and pave the way for more\nnuanced approaches to model editing in the future.",
        "score": 0.8714523911476135
      },
      {
        "title": "KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large\n  Language Models,",
        "abstract": "Large language models (LLMs) demonstrate remarkable performance on\nknowledge-intensive tasks, suggesting that real-world knowledge is encoded in\ntheir model parameters. However, besides explorations on a few probing tasks in\nlimited knowledge domains, it is not well understood how to evaluate LLMs'\nknowledge systematically and how well their knowledge abilities generalize,\nacross a spectrum of knowledge domains and progressively complex task formats.\nTo this end, we propose KGQuiz, a knowledge-intensive benchmark to\ncomprehensively investigate the knowledge generalization abilities of LLMs.\nKGQuiz is a scalable framework constructed from triplet-based knowledge, which\ncovers three knowledge domains and consists of five tasks with increasing\ncomplexity: true-or-false, multiple-choice QA, blank filling, factual editing,\nand open-ended knowledge generation. To gain a better understanding of LLMs'\nknowledge abilities and their generalization, we evaluate 10 open-source and\nblack-box LLMs on the KGQuiz benchmark across the five knowledge-intensive\ntasks and knowledge domains. Extensive experiments demonstrate that LLMs\nachieve impressive performance in straightforward knowledge QA tasks, while\nsettings and contexts requiring more complex reasoning or employing\ndomain-specific facts still present significant challenges. We envision KGQuiz\nas a testbed to analyze such nuanced variations in performance across domains\nand task formats, and ultimately to understand, evaluate, and improve LLMs'\nknowledge abilities across a wide spectrum of knowledge domains and tasks.",
        "score": 0.836879312992096
      },
      {
        "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models,",
        "abstract": "As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.",
        "score": 0.7360166907310486
      },
      {
        "title": "DUnE: Dataset for Unified Editing,",
        "abstract": "Even the most advanced language models remain susceptible to errors\nnecessitating to modify these models without initiating a comprehensive\nretraining process. Model editing refers to the modification of a model's\nknowledge or representations in a manner that produces the desired outcomes.\nPrior research primarily centered around editing factual data e.g. \"Messi plays\nfor Inter Miami\" confining the definition of an edit to a knowledge triplet\ni.e. (subject, object, relation). However, as the applications of language\nmodels expand, so do the diverse ways in which we wish to edit and refine their\noutputs. In this study, we broaden the scope of the editing problem to include\nan array of editing cases such as debiasing and rectifying reasoning errors and\ndefine an edit as any natural language expression that solicits a change in the\nmodel's outputs. We are introducing DUnE-an editing benchmark where edits are\nnatural language sentences and propose that DUnE presents a challenging yet\nrelevant task. To substantiate this claim, we conduct an extensive series of\nexperiments testing various editing approaches to address DUnE, demonstrating\ntheir respective strengths and weaknesses. We show that retrieval-augmented\nlanguage modeling can outperform specialized editing techniques and neither set\nof approaches has fully solved the generalized editing problem covered by our\nbenchmark.",
        "score": 0.607912003993988
      },
      {
        "title": "Knowledge Editing for Large Language Models: A Survey,",
        "abstract": "Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.",
        "score": 0.30882397294044495
      },
      {
        "title": "Editing Language Model-based Knowledge Graph Embeddings,",
        "abstract": "Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hypernetwork to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.",
        "score": -0.9624717235565186
      }
    ]
  },
  {
    "title": "Analyzing Cultural Representations of Emotions in LLMs through Mixed\n  Emotion Survey",
    "abstract": "  Large Language Models (LLMs) have gained widespread global adoption,\nshowcasing advanced linguistic capabilities across multiple of languages. There\nis a growing interest in academia to use these models to simulate and study\nhuman behaviors. However, it is crucial to acknowledge that an LLM's\nproficiency in a specific language might not fully encapsulate the norms and\nvalues associated with its culture. Concerns have emerged regarding potential\nbiases towards Anglo-centric cultures and values due to the predominance of\nWestern and US-based training data. This study focuses on analyzing the\ncultural representations of emotions in LLMs, in the specific case of\nmixed-emotion situations. Our methodology is based on the studies of Miyamoto\net al. (2010), which identified distinctive emotional indicators in Japanese\nand American human responses. We first administer their mixed emotion survey to\nfive different LLMs and analyze their outputs. Second, we experiment with\ncontextual variables to explore variations in responses considering both\nlanguage and speaker origin. Thirdly, we expand our investigation to encompass\nadditional East Asian and Western European origin languages to gauge their\nalignment with their respective cultures, anticipating a closer fit. We find\nthat (1) models have limited alignment with the evidence in the literature; (2)\nwritten language has greater effect on LLMs' response than information on\nparticipants origin; and (3) LLMs responses were found more similar for East\nAsian languages than Western European languages.\n",
    "related_paper_titles": [
      "BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation\n  Suite for Large Language Models",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models"
    ],
    "related_paper_abstract": [
      "  The rapid development of Large Language Models (LLMs) and the emergence of\nnovel abilities with scale have necessitated the construction of holistic,\ndiverse and challenging benchmarks such as HELM and BIG-bench. However, at the\nmoment, most of these benchmarks focus only on performance in English and\nevaluations that include Southeast Asian (SEA) languages are few in number. We\ntherefore propose BHASA, a holistic linguistic and cultural evaluation suite\nfor LLMs in SEA languages. It comprises three components: (1) a NLP benchmark\ncovering eight tasks across Natural Language Understanding (NLU), Generation\n(NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit\nthat spans the gamut of linguistic phenomena including syntax, semantics and\npragmatics, and (3) a cultural diagnostics dataset that probes for both\ncultural representation and sensitivity. For this preliminary effort, we\nimplement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil,\nand we only include Indonesian and Tamil for LINDSEA and the cultural\ndiagnostics dataset. As GPT-4 is purportedly one of the best-performing\nmultilingual LLMs at the moment, we use it as a yardstick to gauge the\ncapabilities of LLMs in the context of SEA languages. Our initial experiments\non GPT-4 with BHASA find it lacking in various aspects of linguistic\ncapabilities, cultural representation and sensitivity in the targeted SEA\nlanguages. BHASA is a work in progress and will continue to be improved and\nexpanded in the future. The repository for this paper can be found at:\nhttps://github.com/aisingapore/BHASA\n",
      "  In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.\n",
      "  As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel\n"
    ],
    "entities": [
      "RAG",
      "Large Language Models Large Language Models",
      "Large Language",
      "Mistral",
      "Large Language Models Large",
      "CAD",
      "LLaMA",
      "ESG",
      "ICL",
      "Adam",
      "KGE",
      "Llama",
      "DPO",
      "DTs",
      "SVM",
      "ANN",
      "CoT",
      "NLG",
      "WSIs",
      "Verilog",
      "RLHF",
      "Retrieval Augmented",
      "MATH",
      "APIs",
      "OpenAI",
      "Gemini",
      "Human Feedback",
      "Retrieval",
      "KV",
      "Large Language Model"
    ],
    "retrieved_papers": [
      {
        "title": "Are Generative Language Models Multicultural? A Study on Hausa Culture\n  and Emotions using ChatGPT,",
        "abstract": "Large Language Models (LLMs), such as ChatGPT, are widely used to generate\ncontent for various purposes and audiences. However, these models may not\nreflect the cultural and emotional diversity of their users, especially for\nlow-resource languages. In this paper, we investigate how ChatGPT represents\nHausa's culture and emotions. We compare responses generated by ChatGPT with\nthose provided by native Hausa speakers on 37 culturally relevant questions. We\nconducted experiments using emotion analysis and applied two similarity metrics\nto measure the alignment between human and ChatGPT responses. We also collected\nhuman participants ratings and feedback on ChatGPT responses. Our results show\nthat ChatGPT has some level of similarity to human responses, but also exhibits\nsome gaps and biases in its knowledge and awareness of the Hausa culture and\nemotions. We discuss the implications and limitations of our methodology and\nanalysis and suggest ways to improve the performance and evaluation of LLMs for\nlow-resource languages.",
        "score": 2.9268949031829834
      },
      {
        "title": "NormAd: A Benchmark for Measuring the Cultural Adaptability of Large\n  Language Models,",
        "abstract": "The integration of large language models (LLMs) into various global cultures\nfundamentally presents a challenge: LLMs must navigate interactions, respect\nsocial norms, and avoid transgressing cultural boundaries. However, it is still\nunclear if LLMs can adapt their outputs to diverse cultural norms. Our study\nfocuses on this aspect. We introduce NormAd, a novel dataset, which includes\n2.6k stories that represent social and cultural norms from 75 countries, to\nassess the ability of LLMs to adapt to different granular levels of\nsocio-cultural contexts such as the country of origin, its associated cultural\nvalues, and prevalent social norms. Our study reveals that LLMs struggle with\ncultural reasoning across all contextual granularities, showing stronger\nadaptability to English-centric cultures over those from the Global South. Even\nwith explicit social norms, the top-performing model, Mistral-7b-Instruct,\nachieves only 81.8% accuracy, lagging behind the 95.6% achieved by humans.\nEvaluation on NormAd further reveals that LLMs struggle to adapt to stories\ninvolving gift-giving across cultures. Due to inherent agreement or sycophancy\nbiases, LLMs find it considerably easier to assess the social acceptability of\nstories that adhere to norms than those that deviate. Our benchmark measures\nthe cultural adaptability (or lack thereof) of LLMs, emphasizing the potential\nto make these technologies more equitable and useful for global audiences. We\nrelease the NormAd dataset and its associated code on GitHub.",
        "score": 2.8702340126037598
      },
      {
        "title": "Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced,\n  Low-Resource Real-World Scenarios,",
        "abstract": "The deployment of Large Language Models (LLMs) in real-world applications\npresents both opportunities and challenges, particularly in multilingual and\ncode-mixed communication settings. This research evaluates the performance of\nseven leading LLMs in sentiment analysis on a dataset derived from multilingual\nand code-mixed WhatsApp chats, including Swahili, English and Sheng. Our\nevaluation includes both quantitative analysis using metrics like F1 score and\nqualitative assessment of LLMs' explanations for their predictions. We find\nthat, while Mistral-7b and Mixtral-8x7b achieved high F1 scores, they and other\nLLMs such as GPT-3.5-Turbo, Llama-2-70b, and Gemma-7b struggled with\nunderstanding linguistic and contextual nuances, as well as lack of\ntransparency in their decision-making process as observed from their\nexplanations. In contrast, GPT-4 and GPT-4-Turbo excelled in grasping diverse\nlinguistic inputs and managing various contextual information, demonstrating\nhigh consistency with human alignment and transparency in their decision-making\nprocess. The LLMs however, encountered difficulties in incorporating cultural\nnuance especially in non-English settings with GPT-4s doing so inconsistently.\nThe findings emphasize the necessity of continuous improvement of LLMs to\neffectively tackle the challenges of culturally nuanced, low-resource\nreal-world settings and the need for developing evaluation benchmarks for\ncapturing these issues.",
        "score": 2.795769453048706
      },
      {
        "title": "Cultural Alignment in Large Language Models: An Explanatory Analysis\n  Based on Hofstede's Cultural Dimensions,",
        "abstract": "The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals and\nsocieties with diverse cultural backgrounds. While the discourse has focused\nmainly on political and social biases, our research proposes a Cultural\nAlignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's\ncultural dimension framework, which offers an explanatory cross-cultural\ncomparison through the latent variable analysis. We apply our approach to\nquantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the\ncultural dimensions of regions like the United States, China, and Arab\ncountries, using different prompting styles and exploring the effects of\nlanguage-specific fine-tuning on the models' behavioural tendencies and\ncultural values. Our results quantify the cultural alignment of LLMs and reveal\nthe difference between LLMs in explanatory cultural dimensions. Our study\ndemonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows\na unique capability to adapt to cultural nuances, particularly in Chinese\nsettings. However, it faces challenges with American and Arab cultures. The\nresearch also highlights that fine-tuning LLama 2 models with different\nlanguages changes their responses to cultural questions, emphasizing the need\nfor culturally diverse development in AI for worldwide acceptance and ethical\nuse. For more details or to contribute to this research, visit our GitHub page\nhttps://github.com/reemim/Hofstedes_CAT/",
        "score": 2.5313122272491455
      },
      {
        "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey,",
        "abstract": "We present a survey of more than 90 recent papers that aim to study cultural\nrepresentation and inclusion in large language models (LLMs). We observe that\nnone of the studies explicitly define \"culture, which is a complex,\nmultifaceted concept; instead, they probe the models on some specially designed\ndatasets which represent certain aspects of \"culture\". We call these aspects\nthe proxies of culture, and organize them across two dimensions of demographic\nand semantic proxies. We also categorize the probing methods employed. Our\nanalysis indicates that only certain aspects of ``culture,'' such as values and\nobjectives, have been studied, leaving several other interesting and important\nfacets, especially the multitude of semantic domains (Thompson et al., 2020)\nand aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps\nare the lack of robustness of probing techniques and situated studies on the\nimpact of cultural mis- and under-representation in LLM-based applications.",
        "score": 2.461859941482544
      },
      {
        "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench,",
        "abstract": "Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has\nbecome increasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, i.e., how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes five LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4 and LLaMA-2. We find that,\ndespite several misalignments, LLMs can generally respond appropriately to\ncertain situations. Nevertheless, they fall short in alignment with the\nemotional behaviors of human beings and cannot establish connections between\nsimilar situations. Our collected dataset of situations, the human evaluation\nresults, and the code of our testing framework, dubbed EmotionBench, is made\nopenly accessible via https://github.com/CUHK-ARISE/EmotionBench. We aspire to\ncontribute to the advancement of LLMs regarding better alignment with the\nemotional behaviors of human beings, thereby enhancing their utility and\napplicability as intelligent assistants.",
        "score": 2.3356924057006836
      },
      {
        "title": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting,",
        "abstract": "As the utilization of large language models (LLMs) has proliferated\nworldwide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found in: https://github.com/huihanlhh/Culture-Gen/",
        "score": 2.30239200592041
      },
      {
        "title": "Cultural Value Differences of LLMs: Prompt, Language, and Model Size,",
        "abstract": "Our study aims to identify behavior patterns in cultural values exhibited by\nlarge language models (LLMs). The studied variants include question ordering,\nprompting language, and model size. Our experiments reveal that each tested LLM\ncan efficiently behave with different cultural values. More interestingly: (i)\nLLMs exhibit relatively consistent cultural values when presented with prompts\nin a single language. (ii) The prompting language e.g., Chinese or English, can\ninfluence the expression of cultural values. The same question can elicit\ndivergent cultural values when the same LLM is queried in a different language.\n(iii) Differences in sizes of the same model (e.g., Llama2-7B vs 13B vs 70B)\nhave a more significant impact on their demonstrated cultural values than model\ndifferences (e.g., Llama2 vs Mixtral). Our experiments reveal that query\nlanguage and model size of LLM are the main factors resulting in cultural value\ndifferences.",
        "score": 2.2104098796844482
      },
      {
        "title": "Multilingual Language Models are not Multicultural: A Case Study in\n  Emotion,",
        "abstract": "Emotions are experienced and expressed differently across the world. In order\nto use Large Language Models (LMs) for multilingual tasks that require\nemotional sensitivity, LMs must reflect this cultural variation in emotion. In\nthis study, we investigate whether the widely-used multilingual LMs in 2023\nreflect differences in emotional expressions across cultures and languages. We\nfind that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,\nand generative LMs (e.g., ChatGPT) reflect Western norms, even when responding\nto prompts in other languages. Our results show that multilingual LMs do not\nsuccessfully learn the culturally appropriate nuances of emotion and we\nhighlight possible research directions towards correcting this.",
        "score": 2.1070051193237305
      },
      {
        "title": "Sociolinguistically Informed Interpretability: A Case Study on Hinglish\n  Emotion Classification,",
        "abstract": "Emotion classification is a challenging task in NLP due to the inherent\nidiosyncratic and subjective nature of linguistic expression, especially with\ncode-mixed data. Pre-trained language models (PLMs) have achieved high\nperformance for many tasks and languages, but it remains to be seen whether\nthese models learn and are robust to the differences in emotional expression\nacross languages. Sociolinguistic studies have shown that Hinglish speakers\nswitch to Hindi when expressing negative emotions and to English when\nexpressing positive emotions. To understand if language models can learn these\nassociations, we study the effect of language on emotion prediction across 3\nPLMs on a Hinglish emotion classification dataset. Using LIME and token level\nlanguage ID, we find that models do learn these associations between language\nchoice and emotional expression. Moreover, having code-mixed data present in\nthe pre-training can augment that learning when task-specific data is scarce.\nWe also conclude from the misclassifications that the models may overgeneralise\nthis heuristic to other infrequent examples where this sociolinguistic\nphenomenon does not apply.",
        "score": 2.0316696166992188
      },
      {
        "title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of\n  LLM Responses Based on Hofstede Cultural Dimensions,",
        "abstract": "Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs.",
        "score": 2.015960693359375
      },
      {
        "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models,",
        "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for\nrobust, comprehensive, and challenging benchmarks. Yet, research on evaluating\ntheir Emotional Intelligence (EI) is considerably limited. Existing benchmarks\nhave two major shortcomings: first, they mainly focus on emotion recognition,\nneglecting essential EI capabilities such as emotion regulation and thought\nfacilitation through emotion understanding; second, they are primarily\nconstructed from existing datasets, which include frequent patterns, explicit\ninformation, and annotation errors, leading to unreliable evaluation. We\npropose EmoBench, a benchmark that draws upon established psychological\ntheories and proposes a comprehensive definition for machine EI, including\nEmotional Understanding and Emotional Application. EmoBench includes a set of\n400 hand-crafted questions in English and Chinese, which are meticulously\ndesigned to require thorough reasoning and understanding. Our findings reveal a\nconsiderable gap between the EI of existing LLMs and the average human,\nhighlighting a promising direction for future research. Our code and data are\npublicly available at https://github.com/Sahandfer/EmoBench.",
        "score": 1.9046647548675537
      },
      {
        "title": "Predicting Emotion Intensity in Polish Political Texts: Comparing\n  Supervised Models and Large Language Models in a Resource-Poor Language,",
        "abstract": "This study explores the use of large language models (LLMs) to predict\nemotion intensity in Polish political texts, a resource-poor language context.\nThe research compares the performance of several LLMs against a supervised\nmodel trained on an annotated corpus of 10,000 social media texts, evaluated\nfor the intensity of emotions by expert judges. The findings indicate that\nwhile the supervised model generally outperforms LLMs, offering higher accuracy\nand lower variance, LLMs present a viable alternative, especially given the\nhigh costs associated with data annotation. The study highlights the potential\nof LLMs in low-resource language settings and underscores the need for further\nresearch on emotion intensity prediction and its application across different\nlanguages and continuous features. The implications suggest a nuanced\ndecision-making process to choose the right approach to emotion prediction for\nresearchers and practitioners based on resource availability and the specific\nrequirements of their tasks.",
        "score": 1.8912609815597534
      },
      {
        "title": "Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models,",
        "abstract": "As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel",
        "score": 1.83847975730896
      },
      {
        "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in\n  Large Language Models,",
        "abstract": "This paper identifies a cultural dominance issue within large language models\n(LLMs) due to the predominant use of English data in model training (e.g.,\nChatGPT). LLMs often provide inappropriate English-culture-related answers that\nare not relevant to the expected culture when users ask in non-English\nlanguages. To systematically evaluate the cultural dominance issue, we build a\nbenchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and\nopinions) cultural objects. Empirical results show that the representative GPT\nmodels suffer from the culture dominance problem, where GPT-4 is the most\naffected while text-davinci-003 suffers the least from this problem. Our study\nemphasizes the need to critically examine cultural dominance and ethical\nconsideration in their development and deployment. We show that two\nstraightforward methods in model development (i.e., pretraining on more diverse\ndata) and deployment (e.g., culture-aware prompting) can significantly mitigate\nthe cultural dominance issue in LLMs.",
        "score": 1.82612144947052
      },
      {
        "title": "Emotion Embeddings $\\unicode{x2014}$ Learning Stable and Homogeneous\n  Abstractions from Heterogeneous Affective Datasets,",
        "abstract": "Human emotion is expressed in many communication modalities and media formats\nand so their computational study is equally diversified into natural language\nprocessing, audio signal analysis, computer vision, etc. Similarly, the large\nvariety of representation formats used in previous research to describe\nemotions (polarity scales, basic emotion categories, dimensional approaches,\nappraisal theory, etc.) have led to an ever proliferating diversity of\ndatasets, predictive models, and software tools for emotion analysis. Because\nof these two distinct types of heterogeneity, at the expressional and\nrepresentational level, there is a dire need to unify previous work on\nincreasingly diverging data and label types. This article presents such a\nunifying computational model. We propose a training procedure that learns a\nshared latent representation for emotions, so-called emotion embeddings,\nindependent of different natural languages, communication modalities, media or\nrepresentation label formats, and even disparate model architectures.\nExperiments on a wide range of heterogeneous affective datasets indicate that\nthis approach yields the desired interoperability for the sake of reusability,\ninterpretability and flexibility, without penalizing prediction quality. Code\nand data are archived under https://doi.org/10.5281/zenodo.7405327 .",
        "score": 1.638511061668396
      },
      {
        "title": "MASIVE: Open-Ended Affective State Identification in English and Spanish,",
        "abstract": "In the field of emotion analysis, much NLP research focuses on identifying a\nlimited number of discrete emotion categories, often applied across languages.\nThese basic sets, however, are rarely designed with textual data in mind, and\nculture, language, and dialect can influence how particular emotions are\ninterpreted. In this work, we broaden our scope to a practically unbounded set\nof \\textit{affective states}, which includes any terms that humans use to\ndescribe their experiences of feeling. We collect and publish MASIVE, a dataset\nof Reddit posts in English and Spanish containing over 1,000 unique affective\nstates each. We then define the new problem of \\textit{affective state\nidentification} for language generation models framed as a masked span\nprediction task. On this task, we find that smaller finetuned multilingual\nmodels outperform much larger LLMs, even on region-specific Spanish affective\nstates. Additionally, we show that pretraining on MASIVE improves model\nperformance on existing emotion benchmarks. Finally, through machine\ntranslation experiments, we find that native speaker-written data is vital to\ngood performance on this task.",
        "score": 1.3460299968719482
      },
      {
        "title": "EnCBP: A New Benchmark Dataset for Finer-Grained Cultural Background\n  Prediction in English,",
        "abstract": "While cultural backgrounds have been shown to affect linguistic expressions,\nexisting natural language processing (NLP) research on culture modeling is\noverly coarse-grained and does not examine cultural differences among speakers\nof the same language. To address this problem and augment NLP models with\ncultural background features, we collect, annotate, manually validate, and\nbenchmark EnCBP, a finer-grained news-based cultural background prediction\ndataset in English. Through language modeling (LM) evaluations and manual\nanalyses, we confirm that there are noticeable differences in linguistic\nexpressions among five English-speaking countries and across four states in the\nUS. Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic\n(PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2,\nEmotion, and Go-Emotions) show that, while introducing cultural background\ninformation does not benefit the Go-Emotions task due to text domain conflicts,\nit noticeably improves deep learning (DL) model performance on other tasks. Our\nfindings strongly support the importance of cultural background modeling to a\nwide variety of NLP tasks and demonstrate the applicability of EnCBP in\nculture-related research.",
        "score": 1.2684884071350098
      },
      {
        "title": "Quantifying Valence and Arousal in Text with Multilingual Pre-trained\n  Transformers,",
        "abstract": "The analysis of emotions expressed in text has numerous applications. In\ncontrast to categorical analysis, focused on classifying emotions according to\na pre-defined set of common classes, dimensional approaches can offer a more\nnuanced way to distinguish between different emotions. Still, dimensional\nmethods have been less studied in the literature. Considering a valence-arousal\ndimensional space, this work assesses the use of pre-trained Transformers to\npredict these two dimensions on a continuous scale, with input texts from\nmultiple languages and domains. We specifically combined multiple annotated\ndatasets from previous studies, corresponding to either emotional lexica or\nshort text documents, and evaluated models of multiple sizes and trained under\ndifferent settings. Our results show that model size can have a significant\nimpact on the quality of predictions, and that by fine-tuning a large model we\ncan confidently predict valence and arousal in multiple languages. We make\navailable the code, models, and supporting data.",
        "score": 1.031447172164917
      },
      {
        "title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large\n  Language Models,",
        "abstract": "The emotions we experience involve complex processes; besides physiological\naspects, research in psychology has studied cognitive appraisals where people\nassess their situations subjectively, according to their own values (Scherer,\n2005). Thus, the same situation can often result in different emotional\nexperiences. While the detection of emotion is a well-established task, there\nis very limited work so far on the automatic prediction of cognitive\nappraisals. This work fills the gap by presenting CovidET-Appraisals, the most\ncomprehensive dataset to-date that assesses 24 appraisal dimensions, each with\na natural language rationale, across 241 Reddit posts. CovidET-Appraisals\npresents an ideal testbed to evaluate the ability of large language models --\nexcelling at a wide range of NLP tasks -- to automatically assess and explain\ncognitive appraisals. We found that while the best models are performant,\nopen-sourced LLMs fall short at this task, presenting a new challenge in the\nfuture development of emotionally intelligent models. We release our dataset at\nhttps://github.com/honglizhan/CovidET-Appraisals-Public.",
        "score": 0.9166868329048157
      },
      {
        "title": "Cross-lingual Emotion Detection,",
        "abstract": "Emotion detection can provide us with a window into understanding human\nbehavior. Due to the complex dynamics of human emotions, however, constructing\nannotated datasets to train automated models can be expensive. Thus, we explore\nthe efficacy of cross-lingual approaches that would use data from a source\nlanguage to build models for emotion detection in a target language. We compare\nthree approaches, namely: i) using inherently multilingual models; ii)\ntranslating training data into the target language; and iii) using an\nautomatically tagged parallel corpus. In our study, we consider English as the\nsource language with Arabic and Spanish as target languages. We study the\neffectiveness of different classification models such as BERT and SVMs trained\nwith different features. Our BERT-based monolingual models that are trained on\ntarget language data surpass state-of-the-art (SOTA) by 4% and 5% absolute\nJaccard score for Arabic and Spanish respectively. Next, we show that using\ncross-lingual approaches with English data alone, we can achieve more than 90%\nand 80% relative effectiveness of the Arabic and Spanish BERT models\nrespectively. Lastly, we use LIME to analyze the challenges of training\ncross-lingual models for different language pairs",
        "score": 0.6871278285980225
      },
      {
        "title": "Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in\n  Sentiment Analysis,",
        "abstract": "Emotion arcs capture how an individual (or a population) feels over time.\nThey are widely used in industry and research; however, there is little work on\nevaluating the automatically generated arcs. This is because of the difficulty\nof establishing the true (gold) emotion arc. Our work, for the first time,\nsystematically and quantitatively evaluates automatically generated emotion\narcs. We also compare two common ways of generating emotion arcs:\nMachine-Learning (ML) models and Lexicon-Only (LexO) methods. By running\nexperiments on 18 diverse datasets in 9 languages, we show that despite being\nmarkedly poor at instance level emotion classification, LexO methods are highly\naccurate at generating emotion arcs when aggregating information from hundreds\nof instances. We also show, through experiments on six indigenous African\nlanguages, as well as Arabic, and Spanish, that automatic translations of\nEnglish emotion lexicons can be used to generate high-quality emotion arcs in\nless-resource languages. This opens up avenues for work on emotions in\nlanguages from around the world; which is crucial for commerce, public policy,\nand health research in service of speakers often left behind. Code and\nresources: https://github.com/dteodore/EmotionArcs",
        "score": 0.4591919481754303
      },
      {
        "title": "English Prompts are Better for NLI-based Zero-Shot Emotion\n  Classification than Target-Language Prompts,",
        "abstract": "Emotion classification in text is a challenging task due to the processes\ninvolved when interpreting a textual description of a potential emotion\nstimulus. In addition, the set of emotion categories is highly domain-specific.\nFor instance, literature analysis might require the use of aesthetic emotions\n(e.g., finding something beautiful), and social media analysis could benefit\nfrom fine-grained sets (e.g., separating anger from annoyance) than only those\nthat represent basic categories as they have been proposed by Paul Ekman\n(anger, disgust, fear, joy, surprise, sadness). This renders the task an\ninteresting field for zero-shot classifications, in which the label set is not\nknown at model development time. Unfortunately, most resources for emotion\nanalysis are English, and therefore, most studies on emotion analysis have been\nperformed in English, including those that involve prompting language models\nfor text labels. This leaves us with a research gap that we address in this\npaper: In which language should we prompt for emotion labels on non-English\ntexts? This is particularly of interest when we have access to a multilingual\nlarge language model, because we could request labels with English prompts even\nfor non-English data. Our experiments with natural language inference-based\nlanguage models show that it is consistently better to use English prompts even\nif the data is in a different language.",
        "score": 0.3707005977630615
      },
      {
        "title": "Quite Good, but Not Enough: Nationality Bias in Large Language Models --\n  A Case Study of ChatGPT,",
        "abstract": "While nationality is a pivotal demographic element that enhances the\nperformance of language models, it has received far less scrutiny regarding\ninherent biases. This study investigates nationality bias in ChatGPT (GPT-3.5),\na large language model (LLM) designed for text generation. The research covers\n195 countries, 4 temperature settings, and 3 distinct prompt types, generating\n4,680 discourses about nationality descriptions in Chinese and English.\nAutomated metrics were used to analyze the nationality bias, and expert\nannotators alongside ChatGPT itself evaluated the perceived bias. The results\nshow that ChatGPT's generated discourses are predominantly positive, especially\ncompared to its predecessor, GPT-2. However, when prompted with negative\ninclinations, it occasionally produces negative content. Despite ChatGPT\nconsidering its generated text as neutral, it shows consistent self-awareness\nabout nationality bias when subjected to the same pair-wise comparison\nannotation framework used by human annotators. In conclusion, while ChatGPT's\ngenerated texts seem friendly and positive, they reflect the inherent\nnationality biases in the real world. This bias may vary across different\nlanguage versions of ChatGPT, indicating diverse cultural perspectives. The\nstudy highlights the subtle and pervasive nature of biases within LLMs,\nemphasizing the need for further scrutiny.",
        "score": 0.3629646599292755
      },
      {
        "title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in\n  Values,",
        "abstract": "Language embeds information about social, cultural, and political values\npeople hold. Prior work has explored social and potentially harmful biases\nencoded in Pre-Trained Language models (PTLMs). However, there has been no\nsystematic study investigating how values embedded in these models vary across\ncultures. In this paper, we introduce probes to study which values across\ncultures are embedded in these models, and whether they align with existing\ntheories and cross-cultural value surveys. We find that PTLMs capture\ndifferences in values across cultures, but those only weakly align with\nestablished value surveys. We discuss implications of using mis-aligned models\nin cross-cultural settings, as well as ways of aligning PTLMs with value\nsurveys.",
        "score": 0.3117140531539917
      },
      {
        "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in\n  Korean,",
        "abstract": "Despite the rapid development of large language models (LLMs) for the Korean\nlanguage, there remains an obvious lack of benchmark datasets that test the\nrequisite Korean cultural and linguistic knowledge. Because many existing\nKorean benchmark datasets are derived from the English counterparts through\ntranslation, they often overlook the different cultural contexts. For the few\nbenchmark datasets that are sourced from Korean data capturing cultural\nknowledge, only narrow tasks such as bias and hate speech detection are\noffered. To address this gap, we introduce a benchmark of Cultural and\nLinguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.\nCLIcK sources its data from official Korean exams and textbooks, partitioning\nthe questions into eleven categories under the two main categories of language\nand culture. For each instance in CLIcK, we provide fine-grained annotation of\nwhich cultural and linguistic knowledge is required to answer the question\ncorrectly. Using CLIcK, we test 13 language models to assess their performance.\nOur evaluation uncovers insights into their performances across the categories,\nas well as the diverse factors affecting their comprehension. CLIcK offers the\nfirst large-scale comprehensive Korean-centric analysis of LLMs' proficiency in\nKorean culture and language.",
        "score": 0.21768172085285187
      },
      {
        "title": "Pre-trained Speech Processing Models Contain Human-Like Biases that\n  Propagate to Speech Emotion Recognition,",
        "abstract": "Previous work has established that a person's demographics and speech style\naffect how well speech processing models perform for them. But where does this\nbias come from? In this work, we present the Speech Embedding Association Test\n(SpEAT), a method for detecting bias in one type of model used for many speech\ntasks: pre-trained models. The SpEAT is inspired by word embedding association\ntests in natural language processing, which quantify intrinsic bias in a\nmodel's representations of different concepts, such as race or valence\n(something's pleasantness or unpleasantness) and capture the extent to which a\nmodel trained on large-scale socio-cultural data has learned human-like biases.\nUsing the SpEAT, we test for six types of bias in 16 English speech models\n(including 4 models also trained on multilingual data), which come from the\nwav2vec 2.0, HuBERT, WavLM, and Whisper model families. We find that 14 or more\nmodels reveal positive valence (pleasantness) associations with abled people\nover disabled people, with European-Americans over African-Americans, with\nfemales over males, with U.S. accented speakers over non-U.S. accented\nspeakers, and with younger people over older people. Beyond establishing that\npre-trained speech models contain these biases, we also show that they can have\nreal world effects. We compare biases found in pre-trained models to biases in\ndownstream models adapted to the task of Speech Emotion Recognition (SER) and\nfind that in 66 of the 96 tests performed (69%), the group that is more\nassociated with positive valence as indicated by the SpEAT also tends to be\npredicted as speaking with higher valence by the downstream model. Our work\nprovides evidence that, like text and image-based models, pre-trained speech\nbased-models frequently learn human-like biases. Our work also shows that bias\nfound in pre-trained models can propagate to the downstream task of SER.",
        "score": 0.07202854007482529
      },
      {
        "title": "Emotion Classification in Low and Moderate Resource Languages,",
        "abstract": "It is important to be able to analyze the emotional state of people around\nthe globe. There are 7100+ active languages spoken around the world and\nbuilding emotion classification for each language is labor intensive.\nParticularly for low-resource and endangered languages, building emotion\nclassification can be quite challenging. We present a cross-lingual emotion\nclassifier, where we train an emotion classifier with resource-rich languages\n(i.e. \\textit{English} in our work) and transfer the learning to low and\nmoderate resource languages. We compare and contrast two approaches of transfer\nlearning from a high-resource language to a low or moderate-resource language.\nOne approach projects the annotation from a high-resource language to low and\nmoderate-resource language in parallel corpora and the other one uses direct\ntransfer from high-resource language to the other languages. We show the\nefficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano,\nOdia, and Azerbaijani. Our results indicate that our approaches outperform\nrandom baselines and transfer emotions across languages successfully. For all\nlanguages, the direct cross-lingual transfer of emotion yields better results.\nWe also create annotated emotion-labeled resources for four languages: Farsi,\nAzerbaijani, Ilocano and Odia.",
        "score": -0.012300705537199974
      },
      {
        "title": "Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions,",
        "abstract": "Emotions are a central aspect of communication. Consequently, emotion\nanalysis (EA) is a rapidly growing field in natural language processing (NLP).\nHowever, there is no consensus on scope, direction, or methods. In this paper,\nwe conduct a thorough review of 154 relevant NLP publications from the last\ndecade. Based on this review, we address four different questions: (1) How are\nEA tasks defined in NLP? (2) What are the most prominent emotion frameworks and\nwhich emotions are modeled? (3) Is the subjectivity of emotions considered in\nterms of demographics and cultural factors? and (4) What are the primary NLP\napplications for EA? We take stock of trends in EA and tasks, emotion\nframeworks used, existing datasets, methods, and applications. We then discuss\nfour lacunae: (1) the absence of demographic and cultural aspects does not\naccount for the variation in how emotions are perceived, but instead assumes\nthey are universally experienced in the same manner; (2) the poor fit of\nemotion categories from the two main emotion theories to the task; (3) the lack\nof standardized EA terminology hinders gap identification, comparison, and\nfuture goals; and (4) the absence of interdisciplinary research isolates EA\nfrom insights in other fields. Our work will enable more focused research into\nEA and a more holistic approach to modeling emotions in NLP.",
        "score": -0.34701240062713623
      },
      {
        "title": "Towards Intercultural Affect Recognition: Audio-Visual Affect\n  Recognition in the Wild Across Six Cultures,",
        "abstract": "In our multicultural world, affect-aware AI systems that support humans need\nthe ability to perceive affect across variations in emotion expression patterns\nacross cultures. These systems must perform well in cultural contexts without\nannotated affect datasets available for training models. A standard assumption\nin affective computing is that affect recognition models trained and used\nwithin the same culture (intracultural) will perform better than models trained\non one culture and used on different cultures (intercultural). We test this\nassumption and present the first systematic study of intercultural affect\nrecognition models using videos of real-world dyadic interactions from six\ncultures. We develop an attention-based feature selection approach under\ntemporal causal discovery to identify behavioral cues that can be leveraged in\nintercultural affect recognition models. Across all six cultures, our findings\ndemonstrate that intercultural affect recognition models were as effective or\nmore effective than intracultural models. We identify and contribute useful\nbehavioral features for intercultural affect recognition; facial features from\nthe visual modality were more useful than the audio modality in this study's\ncontext. Our paper presents a proof-of-concept and motivation for the future\ndevelopment of intercultural affect recognition systems, especially those\ndeployed in low-resource situations without annotated data.",
        "score": -0.37649911642074585
      }
    ]
  },
  {
    "title": "Automatic Library Migration Using Large Language Models: First Results",
    "abstract": "  Despite being introduced only a few years ago, Large Language Models (LLMs)\nare already widely used by developers for code generation. However, their\napplication in automating other Software Engineering activities remains largely\nunexplored. Thus, in this paper, we report the first results of a study in\nwhich we are exploring the use of ChatGPT to support API migration tasks, an\nimportant problem that demands manual effort and attention from developers.\nSpecifically, in the paper, we share our initial results involving the use of\nChatGPT to migrate a client application to use a newer version of SQLAlchemy,\nan ORM (Object Relational Mapping) library widely used in Python. We evaluate\nthe use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts)\nand show that the best results are achieved by the One-Shot prompt, followed by\nthe Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to\nsuccessfully migrate all columns of our target application and upgrade its code\nto use new functionalities enabled by SQLAlchemy's latest version, such as\nPython's asyncio and typing modules, while preserving the original code\nbehavior.\n",
    "related_paper_titles": [],
    "related_paper_abstract": [],
    "entities": [
      "Mamba",
      "EEG",
      "TTS",
      "Shapley",
      "SSMs",
      "ECG",
      "NTK",
      "MDP",
      "IFT",
      "RLHF",
      "Online",
      "FLOPs",
      "Toward",
      "Human Feedback",
      "Speech",
      "Contrastive",
      "MPC",
      "CAD",
      "ToM",
      "Direct Preference",
      "Random Forest",
      "DRL",
      "MRI",
      "NAS",
      "SNN",
      "Visual",
      "ANNs",
      "WSIs",
      "SAT",
      "KV"
    ],
    "retrieved_papers": [
      {
        "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large\n  Language Models,",
        "abstract": "Recently, large language models (LLMs), especially those that are pretrained\non code, have demonstrated strong capabilities in generating programs from\nnatural language inputs in a few-shot or even zero-shot manner. Despite\npromising results, there is a notable lack of a comprehensive evaluation of\nthese models language-to-code generation capabilities. Existing studies often\nfocus on specific tasks, model architectures, or learning paradigms, leading to\na fragmented understanding of the overall landscape. In this work, we present\nL2CEval, a systematic evaluation of the language-to-code generation\ncapabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,\nmath reasoning and Python programming, analyzing the factors that potentially\naffect their performance, such as model size, pretraining data, instruction\ntuning, and different prompting methods. In addition to assessing model\nperformance, we measure confidence calibration for the models and conduct human\nevaluations of the output programs. This enables us to identify and analyze the\ntypical failure modes across various tasks and models. L2CEval offers a\ncomprehensive understanding of the capabilities and limitations of LLMs in\nlanguage-to-code generation. We also release the evaluation framework and all\nmodel outputs, hoping to lay the groundwork for further future research in this\ndomain.",
        "score": 1.6497832536697388
      },
      {
        "title": "A Survey on Large Language Models for Code Generation,",
        "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across\ndiverse code-related tasks, known as Code LLMs, particularly in code generation\nthat generates source code with LLM from natural language descriptions. This\nburgeoning field has captured significant interest from both academic\nresearchers and industry professionals due to its practical significance in\nsoftware development, e.g., GitHub Copilot. Despite the active exploration of\nLLMs for a variety of code tasks, either from the perspective of natural\nlanguage processing (NLP) or software engineering (SE) or both, there is a\nnoticeable absence of a comprehensive and up-to-date literature review\ndedicated to LLM for code generation. In this survey, we aim to bridge this gap\nby providing a systematic literature review that serves as a valuable reference\nfor researchers investigating the cutting-edge progress in LLMs for code\ngeneration. We introduce a taxonomy to categorize and discuss the recent\ndevelopments in LLMs for code generation, covering aspects such as data\ncuration, latest advances, performance evaluation, and real-world applications.\nIn addition, we present a historical overview of the evolution of LLMs for code\ngeneration and offer an empirical comparison using the widely recognized\nHumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM\ncapabilities for code generation. We identify critical challenges and promising\nopportunities regarding the gap between academia and practical development.\nFurthermore, we have established a dedicated resource website\n(https://codellm.github.io) to continuously document and disseminate the most\nrecent advances in the field.",
        "score": 1.169126033782959
      },
      {
        "title": "Gorilla: Large Language Model Connected with Massive APIs,",
        "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances\nrecently, with models now excelling in a variety of tasks, such as mathematical\nreasoning and program synthesis. However, their potential to effectively use\ntools via API calls remains unfulfilled. This is a challenging task even for\ntoday's state-of-the-art LLMs such as GPT-4, largely due to their inability to\ngenerate accurate input arguments and their tendency to hallucinate the wrong\nusage of an API call. We release Gorilla, a finetuned LLaMA-based model that\nsurpasses the performance of GPT-4 on writing API calls. When combined with a\ndocument retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, enabling flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly\nencountered when prompting LLMs directly. To evaluate the model's ability, we\nintroduce APIBench, a comprehensive dataset consisting of HuggingFace,\nTorchHub, and TensorHub APIs. The successful integration of the retrieval\nsystem with Gorilla demonstrates the potential for LLMs to use tools more\naccurately, keep up with frequently updated documentation, and consequently\nincrease the reliability and applicability of their outputs. Gorilla's code,\nmodel, data, and demo are available at https://gorilla.cs.berkeley.edu",
        "score": 0.6619040966033936
      },
      {
        "title": "Investigating the Efficacy of Large Language Models for Code Clone\n  Detection,",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in various\nnatural language processing and software engineering tasks, such as code\ngeneration. The LLMs are mainly utilized in the prompt-based zero/few-shot\nparadigm to guide the model in accomplishing the task. GPT-based models are one\nof the popular ones studied for tasks such as code comment generation or test\ngeneration. These tasks are `generative' tasks. However, there is limited\nresearch on the usage of LLMs for `non-generative' tasks such as classification\nusing the prompt-based paradigm. In this preliminary exploratory study, we\ninvestigated the applicability of LLMs for Code Clone Detection (CCD), a\nnon-generative task. By building a mono-lingual and cross-lingual CCD dataset\nderived from CodeNet, we first investigated two different prompts using ChatGPT\nto detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot\nsetting. We then conducted an analysis to understand the strengths and\nweaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language\nCCD attaining an F1-score of 0.877 and achieves comparable performance to fully\nfine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the\nprompt and the difficulty level of the problems has an impact on the\nperformance of ChatGPT. Finally we provide insights and future directions based\non our initial analysis",
        "score": 0.3463417589664459
      },
      {
        "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world\n  APIs,",
        "abstract": "Despite the advancements of open-source large language models (LLMs), e.g.,\nLLaMA, they remain significantly limited in tool-use capabilities, i.e., using\nexternal tools (APIs) to fulfill human instructions. The reason is that current\ninstruction tuning largely focuses on basic language tasks but ignores the\ntool-use domain. This is in contrast to the excellent tool-use capabilities of\nstate-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap,\nwe introduce ToolLLM, a general tool-use framework encompassing data\nconstruction, model training, and evaluation. We first present ToolBench, an\ninstruction-tuning dataset for tool use, which is constructed automatically\nusing ChatGPT. Specifically, the construction can be divided into three stages:\n(i) API collection: we collect 16,464 real-world RESTful APIs spanning 49\ncategories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to\ngenerate diverse instructions involving these APIs, covering both single-tool\nand multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to\nsearch for a valid solution path (chain of API calls) for each instruction. To\nenhance the reasoning capabilities of LLMs, we develop a novel depth-first\nsearch-based decision tree algorithm. It enables LLMs to evaluate multiple\nreasoning traces and expand the search space. Moreover, to evaluate the\ntool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.\nBased on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it\nwith a neural API retriever to recommend appropriate APIs for each instruction.\nExperiments show that ToolLLaMA demonstrates a remarkable ability to execute\ncomplex instructions and generalize to unseen APIs, and exhibits comparable\nperformance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot\ngeneralization ability in an out-of-distribution tool-use dataset: APIBench.",
        "score": 0.1879252791404724
      },
      {
        "title": "Language Models Enable Simple Systems for Generating Structured Views of\n  Heterogeneous Data Lakes,",
        "abstract": "A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n  We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each.",
        "score": 0.17546996474266052
      },
      {
        "title": "Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability\n  of Large Language Model Code Generation,",
        "abstract": "Recently, the large language models (LLMs) have shown extraordinary ability\nin understanding natural language and generating programming code. It has been\na common practice of software engineers to consult LLMs when encountering\ncoding questions. Although efforts have been made to avoid syntax errors and\nalign the code with the intended semantics, the reliability and robustness of\nthe code generationfrom LLMs have not yet been thoroughly studied. The\nexecutable code is not equivalent to the reliable and robust code, especially\nin the context of real-world software development. The misuse of APIs in the\ngenerated code could lead to severe problem, such as resource leaks, program\ncrashes. To make things worse, the users of LLM code generation services are\nactually the developers that are most vulnerable to these code that seems right\n-- They are always novice developers that are not familiar with the APIs that\nLLMs generate code for them. Therefore, they could hardly tell the misuse in\nthe code generated by LLMs, which further facilitates the incorrect code\napplied in real-world software. Existing code evaluation benchmark and datasets\nfocus on crafting small tasks such as programming questions in coding\ninterviews, which however deviates from the problem that developers would ask\nLLM for real-world coding help. To fill the missing piece, in this work, we\npropose a dataset RobustAPI for evaluating the reliability and robustness of\ncode generated by LLMs. We collect 1208 coding questions from StackOverflow on\n24 representative Java APIs. We summarize thecommon misuse patterns of these\nAPIs and evaluate them oncurrent popular LLMs. The evaluation results show that\nevenfor GPT-4, 62% of the generated code contains API misuses,which would cause\nunexpected consequences if the code isintroduced into real-world software.",
        "score": 0.02719583548605442
      },
      {
        "title": "A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized\n  Retrieval Augmentation,",
        "abstract": "Natural Language to Code Generation has made significant progress in recent\nyears with the advent of Large Language Models(LLMs). While generation for\ngeneral-purpose languages like C, C++, and Python has improved significantly,\nLLMs struggle with custom function names in Domain Specific Languages or DSLs.\nThis leads to higher hallucination rates and syntax errors, specially for DSLs\nhaving a high number of custom function names. Additionally, constant updates\nto function names add to the challenge as LLMs need to stay up-to-date. In this\npaper, we present optimizations for using Retrieval Augmented Generation (or\nRAG) with LLMs for DSL generation along with an ablation study comparing these\nstrategies. We generated a train as well as test dataset with a DSL to\nrepresent automation tasks across roughly 700 APIs in public domain. We used\nthe training dataset to fine-tune a Codex model for this DSL. Our results\nshowed that the fine-tuned model scored the best on code similarity metric.\nWith our RAG optimizations, we achieved parity for similarity metric. The\ncompilation rate, however, showed that both the models still got the syntax\nwrong many times, with RAG-based method being 2 pts better. Conversely,\nhallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for\nAPI parameter keys. We conclude that an optimized RAG model can match the\nquality of fine-tuned models and offer advantages for new, unseen APIs.",
        "score": -0.01687764562666416
      },
      {
        "title": "Prompt2Model: Generating Deployable Models from Natural Language\n  Instructions,",
        "abstract": "Large language models (LLMs) enable system builders today to create competent\nNLP systems through prompting, where they only need to describe the task in\nnatural language and provide a few examples. However, in other ways, LLMs are a\nstep backward from traditional special-purpose NLP models; they require\nextensive computational resources for deployment and can be gated behind APIs.\nIn this paper, we propose Prompt2Model, a general-purpose method that takes a\nnatural language task description like the prompts provided to LLMs, and uses\nit to train a special-purpose model that is conducive to deployment. This is\ndone through a multi-step process of retrieval of existing datasets and\npretrained models, dataset generation using LLMs, and supervised fine-tuning on\nthese retrieved and generated datasets. Over three tasks, we demonstrate that\ngiven the same few-shot prompt as input, Prompt2Model trains models that\noutperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%\nwhile being up to 700 times smaller. We also show that this data can be used to\nobtain reliable performance estimates of model performance, enabling model\ndevelopers to assess model reliability before deployment. Prompt2Model is\navailable open-source at https://github.com/neulab/prompt2model.",
        "score": -0.4686196446418762
      },
      {
        "title": "APPL: A Prompt Programming Language for Harmonious Integration of\n  Programs and Large Language Model Prompts,",
        "abstract": "Large Language Models (LLMs) have become increasingly capable of handling\ndiverse tasks with the aid of well-crafted prompts and integration of external\ntools, but as task complexity rises, the workflow involving LLMs can be\ncomplicated and thus challenging to implement and maintain. To address this\nchallenge, we propose APPL, A Prompt Programming Language that acts as a bridge\nbetween computer programs and LLMs, allowing seamless embedding of prompts into\nPython functions, and vice versa. APPL provides an intuitive and Python-native\nsyntax, an efficient parallelized runtime with asynchronous semantics, and a\ntracing module supporting effective failure diagnosis and replaying without\nextra costs. We demonstrate that APPL programs are intuitive, concise, and\nefficient through three representative scenarios: Chain-of-Thought with\nself-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat.\nExperiments on three parallelizable workflows further show that APPL can\neffectively parallelize independent LLM calls, with a significant speedup ratio\nthat almost matches the estimation.",
        "score": -0.5064377784729004
      },
      {
        "title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates,",
        "abstract": "Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs.",
        "score": -0.8145900368690491
      },
      {
        "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program\n  Synthesis,",
        "abstract": "Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification, expressed with input-output examples or natural\nlanguage descriptions. The prevalence of large language models advances the\nstate-of-the-art for program synthesis, though limited training resources and\ndata impede open access to such models. To democratize this, we train and\nrelease a family of large language models up to 16.1B parameters, called\nCODEGEN, on natural language and programming language data, and open source the\ntraining library JAXFORMER. We show the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-the-art on\nzero-shot Python code generation on HumanEval. We further investigate the\nmulti-step paradigm for program synthesis, where a single program is factorized\ninto multiple prompts specifying subproblems. To this end, we construct an open\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\nshows that the same intent provided to CODEGEN in multi-turn fashion\nsignificantly improves program synthesis over that provided as a single turn.\nWe make the training library JAXFORMER and model checkpoints available as open\nsource contribution: https://github.com/salesforce/CodeGen.",
        "score": -0.8670067191123962
      },
      {
        "title": "AVATAR: A Parallel Corpus for Java-Python Program Translation,",
        "abstract": "Program translation refers to migrating source code from one programming\nlanguage to another. It has tremendous practical value in software development,\nas porting software across languages is time-consuming and costly. Automating\nprogram translation is of paramount importance in software migration, and\nrecently researchers explored unsupervised approaches due to the unavailability\nof parallel corpora. However, the availability of pre-trained language models\nfor programming languages enables supervised fine-tuning with a small number of\nlabeled examples. Therefore, we present AVATAR, a collection of 9,515\nprogramming problems and their solutions written in two popular languages, Java\nand Python. AVATAR is collected from competitive programming sites, online\nplatforms, and open-source repositories. Furthermore, AVATAR includes unit\ntests for 250 examples to facilitate functional correctness evaluation. We\nbenchmark several pre-trained language models fine-tuned on AVATAR. Experiment\nresults show that the models lack in generating functionally accurate code.",
        "score": -1.062180995941162
      },
      {
        "title": "DocCGen: Document-based Controlled Code Generation,",
        "abstract": "Recent developments show that Large Language Models (LLMs) produce\nstate-of-the-art performance on natural language (NL) to code generation for\nresource-rich general-purpose languages like C++, Java, and Python. However,\ntheir practical usage for structured domain-specific languages (DSLs) such as\nYAML, JSON is limited due to domain-specific schema, grammar, and\ncustomizations generally unseen by LLMs during pre-training. Efforts have been\nmade to mitigate this challenge via in-context learning through relevant\nexamples or by fine-tuning. However, it suffers from problems, such as limited\nDSL samples and prompt sensitivity but enterprises maintain good documentation\nof the DSLs. Therefore, we propose DocCGen, a framework that can leverage such\nrich knowledge by breaking the NL-to-Code generation task for structured code\nlanguages into a two-step process. First, it detects the correct libraries\nusing the library documentation that best matches the NL query. Then, it\nutilizes schema rules extracted from the documentation of these libraries to\nconstrain the decoding. We evaluate our framework for two complex structured\nlanguages, Ansible YAML and Bash command, consisting of two settings:\nOut-of-domain (OOD) and In-domain (ID). Our extensive experiments show that\nDocCGen consistently improves different-sized language models across all six\nevaluation metrics, reducing syntactic and semantic errors in structured code.\nWe plan to open-source the datasets and code to motivate research in\nconstrained code generation.",
        "score": -1.1866955757141113
      },
      {
        "title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM\n  Workflows,",
        "abstract": "Large language models (LLMs) have become a dominant and important tool for\nNLP researchers in a wide range of tasks. Today, many researchers use LLMs in\nsynthetic data generation, task evaluation, fine-tuning, distillation, and\nother model-in-the-loop research workflows. However, challenges arise when\nusing these models that stem from their scale, their closed source nature, and\nthe lack of standardized tooling for these new and emerging workflows. The\nrapid rise to prominence of these models and these unique challenges has had\nimmediate adverse impacts on open science and on the reproducibility of work\nthat uses them. In this paper, we introduce DataDreamer, an open source Python\nlibrary that allows researchers to write simple code to implement powerful LLM\nworkflows. DataDreamer also helps researchers adhere to best practices that we\npropose to encourage open science and reproducibility. The library and\ndocumentation are available at https://github.com/datadreamer-dev/DataDreamer .",
        "score": -1.2513794898986816
      },
      {
        "title": "Prompting Is Programming: A Query Language for Large Language Models,",
        "abstract": "Large language models have demonstrated outstanding performance on a wide\nrange of tasks such as question answering and code generation. On a high level,\ngiven an input, a language model can be used to automatically complete the\nsequence in a statistically-likely way. Based on this, users prompt these\nmodels with language instructions or examples, to implement a variety of\ndownstream tasks. Advanced prompting methods can even imply interaction between\nthe language model, a user, and external tools such as calculators. However, to\nobtain state-of-the-art performance or adapt language models for specific\ntasks, complex task- and model-specific programs have to be implemented, which\nmay still require ad-hoc interaction.\n  Based on this, we present the novel idea of Language Model Programming (LMP).\nLMP generalizes language model prompting from pure text prompts to an intuitive\ncombination of text prompting and scripting. Additionally, LMP allows\nconstraints to be specified over the language model output. This enables easy\nadaption to many tasks while abstracting language model internals and providing\nhigh-level semantics.\n  To enable LMP, we implement LMQL(short for Language Model Query Language),\nwhich leverages the constraints and control flow from an LMP prompt to generate\nan efficient inference procedure that minimizes the number of expensive calls\nto the underlying language model.\n  We show that LMQL can capture a wide range of state-of-the-art prompting\nmethods in an intuitive way, especially facilitating interactive flows that are\nchallenging to implement with existing high-level APIs. Our evaluation shows\nthat we retain or increase the accuracy on several downstream tasks, while also\nsignificantly reducing the required amount of computation or cost in the case\nof pay-to-use APIs (26-85% cost savings).",
        "score": -1.317674160003662
      },
      {
        "title": "GAP-Gen: Guided Automatic Python Code Generation,",
        "abstract": "Automatic code generation from natural language descriptions can be highly\nbeneficial during the process of software development. In this work, we propose\nGAP-Gen, a Guided Automatic Python Code Generation method based on Python\nsyntactic constraints and semantic constraints. We first introduce Python\nsyntactic constraints in the form of Syntax-Flow, which is a simplified version\nof Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract\nSyntax Tree but maintaining crucial syntactic information of Python code. In\naddition to Syntax-Flow, we introduce Variable-Flow which abstracts variable\nand function names consistently through out the code. In our work, rather than\npretraining, we focus on modifying the finetuning process which reduces\ncomputational requirements but retains high generation performance on automatic\nPython code generation task. GAP-Gen fine-tunes the transformer based language\nmodels T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet,\nCodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our\nexperiments show that GAP-Gen achieves better results on automatic Python code\ngeneration task than previous works.",
        "score": -1.5302809476852417
      },
      {
        "title": "Structured Chain-of-Thought Prompting for Code Generation,",
        "abstract": "Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive\nperformance in code generation. LLMs take prompts as inputs, and\nChain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.\nCoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural\nlanguage reasoning steps) and then output the code. However, CoT prompting is\ndesigned for natural language generation and has low accuracy in code\ngeneration.\n  In this paper, we propose Structured CoTs (SCoTs) and present a novel\nprompting technique for code generation, named SCoT prompting. Our motivation\nis source code contains rich structural information and any code can be\ncomposed of three program structures (i.e., sequence, branch, and loop\nstructures). Intuitively, structured intermediate reasoning steps make for\nstructured source code. Thus, we ask LLMs to use program structures to build\nCoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.\nCompared to CoT prompting, SCoT prompting explicitly constrains LLMs to think\nabout how to solve requirements from the view of source code and further the\nperformance of LLMs in code generation. We apply SCoT prompting to two LLMs\n(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,\nMBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline\n- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human\ndevelopers prefer programs from SCoT prompting. (3) SCoT prompting is robust to\nexamples and achieves substantial improvements.",
        "score": -1.9207838773727417
      },
      {
        "title": "API Pack: A Massive Multi-Programming Language Dataset for API Call\n  Generation,",
        "abstract": "We introduce API Pack, a massive multi-programming language dataset\ncontaining more than 1 million instruction-API call pairs to improve the API\ncall generation capabilities of large language models. By fine-tuning\nCodeLlama-13B on 20,000 Python instances from API Pack, we enable it to\noutperform GPT-3.5 and GPT-4 in generating unseen API calls. Fine-tuning on API\nPack also facilitates cross-programming language generalization by leveraging a\nlarge amount of data in one language and small amounts of data from other\nlanguages. Scaling the training data to 1 million instances further improves\nthe model's ability to generalize to new APIs not used in training. To\nfacilitate further research, we open-source the API Pack dataset, trained\nmodel, and associated source code at https://github.com/zguo0525/API-Pack.",
        "score": -2.102081060409546
      },
      {
        "title": "Multi-lingual Evaluation of Code Generation Models,",
        "abstract": "We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.",
        "score": -2.1543922424316406
      },
      {
        "title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on\n  Language Models for Code,",
        "abstract": "In this work we systematically review the recent advancements in software\nengineering with language models, covering 70+ models, 40+ evaluation tasks,\n180+ datasets, and 900 related works. Unlike previous works, we integrate\nsoftware engineering (SE) with natural language processing (NLP) by discussing\nthe perspectives of both sides: SE applies language models for development\nautomation, while NLP adopts SE tasks for language model evaluation. We break\ndown code processing models into general language models represented by the GPT\nfamily and specialized models that are specifically pretrained on code, often\nwith tailored objectives. We discuss the relations and differences between\nthese models, and highlight the historical transition of code modeling from\nstatistical models and RNNs to pretrained Transformers and LLMs, which is\nexactly the same course that had been taken by NLP. We also go beyond\nprogramming and review LLMs' application in other software engineering\nactivities including requirement engineering, testing, deployment, and\noperations in an endeavor to provide a global view of NLP in SE, and identify\nkey challenges and potential future directions in this domain. We keep the\nsurvey open and updated on GitHub at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.",
        "score": -2.158618688583374
      },
      {
        "title": "RE-GAINS & EnChAnT: Intelligent Tool Manipulation Systems For Enhanced\n  Query Responses,",
        "abstract": "Large Language Models (LLMs) currently struggle with tool invocation and\nchaining, as they often hallucinate or miss essential steps in a sequence. We\npropose RE-GAINS and EnChAnT, two novel frameworks that empower LLMs to tackle\ncomplex user queries by making API calls to external tools based on tool\ndescriptions and argument lists. Tools are chained based on the expected\noutput, without receiving the actual results from each individual call.\nEnChAnT, an open-source solution, leverages an LLM format enforcer, OpenChat\n3.5 (an LLM), and ToolBench's API Retriever. RE-GAINS utilizes OpenAI models\nand embeddings with a specialized prompt based on the $\\underline{R}$easoning\nvi$\\underline{a}$ $\\underline{P}$lanning $(RAP)$ framework. Both frameworks are\nlow cost (0.01\\$ per query). Our key contribution is enabling LLMs for tool\ninvocation and chaining using modifiable, externally described tools.",
        "score": -2.199232578277588
      },
      {
        "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,",
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but\nfor their future development it is essential to study the frontier of their\ncapabilities. We find real-world software engineering to be a rich,\nsustainable, and challenging testbed for evaluating the next generation of\nlanguage models. To this end, we introduce SWE-bench, an evaluation framework\nconsisting of $2,294$ software engineering problems drawn from real GitHub\nissues and corresponding pull requests across $12$ popular Python repositories.\nGiven a codebase along with a description of an issue to be resolved, a\nlanguage model is tasked with editing the codebase to address the issue.\nResolving issues in SWE-bench frequently requires understanding and\ncoordinating changes across multiple functions, classes, and even files\nsimultaneously, calling for models to interact with execution environments,\nprocess extremely long contexts and perform complex reasoning that goes far\nbeyond traditional code generation tasks. Our evaluations show that both\nstate-of-the-art proprietary models and our fine-tuned model SWE-Llama can\nresolve only the simplest issues. The best-performing model, Claude 2, is able\nto solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps\ntowards LMs that are more practical, intelligent, and autonomous.",
        "score": -2.239731788635254
      },
      {
        "title": "Execution-Based Evaluation for Open-Domain Code Generation,",
        "abstract": "To extend the scope of coding queries to more realistic settings, we propose\nODEX, the first Open-Domain EXecution-based natural language (NL) to Python\ncode generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse\nlibraries, along with 1,707 human-written test cases for execution. Our NL-Code\npairs are harvested from StackOverflow forums to encourage natural and\npractical coding queries. Moreover, ODEX supports four natural languages as\nintents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing\nbehavioral differences among top-performing code language models (LM). While\nCODEX achieves better overall results, CODEGEN improves effectively via scaling\n-- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show\nsubstantial gaps between open and closed domains, but CODEGEN gaps tend to\ndecrease with model size while CODEX gaps increase. We release ODEX to\nfacilitate research into open-domain problems for the code generation\ncommunity.",
        "score": -3.208665132522583
      },
      {
        "title": "Code Execution with Pre-trained Language Models,",
        "abstract": "Code execution is a fundamental aspect of programming language semantics that\nreflects the exact behavior of the code. However, most pre-trained models for\ncode intelligence ignore the execution trace and only rely on source code and\nsyntactic structures. In this paper, we investigate how well pre-trained models\ncan understand and perform code execution. We develop a mutation-based data\naugmentation technique to create a large-scale and realistic Python dataset and\ntask for code execution, which challenges existing models such as Codex. We\nthen present CodeExecutor, a Transformer model that leverages code execution\npre-training and curriculum learning to enhance its semantic comprehension. We\nevaluate CodeExecutor on code execution and show its promising performance and\nlimitations. We also demonstrate its potential benefits for code intelligence\ntasks such as zero-shot code-to-code search and text-to-code generation. Our\nanalysis provides insights into the learning and generalization abilities of\npre-trained models for code execution.",
        "score": -3.272400379180908
      },
      {
        "title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls,",
        "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize\nthe utilization of a vast array of tools in addressing user queries. We utilize\nover 16,000 APIs from Rapid API, operating under the assumption that a subset\nof these APIs could potentially resolve the queries. AnyTool primarily\nincorporates three elements: an API retriever with a hierarchical structure, a\nsolver aimed at resolving user queries using a selected set of API candidates,\nand a self-reflection mechanism, which re-activates AnyTool if the initial\nsolution proves impracticable. AnyTool is powered by the function calling\nfeature of GPT-4, eliminating the need for training external modules. We also\nrevisit the evaluation protocol introduced by previous works and identify a\nlimitation in this protocol that leads to an artificially high pass rate. By\nrevising the evaluation protocol to better reflect practical application\nscenarios, we introduce an additional benchmark, termed AnyToolBench.\nExperiments across various datasets demonstrate the superiority of our AnyTool\nover strong baselines such as ToolLLM and a GPT-4 variant tailored for tool\nutilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of\naverage pass rate on ToolBench. Code will be available at\nhttps://github.com/dyabel/AnyTool.",
        "score": -3.3669021129608154
      },
      {
        "title": "Code Generation for Unknown Libraries via Reading API Documentations,",
        "abstract": "Open-domain code generation is a challenging problem because the set of\nfunctions and classes that we use are frequently changed and extended in\nprogramming communities. We consider the challenge of code generation for\nunknown libraries without additional training. In this paper, we explore a\nframework of code generation that can refer to relevant API documentations like\nhuman programmers to handle unknown libraries. As a first step of this\ndirection, we implement a model that can extract relevant code signatures from\nAPI documentations based on a natural language intent and copy primitives from\nthe extracted signatures. Moreover, to evaluate code generation for unknown\nlibraries and our framework, we extend an existing dataset of open-domain code\ngeneration and resplit it so that the evaluation data consist of only examples\nusing the libraries that do not appear in the training data. Experiments on our\nnew split show that baseline encoder-decoder models cannot generate code using\nprimitives of unknown libraries as expected. In contrast, our model outperforms\nthe baseline on the new split and can properly generate unknown primitives when\nextracted code signatures are noiseless.",
        "score": -3.5017430782318115
      },
      {
        "title": "Pop Quiz! Do Pre-trained Code Models Possess Knowledge of Correct API\n  Names?,",
        "abstract": "Recent breakthroughs in pre-trained code models, such as CodeBERT and Codex,\nhave shown their superior performance in various downstream tasks. The\ncorrectness and unambiguity of API usage among these code models are crucial\nfor achieving desirable program functionalities, requiring them to learn\nvarious API fully qualified names structurally and semantically. Recent studies\nreveal that even state-of-the-art pre-trained code models struggle with\nsuggesting the correct APIs during code generation. However, the reasons for\nsuch poor API usage performance are barely investigated. To address this\nchallenge, we propose using knowledge probing as a means of interpreting code\nmodels, which uses cloze-style tests to measure the knowledge stored in models.\nOur comprehensive study examines a code model's capability of understanding API\nfully qualified names from two different perspectives: API call and API import.\nSpecifically, we reveal that current code models struggle with understanding\nAPI names, with pre-training strategies significantly affecting the quality of\nAPI name learning. We demonstrate that natural language context can assist code\nmodels in locating Python API names and generalize Python API name knowledge to\nunseen data. Our findings provide insights into the limitations and\ncapabilities of current pre-trained code models, and suggest that incorporating\nAPI structure into the pre-training process can improve automated API usage and\ncode representations. This work provides significance for advancing code\nintelligence practices and direction for future studies. All experiment\nresults, data and source code used in this work are available at\n\\url{https://doi.org/10.5281/zenodo.7902072}.",
        "score": -3.674142837524414
      },
      {
        "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving\n  Pipelines,",
        "abstract": "The ML community is rapidly exploring techniques for prompting language\nmodels (LMs) and for stacking them into pipelines that solve complex tasks.\nUnfortunately, existing LM pipelines are typically implemented using hard-coded\n\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward\na more systematic approach for developing and optimizing LM pipelines, we\nintroduce DSPy, a programming model that abstracts LM pipelines as text\ntransformation graphs, i.e. imperative computational graphs where LMs are\ninvoked through declarative modules. DSPy modules are parameterized, meaning\nthey can learn (by creating and collecting demonstrations) how to apply\ncompositions of prompting, finetuning, augmentation, and reasoning techniques.\nWe design a compiler that will optimize any DSPy pipeline to maximize a given\nmetric. We conduct two case studies, showing that succinct DSPy programs can\nexpress and optimize sophisticated LM pipelines that reason about math word\nproblems, tackle multi-hop retrieval, answer complex questions, and control\nagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and\nllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot\nprompting (generally by over 25% and 65%, respectively) and pipelines with\nexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top\nof that, DSPy programs compiled to open and relatively small LMs like\n770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely\non expert-written prompt chains for proprietary GPT-3.5. DSPy is available at\nhttps://github.com/stanfordnlp/dspy",
        "score": -4.709413528442383
      },
      {
        "title": "API-Assisted Code Generation for Question Answering on Varied Table\n  Structures,",
        "abstract": "A persistent challenge to table question answering (TableQA) by generating\nexecutable programs has been adapting to varied table structures, typically\nrequiring domain-specific logical forms. In response, this paper introduces a\nunified TableQA framework that: (1) provides a unified representation for\nstructured tables as multi-index Pandas data frames, (2) uses Python as a\npowerful querying language, and (3) uses few-shot prompting to translate NL\nquestions into Python programs, which are executable on Pandas data frames.\nFurthermore, to answer complex relational questions with extended program\nfunctionality and external knowledge, our framework allows customized APIs that\nPython programs can call. We experiment with four TableQA datasets that involve\ntables of different structures -- relational, multi-table, and hierarchical\nmatrix shapes -- and achieve prominent improvements over past state-of-the-art\nsystems. In ablation studies, we (1) show benefits from our multi-index\nrepresentation and APIs over baselines that use only an LLM, and (2)\ndemonstrate that our approach is modular and can incorporate additional APIs.",
        "score": -5.259654998779297
      }
    ]
  },
  {
    "title": "NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals",
    "abstract": "  Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm.\n",
    "related_paper_titles": [
      "Transformer-based Spatial-Temporal Feature Learning for EEG Decoding",
      "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large\n  Language Models",
      "The Llama 3 Herd of Models"
    ],
    "related_paper_abstract": [
      "  At present, people usually use some methods based on convolutional neural\nnetworks (CNNs) for Electroencephalograph (EEG) decoding. However, CNNs have\nlimitations in perceiving global dependencies, which is not adequate for common\nEEG paradigms with a strong overall relationship. Regarding this issue, we\npropose a novel EEG decoding method that mainly relies on the attention\nmechanism. The EEG data is firstly preprocessed and spatially filtered. And\nthen, we apply attention transforming on the feature-channel dimension so that\nthe model can enhance more relevant spatial features. The most crucial step is\nto slice the data in the time dimension for attention transforming, and finally\nobtain a highly distinguishable representation. At this time, global averaging\npooling and a simple fully-connected layer are used to classify different\ncategories of EEG data. Experiments on two public datasets indicate that the\nstrategy of attention transforming effectively utilizes spatial and temporal\nfeatures. And we have reached the level of the state-of-the-art in\nmulti-classification of EEG, with fewer parameters. As far as we know, it is\nthe first time that a detailed and complete method based on the transformer\nidea has been proposed in this field. It has good potential to promote the\npracticality of brain-computer interface (BCI). The source code can be found\nat: \\textit{https://github.com/anranknight/EEG-Transformer}.\n",
      "  The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such\nas directly generating websites from handwritten text and identifying humorous\nelements within images. These features are rarely observed in previous\nvision-language models. However, the technical details behind GPT-4 continue to\nremain undisclosed. We believe that the enhanced multi-modal generation\ncapabilities of GPT-4 stem from the utilization of sophisticated large language\nmodels (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a\nfrozen visual encoder with a frozen advanced LLM, Vicuna, using one projection\nlayer. Our work, for the first time, uncovers that properly aligning the visual\nfeatures with an advanced large language model can possess numerous advanced\nmulti-modal abilities demonstrated by GPT-4, such as detailed image description\ngeneration and website creation from hand-drawn drafts. Furthermore, we also\nobserve other emerging capabilities in MiniGPT-4, including writing stories and\npoems inspired by given images, teaching users how to cook based on food\nphotos, and so on. In our experiment, we found that the model trained on short\nimage caption pairs could produce unnatural language outputs (e.g., repetition\nand fragmentation). To address this problem, we curate a detailed image\ndescription dataset in the second stage to finetune the model, which\nconsequently improves the model's generation reliability and overall usability.\nOur code, pre-trained model, and collected dataset are available at\nhttps://minigpt-4.github.io/.\n",
      "  Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.\n"
    ],
    "entities": [
      "Large Language Models Large Language Models",
      "Large Language",
      "KGs",
      "Chinese",
      "MLLMs",
      "LLaMA",
      "Retrieval Augmented",
      "Gemini",
      "Large Language Model",
      "IFT",
      "ESG",
      "Knowledge Graphs",
      "KV",
      "Python",
      "KG",
      "Persian",
      "Retrieval",
      "SQL",
      "APIs",
      "KGE",
      "PEFT",
      "NER",
      "HAR",
      "PTQ",
      "HumanEval",
      "YOLOv8",
      "Multimodal Large Language Models",
      "MMLU",
      "Verilog",
      "GPU"
    ],
    "retrieved_papers": [
      {
        "title": "Large Language Model as a Universal Clinical Multi-task Decoder,",
        "abstract": "The development of effective machine learning methodologies for enhancing the\nefficiency and accuracy of clinical systems is crucial. Despite significant\nresearch efforts, managing a plethora of diversified clinical tasks and\nadapting to emerging new tasks remain significant challenges. This paper\npresents a novel paradigm that employs a pre-trained large language model as a\nuniversal clinical multi-task decoder. This approach leverages the flexibility\nand diversity of language expressions to handle task topic variations and\nassociated arguments. The introduction of a new task simply requires the\naddition of a new instruction template. We validate this framework across\nhundreds of tasks, demonstrating its robustness in facilitating multi-task\npredictions, performing on par with traditional multi-task learning and\nsingle-task learning approaches. Moreover, it shows exceptional adaptability to\nnew tasks, with impressive zero-shot performance in some instances and superior\ndata efficiency in few-shot scenarios. This novel approach offers a unified\nsolution to manage a wide array of new and emerging tasks in clinical\napplications.",
        "score": 1.641947627067566
      },
      {
        "title": "UMBRAE: Unified Multimodal Brain Decoding,",
        "abstract": "We address prevailing challenges of the brain-powered research, departing\nfrom the observation that the literature hardly recover accurate spatial\ninformation and require subject-specific models. To address these challenges,\nwe propose UMBRAE, a unified multimodal decoding of brain signals. First, to\nextract instance-level conceptual and spatial details from neural signals, we\nintroduce an efficient universal brain encoder for multimodal-brain alignment\nand recover object descriptions at multiple levels of granularity from\nsubsequent multimodal large language model (MLLM). Second, we introduce a\ncross-subject training strategy mapping subject-specific features to a common\nfeature space. This allows a model to be trained on multiple subjects without\nextra resources, even yielding superior results compared to subject-specific\nmodels. Further, we demonstrate this supports weakly-supervised adaptation to\nnew subjects, with only a fraction of the total training data. Experiments\ndemonstrate that UMBRAE not only achieves superior results in the newly\nintroduced tasks but also outperforms methods in well established tasks. To\nassess our method, we construct and share with the community a comprehensive\nbrain understanding benchmark BrainHub. Our code and benchmark are available at\nhttps://weihaox.github.io/UMBRAE.",
        "score": 0.7066165804862976
      },
      {
        "title": "Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap\n  for Prompt-Based Large Language Models and Beyond,",
        "abstract": "Task embedding, a meta-learning technique that captures task-specific\ninformation, has gained popularity, especially in areas such as multi-task\nlearning, model editing, and interpretability. However, it faces challenges\nwith the emergence of prompt-guided Large Language Models (LLMs) operating in a\ngradient-free manner. Existing task embedding methods rely on fine-tuned,\ntask-specific language models, which hinders the adaptability of task\nembeddings across diverse models, especially prompt-based LLMs. To hardness the\npotential of task embeddings in the era of LLMs, we propose a framework for\nunified task embeddings (FUTE), harmonizing task embeddings from various\nmodels, including smaller language models and LLMs with varied prompts, within\na single vector space. Such uniformity enables comparison and analysis of\nsimilarities amongst different models, broadening the scope and utility of\nexisting task embedding methods in multi-model scenarios, while maintaining\ntheir performance comparable to architecture-specific methods.",
        "score": 0.581521213054657
      },
      {
        "title": "EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and\n  Multi-View Transformer,",
        "abstract": "Deciphering the intricacies of the human brain has captivated curiosity for\ncenturies. Recent strides in Brain-Computer Interface (BCI) technology,\nparticularly using motor imagery, have restored motor functions such as\nreaching, grasping, and walking in paralyzed individuals. However, unraveling\nnatural language from brain signals remains a formidable challenge.\nElectroencephalography (EEG) is a non-invasive technique used to record\nelectrical activity in the brain by placing electrodes on the scalp. Previous\nstudies of EEG-to-text decoding have achieved high accuracy on small closed\nvocabularies, but still fall short of high accuracy when dealing with large\nopen vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy\nof open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG\npre-training to enhance the learning of semantics from EEG signals and proposes\na multi-view transformer to model the EEG signal processing by different\nspatial regions of the brain. Experiments show that EEG2TEXT has superior\nperformance, outperforming the state-of-the-art baseline methods by a large\nmargin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great\npotential for a high-performance open-vocabulary brain-to-text system to\nfacilitate communication.",
        "score": 0.5329785943031311
      },
      {
        "title": "Leveraging sinusoidal representation networks to predict fMRI signals\n  from EEG,",
        "abstract": "In modern neuroscience, functional magnetic resonance imaging (fMRI) has been\na crucial and irreplaceable tool that provides a non-invasive window into the\ndynamics of whole-brain activity. Nevertheless, fMRI is limited by hemodynamic\nblurring as well as high cost, immobility, and incompatibility with metal\nimplants. Electroencephalography (EEG) is complementary to fMRI and can\ndirectly record the cortical electrical activity at high temporal resolution,\nbut has more limited spatial resolution and is unable to recover information\nabout deep subcortical brain structures. The ability to obtain fMRI information\nfrom EEG would enable cost-effective, imaging across a wider set of brain\nregions. Further, beyond augmenting the capabilities of EEG, cross-modality\nmodels would facilitate the interpretation of fMRI signals. However, as both\nEEG and fMRI are high-dimensional and prone to artifacts, it is currently\nchallenging to model fMRI from EEG. To address this challenge, we propose a\nnovel architecture that can predict fMRI signals directly from multi-channel\nEEG without explicit feature engineering. Our model achieves this by\nimplementing a Sinusoidal Representation Network (SIREN) to learn frequency\ninformation in brain dynamics from EEG, which serves as the input to a\nsubsequent encoder-decoder to effectively reconstruct the fMRI signal from a\nspecific brain region. We evaluate our model using a simultaneous EEG-fMRI\ndataset with 8 subjects and investigate its potential for predicting\nsubcortical fMRI signals. The present results reveal that our model outperforms\na recent state-of-the-art model, and indicates the potential of leveraging\nperiodic activation functions in deep neural networks to model functional\nneuroimaging data.",
        "score": -0.22065888345241547
      },
      {
        "title": "SpeechVerse: A Large-scale Generalizable Audio Language Model,",
        "abstract": "Large language models (LLMs) have shown incredible proficiency in performing\ntasks that require semantic understanding of natural language instructions.\nRecently, many works have further expanded this capability to perceive\nmultimodal audio and text inputs, but their capabilities are often limited to\nspecific fine-tuned tasks such as automatic speech recognition and translation.\nWe therefore develop SpeechVerse, a robust multi-task training and curriculum\nlearning framework that combines pre-trained speech and text foundation models\nvia a small set of learnable parameters, while keeping the pre-trained models\nfrozen during training. The models are instruction finetuned using continuous\nlatent representations extracted from the speech foundation model to achieve\noptimal zero-shot performance on a diverse range of speech processing tasks\nusing natural language instructions. We perform extensive benchmarking that\nincludes comparing our model performance against traditional baselines across\nseveral datasets and tasks. Furthermore, we evaluate the model's capability for\ngeneralized instruction following by testing on out-of-domain datasets, novel\nprompts, and unseen tasks. Our empirical experiments reveal that our multi-task\nSpeechVerse model is even superior to conventional task-specific baselines on 9\nout of the 11 tasks.",
        "score": -0.2905934751033783
      },
      {
        "title": "UniverSLU: Universal Spoken Language Understanding for Diverse Tasks\n  with Natural Language Instructions,",
        "abstract": "Recent studies leverage large language models with multi-tasking\ncapabilities, using natural language prompts to guide the model's behavior and\nsurpassing performance of task-specific models. Motivated by this, we ask: can\nwe build a single model that jointly performs various spoken language\nunderstanding (SLU) tasks? We start by adapting a pre-trained automatic speech\nrecognition model to additional tasks using single-token task specifiers. We\nenhance this approach through instruction tuning, i.e., finetuning by\ndescribing the task using natural language instructions followed by the list of\nlabel options. Our approach can generalize to new task descriptions for the\nseen tasks during inference, thereby enhancing its user-friendliness. We\ndemonstrate the efficacy of our single multi-task learning model \"UniverSLU\"\nfor 12 speech classification and sequence generation task types spanning 17\ndatasets and 9 languages. On most tasks, UniverSLU achieves competitive\nperformance and often even surpasses task-specific models. Additionally, we\nassess the zero-shot capabilities, finding that the model generalizes to new\ndatasets and languages for seen task types.",
        "score": -0.4595130681991577
      },
      {
        "title": "Device Tuning for Multi-Task Large Model,",
        "abstract": "Unsupervised pre-training approaches have achieved great success in many\nfields such as Computer Vision (CV), Natural Language Processing (NLP) and so\non. However, compared to typical deep learning models, pre-training or even\nfine-tuning the state-of-the-art self-attention models is extremely expensive,\nas they require much more computational and memory resources. It severely\nlimits their applications and success in a variety of domains, especially for\nmulti-task learning. To improve the efficiency, we propose Device Tuning for\nthe efficient multi-task model, which is a massively multitask framework across\nthe cloud and device and is designed to encourage learning of representations\nthat generalize better to many different tasks. Specifically, we design Device\nTuning architecture of a multi-task model that benefits both cloud modelling\nand device modelling, which reduces the communication between device and cloud\nby representation compression. Experimental results demonstrate the\neffectiveness of our proposed method.",
        "score": -0.5145362019538879
      },
      {
        "title": "BrainVis: Exploring the Bridge between Brain and Visual Signals via\n  Image Reconstruction,",
        "abstract": "Analyzing and reconstructing visual stimuli from brain signals effectively\nadvances understanding of the human visual system. However, the EEG signals are\ncomplex and contain a amount of noise. This leads to substantial limitations in\nexisting works of visual stimuli reconstruction from EEG, such as difficulties\nin aligning EEG embeddings with the fine-grained semantic information and a\nheavy reliance on additional large self-collected dataset for training. To\naddress these challenges, we propose a novel approach called BrainVis. Firstly,\nwe divide the EEG signals into various units and apply a self-supervised\napproach on them to obtain EEG time-domain features, in an attempt to ease the\ntraining difficulty. Additionally, we also propose to utilize the\nfrequency-domain features to enhance the EEG representations. Then, we\nsimultaneously align EEG time-frequency embeddings with the interpolation of\nthe coarse and fine-grained semantics in the CLIP space, to highlight the\nprimary visual components and reduce the cross-modal alignment difficulty.\nFinally, we adopt the cascaded diffusion models to reconstruct images. Our\nproposed BrainVis outperforms state of the arts in both semantic fidelity\nreconstruction and generation quality. Notably, we reduce the training data\nscale to 10% of the previous work.",
        "score": -0.7580364346504211
      },
      {
        "title": "The first step is the hardest: Pitfalls of Representing and Tokenizing\n  Temporal Data for Large Language Models,",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization\nacross diverse tasks, leading individuals to increasingly use them as personal\nassistants and universal computing engines. Nevertheless, a notable obstacle\nemerges when feeding numerical/temporal data into these models, such as data\nsourced from wearables or electronic health records. LLMs employ tokenizers in\ntheir input that break down text into smaller units. However, tokenizers are\nnot designed to represent numerical values and might struggle to understand\nrepetitive patterns and context, treating consecutive values as separate tokens\nand disregarding their temporal relationships. Here, we discuss recent works\nthat employ LLMs for human-centric tasks such as in mobile health sensing and\npresent a case study showing that popular LLMs tokenize temporal data\nincorrectly. To address that, we highlight potential solutions such as prompt\ntuning with lightweight embedding layers as well as multimodal adapters, that\ncan help bridge this \"modality gap\". While the capability of language models to\ngeneralize to other modalities with minimal or no finetuning is exciting, this\npaper underscores the fact that their outputs cannot be meaningful if they\nstumble over input nuances.",
        "score": -0.7626046538352966
      },
      {
        "title": "Semantic-aware Contrastive Learning for Electroencephalography-to-Text\n  Generation with Curriculum Learning,",
        "abstract": "Electroencephalography-to-Text generation (EEG-to-Text), which aims to\ndirectly generate natural text from EEG signals has drawn increasing attention\nin recent years due to the enormous potential for Brain-computer interfaces\n(BCIs). However, the remarkable discrepancy between the subject-dependent EEG\nrepresentation and the semantic-dependent text representation poses a great\nchallenge to this task. To mitigate this challenge, we devise a Curriculum\nSemantic-aware Contrastive Learning strategy (C-SCL), which effectively\nre-calibrates the subject-dependent EEG representation to the\nsemantic-dependent EEG representation, thus reducing the discrepancy.\nSpecifically, our C-SCL pulls semantically similar EEG representations together\nwhile pushing apart dissimilar ones. Besides, in order to introduce more\nmeaningful contrastive pairs, we carefully employ curriculum learning to not\nonly craft meaningful contrastive pairs but also make the learning\nprogressively. We conduct extensive experiments on the ZuCo benchmark and our\nmethod combined with diverse models and architectures shows stable improvements\nacross three types of metrics while achieving the new state-of-the-art. Further\ninvestigation proves not only its superiority in both the single-subject and\nlow-resource settings but also its robust generalizability in the zero-shot\nsetting.",
        "score": -0.8016519546508789
      },
      {
        "title": "BELT:Bootstrapping Electroencephalography-to-Language Decoding and\n  Zero-Shot Sentiment Classification by Natural Language Supervision,",
        "abstract": "This paper presents BELT, a novel model and learning framework for the\npivotal topic of brain-to-language translation research. The translation from\nnoninvasive brain signals into readable natural language has the potential to\npromote the application scenario as well as the development of brain-computer\ninterfaces (BCI) as a whole. The critical problem in brain signal decoding or\nbrain-to-language translation is the acquisition of semantically appropriate\nand discriminative EEG representation from a dataset of limited scale and\nquality. The proposed BELT method is a generic and efficient framework that\nbootstraps EEG representation learning using off-the-shelf large-scale\npretrained language models (LMs). With a large LM's capacity for understanding\nsemantic information and zero-shot generalization, BELT utilizes large LMs\ntrained on Internet-scale datasets to bring significant improvements to the\nunderstanding of EEG signals.\n  In particular, the BELT model is composed of a deep conformer encoder and a\nvector quantization encoder. Semantical EEG representation is achieved by a\ncontrastive learning step that provides natural language supervision. We\nachieve state-of-the-art results on two featuring brain decoding tasks\nincluding the brain-to-language translation and zero-shot sentiment\nclassification. Specifically, our model surpasses the baseline model on both\ntasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32%\nprecision on the main evaluation metrics for translation and zero-shot\nsentiment classification respectively.",
        "score": -0.8936328291893005
      },
      {
        "title": "Global Contrastive Training for Multimodal Electronic Health Records\n  with Language Supervision,",
        "abstract": "Modern electronic health records (EHRs) hold immense promise in tracking\npersonalized patient health trajectories through sequential deep learning,\nowing to their extensive breadth, scale, and temporal granularity. Nonetheless,\nhow to effectively leverage multiple modalities from EHRs poses significant\nchallenges, given its complex characteristics such as high dimensionality,\nmultimodality, sparsity, varied recording frequencies, and temporal\nirregularities. To this end, this paper introduces a novel multimodal\ncontrastive learning framework, specifically focusing on medical time series\nand clinical notes. To tackle the challenge of sparsity and irregular time\nintervals in medical time series, the framework integrates temporal\ncross-attention transformers with a dynamic embedding and tokenization scheme\nfor learning multimodal feature representations. To harness the interconnected\nrelationships between medical time series and clinical notes, the framework\nequips a global contrastive loss, aligning a patient's multimodal feature\nrepresentations with the corresponding discharge summaries. Since discharge\nsummaries uniquely pertain to individual patients and represent a holistic view\nof the patient's hospital stay, machine learning models are led to learn\ndiscriminative multimodal features via global contrasting. Extensive\nexperiments with a real-world EHR dataset demonstrated that our framework\noutperformed state-of-the-art approaches on the exemplar task of predicting the\noccurrence of nine postoperative complications for more than 120,000 major\ninpatient surgeries using multimodal data from UF health system split among\nthree hospitals (UF Health Gainesville, UF Health Jacksonville, and UF Health\nJacksonville-North).",
        "score": -0.9366967678070068
      },
      {
        "title": "Enhancing EEG-to-Text Decoding through Transferable Representations from\n  Pre-trained Contrastive EEG-Text Masked Autoencoder,",
        "abstract": "Reconstructing natural language from non-invasive electroencephalography\n(EEG) holds great promise as a language decoding technology for brain-computer\ninterfaces (BCIs). However, EEG-based language decoding is still in its nascent\nstages, facing several technical issues such as: 1) Absence of a hybrid\nstrategy that can effectively integrate cross-modality (between EEG and text)\nself-learning with intra-modality self-reconstruction of EEG features or\ntextual sequences; 2) Under-utilization of large language models (LLMs) to\nenhance EEG-based language decoding. To address above issues, we propose the\nContrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that\norchestrates compound self-supervised learning across and within EEG and text\nthrough a dedicated multi-stream encoder. Furthermore, we develop a framework\ncalled E2T-PTR (EEG-to-Text decoding using Pretrained Transferable\nRepresentations), which leverages pre-trained modules alongside the EEG stream\nfrom CET-MAE and further enables an LLM (specifically BART) to decode text from\nEEG sequences. Comprehensive experiments conducted on the popular text-evoked\nEEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms\nthe state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%,\nrespectively. These results indicate significant advancements in the field and\nunderscores the proposed framework's potential to enable more powerful and\nwidespread BCI applications.",
        "score": -0.9618561863899231
      },
      {
        "title": "Du-IN: Discrete units-guided mask modeling for decoding speech from\n  Intracranial Neural signals,",
        "abstract": "Invasive brain-computer interfaces have garnered significant attention due to\ntheir high performance. The current intracranial stereoElectroEncephaloGraphy\n(sEEG) foundation models typically build univariate representations based on a\nsingle channel. Some of them further use Transformer to model the relationship\namong channels. However, due to the locality and specificity of brain\ncomputation, their performance on more difficult tasks, e.g., speech decoding,\nwhich demands intricate processing in specific brain regions, is yet to be\nfully investigated. We hypothesize that building multi-variate representations\nwithin certain brain regions can better capture the specific neural processing.\nTo explore this hypothesis, we collect a well-annotated Chinese word-reading\nsEEG dataset, targeting language-related brain networks, over 12 subjects.\nLeveraging this benchmark dataset, we developed the Du-IN model that can\nextract contextual embeddings from specific brain regions through discrete\ncodebook-guided mask modeling. Our model achieves SOTA performance on the\ndownstream 61-word classification task, surpassing all baseline models. Model\ncomparison and ablation analysis reveal that our design choices, including (i)\nmulti-variate representation by fusing channels in vSMC and STG regions and\n(ii) self-supervision by discrete codebook-guided mask modeling, significantly\ncontribute to these performances. Collectively, our approach, inspired by\nneuroscience findings, capitalizing on multi-variate neural representation from\nspecific brain regions, is suitable for invasive brain modeling. It marks a\npromising neuro-inspired AI approach in BCI.",
        "score": -1.3511700630187988
      },
      {
        "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard\n  Parameter Sharing,",
        "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed\nmulti-tasking methods with soft parameter sharing which leverage machine\ntranslation (MT) data via secondary encoders that map text inputs to an\neventual cross-modal representation. In this work, we instead propose a ST/MT\nmulti-tasking framework with hard parameter sharing in which all model\nparameters are shared cross-modally. Our method reduces the speech-text\nmodality gap via a pre-processing stage which converts speech and text inputs\ninto two discrete token sequences of similar length -- this allows models to\nindiscriminately process both modalities simply using a joint vocabulary. With\nexperiments on MuST-C, we demonstrate that our multi-tasking framework improves\nattentional encoder-decoder, Connectionist Temporal Classification (CTC),\ntransducer, and joint CTC/attention models by an average of +0.5 BLEU without\nany external MT data. Further, we show that this framework incorporates\nexternal MT data, yielding +0.8 BLEU, and also improves transfer learning from\npre-trained textual models, yielding +1.8 BLEU.",
        "score": -1.3663166761398315
      },
      {
        "title": "MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor\n  Imagery EEG Classification,",
        "abstract": "Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.",
        "score": -1.3740234375
      },
      {
        "title": "UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive\n  signals and human language,",
        "abstract": "Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our\nunderstanding of the human language system, paving the way for building\nversatile Brain-Computer Interface. However, existing studies largely focus on\ndecoding individual word-level fMRI volumes from a restricted vocabulary, which\nis far too idealized for real-world application. In this paper, we propose\nfMRI2text, the first openvocabulary task aiming to bridge fMRI time series and\nhuman language. Furthermore, to explore the potential of this new task, we\npresent a baseline solution, UniCoRN: the Unified Cognitive Signal\nReconstructioN for Brain Decoding. By reconstructing both individual time\npoints and time series, UniCoRN establishes a robust encoder for cognitive\nsignals (fMRI & EEG). Leveraging a pre-trained language model as decoder,\nUniCoRN proves its efficacy in decoding coherent text from fMRI series across\nvarious split settings. Our model achieves a 34.77% BLEU score on fMRI2text,\nand a 37.04% BLEU when generalized to EEGto-text decoding, thereby surpassing\nthe former baseline. Experimental results indicate the feasibility of decoding\nconsecutive fMRI volumes, and the effectiveness of decoding different cognitive\nsignals using a unified structure.",
        "score": -1.6371657848358154
      },
      {
        "title": "Towards General Purpose Medical AI: Continual Learning Medical\n  Foundation Model,",
        "abstract": "Inevitable domain and task discrepancies in real-world scenarios can impair\nthe generalization performance of the pre-trained deep models for medical data.\nTherefore, we audaciously propose that we should build a general-purpose\nmedical AI system that can be seamlessly adapted to downstream domains/tasks.\nSince the domain/task adaption procedures usually involve additional labeling\nwork for the target data, designing a data-efficient adaption algorithm is\ndesired to save the cost of transferring the learned knowledge. Our recent work\nfound that vision-language models (VLMs) are efficient learners with\nextraordinary cross-domain ability. Therefore, in this work, we further explore\nthe possibility of leveraging pre-trained VLMs as medical foundation models for\nbuilding general-purpose medical AI, where we thoroughly investigate three\nmachine-learning paradigms, i.e., domain/task-specialized learning, joint\nlearning, and continual learning, for training the VLMs and evaluate their\ngeneralization performance on cross-domain and cross-task test sets. To\nalleviate the catastrophic forgetting during sequential training, we employ\nrehearsal learning and receive a sharp boost in terms of generalization\ncapability. In a nutshell, our empirical evidence suggests that continual\nlearning may be a practical and efficient learning paradigm for the medical\nfoundation model. And we hope researchers can use our empirical evidence as\nbasement to further explore the path toward medical foundation model.",
        "score": -1.9507956504821777
      },
      {
        "title": "DualTime: A Dual-Adapter Multimodal Language Model for Time Series\n  Representation,",
        "abstract": "The recent rapid development of language models (LMs) has attracted attention\nin the field of time series, including multimodal time series modeling.\nHowever, we note that current time series multimodal methods are biased, often\nassigning a primary role to one modality while the other assumes a secondary\nrole. They overlook the mutual benefits and complementary of different\nmodalities. For example, in seizure diagnosis, relying solely on textual\nclinical reports makes it difficult to pinpoint the area and type of the\ndisease, while electroencephalograms (EEGs) alone cannot provide an accurate\ndiagnosis without considering the symptoms. In this study, based on the\ncomplementary information mining of time series multimodal data, we propose\nDualTime, a Dual-adapter multimodal language model for Time series\nrepresentation implementing temporal-primary and textual-primary modeling\nsimultaneously. By injecting lightweight adaption tokens, the LM pipeline\nshared by dual adapters encourages embedding alignment and achieves efficient\nfine-tuning. Empirically, our method outperforms state-of-the-art models in\nboth supervised and unsupervised settings, highlighting the complementary\nbenefits of different modalities. In addition, we conduct few-shot label\ntransfer experiments, which further verifies the transferability and\nexpressiveness of our proposed DualTime.",
        "score": -2.1998684406280518
      },
      {
        "title": "Helping the Weak Makes You Strong: Simple Multi-Task Learning Improves\n  Non-Autoregressive Translators,",
        "abstract": "Recently, non-autoregressive (NAR) neural machine translation models have\nreceived increasing attention due to their efficient parallel decoding.\nHowever, the probabilistic framework of NAR models necessitates conditional\nindependence assumption on target sequences, falling short of characterizing\nhuman language data. This drawback results in less informative learning signals\nfor NAR models under conventional MLE training, thereby yielding unsatisfactory\naccuracy compared to their autoregressive (AR) counterparts. In this paper, we\npropose a simple and model-agnostic multi-task learning framework to provide\nmore informative learning signals. During training stage, we introduce a set of\nsufficiently weak AR decoders that solely rely on the information provided by\nNAR decoder to make prediction, forcing the NAR decoder to become stronger or\nelse it will be unable to support its weak AR partners. Experiments on WMT and\nIWSLT datasets show that our approach can consistently improve accuracy of\nmultiple NAR baselines without adding any additional decoding overhead.",
        "score": -2.245762586593628
      },
      {
        "title": "Cross-Subject Domain Adaptation for Classifying Working Memory Load with\n  Multi-Frame EEG Images,",
        "abstract": "Working memory (WM), denoting the information temporally stored in the mind,\nis a fundamental research topic in the field of human cognition.\nElectroencephalograph (EEG), which can monitor the electrical activity of the\nbrain, has been widely used in measuring the level of WM. However, one of the\ncritical challenges is that individual differences may cause ineffective\nresults, especially when the established model meets an unfamiliar subject. In\nthis work, we propose a cross-subject deep adaptation model with spatial\nattention (CS-DASA) to generalize the workload classifications across subjects.\nFirst, we transform EEG time series into multi-frame EEG images incorporating\nspatial, spectral, and temporal information. First, the Subject-Shared module\nin CS-DASA receives multi-frame EEG image data from both source and target\nsubjects and learns the common feature representations. Then, in the\nsubject-specific module, the maximum mean discrepancy is implemented to measure\nthe domain distribution divergence in a reproducing kernel Hilbert space, which\ncan add an effective penalty loss for domain adaptation. Additionally, the\nsubject-to-subject spatial attention mechanism is employed to focus on the\ndiscriminative spatial features from the target image data. Experiments\nconducted on a public WM EEG dataset containing 13 subjects show that the\nproposed model is capable of achieving better performance than existing\nstate-of-the-art methods.",
        "score": -2.3128018379211426
      },
      {
        "title": "EIT-1M: One Million EEG-Image-Text Pairs for Human Visual-textual\n  Recognition and More,",
        "abstract": "Recently, electroencephalography (EEG) signals have been actively\nincorporated to decode brain activity to visual or textual stimuli and achieve\nobject recognition in multi-modal AI. Accordingly, endeavors have been focused\non building EEG-based datasets from visual or textual single-modal stimuli.\nHowever, these datasets offer limited EEG epochs per category, and the complex\nsemantics of stimuli presented to participants compromise their quality and\nfidelity in capturing precise brain activity. The study in neuroscience unveils\nthat the relationship between visual and textual stimulus in EEG recordings\nprovides valuable insights into the brain's ability to process and integrate\nmulti-modal information simultaneously. Inspired by this, we propose a novel\nlarge-scale multi-modal dataset, named EIT-1M, with over 1 million\nEEG-image-text pairs. Our dataset is superior in its capacity of reflecting\nbrain activities in simultaneously processing multi-modal information. To\nachieve this, we collected data pairs while participants viewed alternating\nsequences of visual-textual stimuli from 60K natural images and\ncategory-specific texts. Common semantic categories are also included to elicit\nbetter reactions from participants' brains. Meanwhile, response-based stimulus\ntiming and repetition across blocks and sessions are included to ensure data\ndiversity. To verify the effectiveness of EIT-1M, we provide an in-depth\nanalysis of EEG data captured from multi-modal stimuli across different\ncategories and participants, along with data quality scores for transparency.\nWe demonstrate its validity on two tasks: 1) EEG recognition from visual or\ntextual stimuli or both and 2) EEG-to-visual generation.",
        "score": -2.520052671432495
      },
      {
        "title": "Deep Representation Learning for Open Vocabulary\n  Electroencephalography-to-Text Decoding,",
        "abstract": "Previous research has demonstrated the potential of using pre-trained\nlanguage models for decoding open vocabulary Electroencephalography (EEG)\nsignals captured through a non-invasive Brain-Computer Interface (BCI).\nHowever, the impact of embedding EEG signals in the context of language models\nand the effect of subjectivity, remain unexplored, leading to uncertainty about\nthe best approach to enhance decoding performance. Additionally, current\nevaluation metrics used to assess decoding effectiveness are predominantly\nsyntactic and do not provide insights into the comprehensibility of the decoded\noutput for human understanding. We present an end-to-end deep learning\nframework for non-invasive brain recordings that brings modern representational\nlearning approaches to neuroscience. Our proposal introduces the following\ninnovations: 1) an end-to-end deep learning architecture for open vocabulary\nEEG decoding, incorporating a subject-dependent representation learning module\nfor raw EEG encoding, a BART language model, and a GPT-4 sentence refinement\nmodule; 2) a more comprehensive sentence-level evaluation metric based on the\nBERTScore; 3) an ablation study that analyses the contributions of each module\nwithin our proposal, providing valuable insights for future research. We\nevaluate our approach on two publicly available datasets, ZuCo v1.0 and v2.0,\ncomprising EEG recordings of 30 subjects engaged in natural reading tasks. Our\nmodel achieves a BLEU-1 score of 42.75%, a ROUGE-1-F of 33.28%, and a\nBERTScore-F of 53.86%, outperforming the previous state-of-the-art methods by\n3.38%, 8.43%, and 6.31%, respectively.",
        "score": -2.5276002883911133
      },
      {
        "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning,",
        "abstract": "Despite the recent success of multi-task learning and transfer learning for\nnatural language processing (NLP), few works have systematically studied the\neffect of scaling up the number of tasks during pre-training. Towards this\ngoal, this paper introduces ExMix (Extreme Mixture): a massive collection of\n107 supervised NLP tasks across diverse domains and task-families. Using ExMix,\nwe study the effect of multi-task pre-training at the largest scale to date,\nand analyze co-training transfer amongst common families of tasks. Through this\nanalysis, we show that manually curating an ideal set of tasks for multi-task\npre-training is not straightforward, and that multi-task scaling can vastly\nimprove models on its own. Finally, we propose ExT5: a model pre-trained using\na multi-task objective of self-supervised span denoising and supervised ExMix.\nVia extensive experiments, we show that ExT5 outperforms strong T5 baselines on\nSuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of\nExMix. ExT5 also significantly improves sample efficiency while pre-training.",
        "score": -2.7230446338653564
      },
      {
        "title": "Analyzing EEG Data with Machine and Deep Learning: A Benchmark,",
        "abstract": "Nowadays, machine and deep learning techniques are widely used in different\nareas, ranging from economics to biology. In general, these techniques can be\nused in two ways: trying to adapt well-known models and architectures to the\navailable data, or designing custom architectures. In both cases, to speed up\nthe research process, it is useful to know which type of models work best for a\nspecific problem and/or data type. By focusing on EEG signal analysis, and for\nthe first time in literature, in this paper a benchmark of machine and deep\nlearning for EEG signal classification is proposed. For our experiments we used\nthe four most widespread models, i.e., multilayer perceptron, convolutional\nneural network, long short-term memory, and gated recurrent unit, highlighting\nwhich one can be a good starting point for developing EEG classification\nmodels.",
        "score": -2.8587582111358643
      },
      {
        "title": "Using i-vectors for subject-independent cross-session EEG transfer\n  learning,",
        "abstract": "Cognitive load classification is the task of automatically determining an\nindividual's utilization of working memory resources during performance of a\ntask based on physiologic measures such as electroencephalography (EEG). In\nthis paper, we follow a cross-disciplinary approach, where tools and\nmethodologies from speech processing are used to tackle this problem. The\ncorpus we use was released publicly in 2021 as part of the first passive\nbrain-computer interface competition on cross-session workload estimation. We\npresent our approach which used i-vector-based neural network classifiers to\naccomplish inter-subject cross-session EEG transfer learning, achieving 18%\nrelative improvement over equivalent subject-dependent models. We also report\nexperiments showing how our subject-independent models perform competitively on\nheld-out subjects and improve with additional subject data, suggesting that\nsubject-dependent training is not required for effective cognitive load\ndetermination.",
        "score": -3.4790971279144287
      },
      {
        "title": "Crafting Interpretable Embeddings by Asking LLMs Questions,",
        "abstract": "Large language models (LLMs) have rapidly improved text embeddings for a\ngrowing array of natural-language processing tasks. However, their opaqueness\nand proliferation into scientific domains such as neuroscience have created a\ngrowing need for interpretability. Here, we ask whether we can obtain\ninterpretable embeddings through LLM prompting. We introduce question-answering\nembeddings (QA-Emb), embeddings where each feature represents an answer to a\nyes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of\nunderlying questions rather than learning model weights.\n  We use QA-Emb to flexibly generate interpretable models for predicting fMRI\nvoxel responses to language stimuli. QA-Emb significantly outperforms an\nestablished interpretable baseline, and does so while requiring very few\nquestions. This paves the way towards building flexible feature spaces that can\nconcretize and evaluate our understanding of semantic brain representations. We\nadditionally find that QA-Emb can be effectively approximated with an efficient\nmodel, and we explore broader applications in simple NLP tasks.",
        "score": -3.8167026042938232
      },
      {
        "title": "Large Language Multimodal Models for 5-Year Chronic Disease Cohort\n  Prediction Using EHR Data,",
        "abstract": "Chronic diseases such as diabetes are the leading causes of morbidity and\nmortality worldwide. Numerous research studies have been attempted with various\ndeep learning models in diagnosis. However, most previous studies had certain\nlimitations, including using publicly available datasets (e.g. MIMIC), and\nimbalanced data. In this study, we collected five-year electronic health\nrecords (EHRs) from the Taiwan hospital database, including 1,420,596 clinical\nnotes, 387,392 laboratory test results, and more than 1,505 laboratory test\nitems, focusing on research pre-training large language models. We proposed a\nnovel Large Language Multimodal Models (LLMMs) framework incorporating\nmultimodal data from clinical notes and laboratory test results for the\nprediction of chronic disease risk. Our method combined a text embedding\nencoder and multi-head attention layer to learn laboratory test values,\nutilizing a deep neural network (DNN) module to merge blood features with\nchronic disease semantics into a latent space. In our experiments, we observe\nthat clinicalBERT and PubMed-BERT, when combined with attention fusion, can\nachieve an accuracy of 73% in multiclass chronic diseases and diabetes\nprediction. By transforming laboratory test values into textual descriptions\nand employing the Flan T-5 model, we achieved a 76% Area Under the ROC Curve\n(AUROC), demonstrating the effectiveness of leveraging numerical text data for\ntraining and inference in language models. This approach significantly improves\nthe accuracy of early-stage diabetes prediction.",
        "score": -3.952890157699585
      },
      {
        "title": "The Temporal Structure of Language Processing in the Human Brain\n  Corresponds to The Layered Hierarchy of Deep Language Models,",
        "abstract": "Deep Language Models (DLMs) provide a novel computational paradigm for\nunderstanding the mechanisms of natural language processing in the human brain.\nUnlike traditional psycholinguistic models, DLMs use layered sequences of\ncontinuous numerical vectors to represent words and context, allowing a\nplethora of emerging applications such as human-like text generation. In this\npaper we show evidence that the layered hierarchy of DLMs may be used to model\nthe temporal dynamics of language comprehension in the brain by demonstrating a\nstrong correlation between DLM layer depth and the time at which layers are\nmost predictive of the human brain. Our ability to temporally resolve\nindividual layers benefits from our use of electrocorticography (ECoG) data,\nwhich has a much higher temporal resolution than noninvasive methods like fMRI.\nUsing ECoG, we record neural activity from participants listening to a\n30-minute narrative while also feeding the same narrative to a high-performing\nDLM (GPT2-XL). We then extract contextual embeddings from the different layers\nof the DLM and use linear encoding models to predict neural activity. We first\nfocus on the Inferior Frontal Gyrus (IFG, or Broca's area) and then extend our\nmodel to track the increasing temporal receptive window along the linguistic\nprocessing hierarchy from auditory to syntactic and semantic areas. Our results\nreveal a connection between human language processing and DLMs, with the DLM's\nlayer-by-layer accumulation of contextual information mirroring the timing of\nneural activity in high-order language areas.",
        "score": -4.175436496734619
      }
    ]
  },
  {
    "title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive\n  Benchmark and Evaluation",
    "abstract": "  The development of Large Language Models (LLMs) has revolutionized Q&A across\nvarious industries, including the database domain. However, there is still a\nlack of a comprehensive benchmark to evaluate the capabilities of different\nLLMs and their modular components in database Q&A. To this end, we introduce\nDQA, the first comprehensive database Q&A benchmark. DQA features an innovative\nLLM-based method for automating the generation, cleaning, and rewriting of\ndatabase Q&A, resulting in over 240,000 Q&A pairs in English and Chinese. These\nQ&A pairs cover nearly all aspects of database knowledge, including database\nmanuals, database blogs, and database tools. This inclusion allows for\nadditional assessment of LLMs' Retrieval-Augmented Generation (RAG) and Tool\nInvocation Generation (TIG) capabilities in the database Q&A task. Furthermore,\nwe propose a comprehensive LLM-based database Q&A testbed on DQA. This testbed\nis highly modular and scalable, with both basic and advanced components like\nQuestion Classification Routing (QCR), RAG, TIG, and Prompt Template\nEngineering (PTE). Besides, DQA provides a complete evaluation pipeline,\nfeaturing diverse metrics and a standardized evaluation process to ensure\ncomprehensiveness, accuracy, and fairness. We use DQA to evaluate the database\nQ&A capabilities under the proposed testbed comprehensively. The evaluation\nreveals findings like (i) the strengths and limitations of nine different\nLLM-based Q&A bots and (ii) the performance impact and potential improvements\nof various service components (e.g., QCR, RAG, TIG). We hope our benchmark and\nfindings will better guide the future development of LLM-based database Q&A\nresearch.\n",
    "related_paper_titles": [
      "LLM As DBA",
      "Retrieval-Augmented Generation for Large Language Models: A Survey",
      "DB-GPT: Empowering Database Interactions with Private Large Language\n  Models"
    ],
    "related_paper_abstract": [
      "  Database administrators (DBAs) play a crucial role in managing, maintaining\nand optimizing a database system to ensure data availability, performance, and\nreliability. However, it is hard and tedious for DBAs to manage a large number\nof database instances (e.g., millions of instances on the cloud databases).\nRecently large language models (LLMs) have shown great potential to understand\nvaluable documents and accordingly generate reasonable answers. Thus, we\npropose D-Bot, a LLM-based database administrator that can continuously acquire\ndatabase maintenance experience from textual sources, and provide reasonable,\nwell-founded, in-time diagnosis and optimization advice for target databases.\nThis paper presents a revolutionary LLM-centric framework for database\nmaintenance, including (i) database maintenance knowledge detection from\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\nexperimental results that D-Bot can efficiently and effectively diagnose the\nroot causes and our code is available at\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.\n",
      "  Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.\n",
      "  The recent breakthroughs in large language models (LLMs) are positioned to\ntransition many areas of software. Database technologies particularly have an\nimportant entanglement with LLMs as efficient and intuitive database\ninteractions are paramount. In this paper, we present DB-GPT, a revolutionary\nand production-ready project that integrates LLMs with traditional database\nsystems to enhance user experience and accessibility. DB-GPT is designed to\nunderstand natural language queries, provide context-aware responses, and\ngenerate complex SQL queries with high accuracy, making it an indispensable\ntool for users ranging from novice to expert. The core innovation in DB-GPT\nlies in its private LLM technology, which is fine-tuned on domain-specific\ncorpora to maintain user privacy and ensure data security while offering the\nbenefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which\nincludes a novel retrieval augmented generation (RAG) knowledge system, an\nadaptive learning mechanism to continuously improve performance based on user\nfeedback and a service-oriented multi-model framework (SMMF) with powerful\ndata-driven agents. Our extensive experiments and user studies confirm that\nDB-GPT represents a paradigm shift in database interactions, offering a more\nnatural, efficient, and secure way to engage with data repositories. The paper\nconcludes with a discussion of the implications of DB-GPT framework on the\nfuture of human-database interaction and outlines potential avenues for further\nenhancements and applications in the field. The project code is available at\nhttps://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself by\ninstalling it with the instructions\nhttps://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minute\nvideo at https://www.youtube.com/watch?v=KYs4nTDzEhk.\n"
    ],
    "entities": [
      "LiDAR",
      "MATH",
      "ESG",
      "Global",
      "Research",
      "KV",
      "IFT",
      "PTQ",
      "NTK",
      "Thought",
      "SSMs",
      "Domain Adaptation",
      "EHRs",
      "ToM",
      "Monte Carlo Tree Search",
      "GPU",
      "Retrieval",
      "PDDL",
      "VAE",
      "Language Model",
      "Online",
      "Persian",
      "TSF",
      "MDP",
      "GAI",
      "Mind",
      "Retrieval Augmented",
      "ASP",
      "GPUs",
      "Random Forest"
    ],
    "retrieved_papers": [
      {
        "title": "QUADRo: Dataset and Models for QUestion-Answer Database Retrieval,",
        "abstract": "An effective paradigm for building Automated Question Answering systems is\nthe re-use of previously answered questions, e.g., for FAQs or forum\napplications. Given a database (DB) of question/answer (q/a) pairs, it is\npossible to answer a target question by scanning the DB for similar questions.\nIn this paper, we scale this approach to open domain, making it competitive\nwith other standard methods, e.g., unstructured document or graph based. For\nthis purpose, we (i) build a large scale DB of 6.3M q/a pairs, using public\nquestions, (ii) design a new system based on neural IR and a q/a pair reranker,\nand (iii) construct training and test data to perform comparative experiments\nwith our models. We demonstrate that Transformer-based models using (q,a) pairs\noutperform models only based on question representation, for both neural search\nand reranking. Additionally, we show that our DB-based approach is competitive\nwith Web-based methods, i.e., a QA system built on top the BING search engine,\ndemonstrating the challenge of finding relevant information. Finally, we make\nour data and models available for future research.",
        "score": -2.153012275695801
      },
      {
        "title": "KU-DMIS at EHRSQL 2024:Generating SQL query via question templatization\n  in EHR,",
        "abstract": "Transforming natural language questions into SQL queries is crucial for\nprecise data retrieval from electronic health record (EHR) databases. A\nsignificant challenge in this process is detecting and rejecting unanswerable\nquestions that request information beyond the database's scope or exceed the\nsystem's capabilities. In this paper, we introduce a novel text-to-SQL\nframework that robustly handles out-of-domain questions and verifies the\ngenerated queries with query execution.Our framework begins by standardizing\nthe structure of questions into a templated format. We use a powerful large\nlanguage model (LLM), fine-tuned GPT-3.5 with detailed prompts involving the\ntable schemas of the EHR database system. Our experimental results demonstrate\nthe effectiveness of our framework on the EHRSQL-2024 benchmark benchmark, a\nshared task in the ClinicalNLP workshop. Although a straightforward fine-tuning\nof GPT shows promising results on the development set, it struggled with the\nout-of-domain questions in the test set. With our framework, we improve our\nsystem's adaptability and achieve competitive performances in the official\nleaderboard of the EHRSQL-2024 challenge.",
        "score": -3.3570921421051025
      },
      {
        "title": "MultiTabQA: Generating Tabular Answers for Multi-Table Question\n  Answering,",
        "abstract": "Recent advances in tabular question answering (QA) with large language models\nare constrained in their coverage and only answer questions over a single\ntable. However, real-world queries are complex in nature, often over multiple\ntables in a relational database or web page. Single table questions do not\ninvolve common table operations such as set operations, Cartesian products\n(joins), or nested queries. Furthermore, multi-table operations often result in\na tabular output, which necessitates table generation capabilities of tabular\nQA models. To fill this gap, we propose a new task of answering questions over\nmultiple tables. Our model, MultiTabQA, not only answers questions over\nmultiple tables, but also generalizes to generate tabular answers. To enable\neffective training, we build a pre-training dataset comprising of 132,645 SQL\nqueries and tabular answers. Further, we evaluate the generated tables by\nintroducing table-specific metrics of varying strictness assessing various\nlevels of granularity of the table structure. MultiTabQA outperforms\nstate-of-the-art single table QA models adapted to a multi-table QA setting by\nfinetuning on three datasets: Spider, Atis and GeoQuery.",
        "score": -3.485550880432129
      },
      {
        "title": "OmniTab: Pretraining with Natural and Synthetic Data for Few-shot\n  Table-based Question Answering,",
        "abstract": "The information in tables can be an important complement to text, making\ntable-based question answering (QA) systems of great value. The intrinsic\ncomplexity of handling tables often adds an extra burden to both model design\nand data annotation. In this paper, we aim to develop a simple table-based QA\nmodel with minimal annotation effort. Motivated by the fact that table-based QA\nrequires both alignment between questions and tables and the ability to perform\ncomplicated reasoning over multiple table elements, we propose an omnivorous\npretraining approach that consumes both natural and synthetic data to endow\nmodels with these respective abilities. Specifically, given freely available\ntables, we leverage retrieval to pair them with relevant natural sentences for\nmask-based pretraining, and synthesize NL questions by converting SQL sampled\nfrom tables for pretraining with a QA loss. We perform extensive experiments in\nboth few-shot and full settings, and the results clearly demonstrate the\nsuperiority of our model OmniTab, with the best multitasking approach achieving\nan absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively,\nalso establishing a new state-of-the-art on WikiTableQuestions. Detailed\nablations and analyses reveal different characteristics of natural and\nsynthetic data, shedding light on future directions in omnivorous pretraining.\nCode, pretraining data, and pretrained models are available at\nhttps://github.com/jzbjyb/OmniTab.",
        "score": -5.196831226348877
      },
      {
        "title": "LLM As DBA,",
        "abstract": "Database administrators (DBAs) play a crucial role in managing, maintaining\nand optimizing a database system to ensure data availability, performance, and\nreliability. However, it is hard and tedious for DBAs to manage a large number\nof database instances (e.g., millions of instances on the cloud databases).\nRecently large language models (LLMs) have shown great potential to understand\nvaluable documents and accordingly generate reasonable answers. Thus, we\npropose D-Bot, a LLM-based database administrator that can continuously acquire\ndatabase maintenance experience from textual sources, and provide reasonable,\nwell-founded, in-time diagnosis and optimization advice for target databases.\nThis paper presents a revolutionary LLM-centric framework for database\nmaintenance, including (i) database maintenance knowledge detection from\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\nexperimental results that D-Bot can efficiently and effectively diagnose the\nroot causes and our code is available at\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.",
        "score": -5.520346641540527
      },
      {
        "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for\n  Large-Scale Database Grounded Text-to-SQLs,",
        "abstract": "Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.",
        "score": -5.915131092071533
      },
      {
        "title": "Plug-and-Play Adaptation for Continuously-updated QA,",
        "abstract": "Language models (LMs) have shown great potential as implicit knowledge bases\n(KBs). And for their practical use, knowledge in LMs need to be updated\nperiodically. However, existing tasks to assess LMs' efficacy as KBs do not\nadequately consider multiple large-scale updates. To this end, we first propose\na novel task--Continuously-updated QA (CuQA)--in which multiple large-scale\nupdates are made to LMs, and the performance is measured with respect to the\nsuccess in adding and updating knowledge while retaining existing knowledge. We\nthen present LMs with plug-in modules that effectively handle the updates.\nExperiments conducted on zsRE QA and NQ datasets show that our method\noutperforms existing approaches. We find that our method is 4x more effective\nin terms of updates/forgets ratio, compared to a fine-tuning baseline.",
        "score": -5.936421871185303
      },
      {
        "title": "NL2KQL: From Natural Language to Kusto Query,",
        "abstract": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.",
        "score": -6.110585689544678
      },
      {
        "title": "Large Language Model for Table Processing: A Survey,",
        "abstract": "Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\ncalculations, and generating reports from web tables. Automating these\ntable-centric tasks with Large Language Models (LLMs) offers significant public\nbenefits, garnering interest from academia and industry. This survey provides\nan extensive overview of table tasks, encompassing not only the traditional\nareas like table question answering (Table QA) and fact verification, but also\nnewly emphasized aspects such as table manipulation and advanced table data\nanalysis. Additionally, it goes beyond the early strategies of pre-training and\nfine-tuning small language models, to include recent paradigms in LLM usage.\nThe focus here is particularly on instruction-tuning, prompting, and\nagent-based approaches within the realm of LLMs. Finally, we highlight several\nchallenges, ranging from private deployment and efficient inference to the\ndevelopment of extensive benchmarks for table manipulation and advanced data\nanalysis.",
        "score": -6.177538871765137
      },
      {
        "title": "Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM,",
        "abstract": "Generating accurate SQL queries for user questions (text-to-SQL) has been a\nlong-standing challenge since it requires a deep understanding of both the\nuser's question and the corresponding database schema in order to retrieve the\ndesired content accurately. Existing methods rely on the comprehensive\ncapability of large language models (LLMs) to generate the SQL. However, some\nnecessary knowledge is not explicitly included in the database schema and user\nquestion or has been learned by LLMs. Thus, the generated SQL of the\nknowledge-insufficient questions may be inaccurate, negatively influencing the\ntext-to-SQL models' performance and robustness. To address this challenge, we\npropose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM\n(DELLM) to provide helpful knowledge for all text-to-SQL models. Specifically,\nwe introduce the detailed implementation of DELLM regarding table reading and\nthe basic fine-tuning process. We further propose a Preference Learning via\nDatabase Feedback (PLDBF) strategy, refining the DELLM to generate more helpful\nknowledge for LLMs. Extensive experiments verify that DELLM can enhance the\nstate-of-the-art approaches for text-to-SQL tasks. The corresponding code of\nDELLM is released for further research.",
        "score": -6.311635971069336
      },
      {
        "title": "DBCopilot: Scaling Natural Language Querying to Massive Databases,",
        "abstract": "Text-to-SQL simplifies database interactions by enabling non-experts to\nconvert their natural language (NL) questions into Structured Query Language\n(SQL) queries. While recent advances in large language models (LLMs) have\nimproved the zero-shot text-to-SQL paradigm, existing methods face scalability\nchallenges when dealing with massive, dynamically changing databases. This\npaper introduces DBCopilot, a framework that addresses these challenges by\nemploying a compact and flexible copilot model for routing across massive\ndatabases. Specifically, DBCopilot decouples the text-to-SQL process into\nschema routing and SQL generation, leveraging a lightweight\nsequence-to-sequence neural network-based router to formulate database\nconnections and navigate natural language questions through databases and\ntables. The routed schemas and questions are then fed into LLMs for efficient\nSQL generation. Furthermore, DBCopilot also introduced a reverse\nschema-to-question generation paradigm, which can learn and adapt the router\nover massive databases automatically without requiring manual intervention.\nExperimental results demonstrate that DBCopilot is a scalable and effective\nsolution for real-world text-to-SQL tasks, providing a significant advancement\nin handling large-scale schemas.",
        "score": -6.328514575958252
      },
      {
        "title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries,",
        "abstract": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.",
        "score": -6.959317207336426
      },
      {
        "title": "Structure Guided Large Language Model for SQL Generation,",
        "abstract": "Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.",
        "score": -7.211348056793213
      },
      {
        "title": "UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training,",
        "abstract": "We present UnifiedQA-v2, a QA model built with the same process as UnifiedQA,\nexcept that it utilizes more supervision -- roughly 3x the number of datasets\nused for UnifiedQA. This generally leads to better in-domain and cross-domain\nresults.",
        "score": -7.477830410003662
      },
      {
        "title": "Interactive Text-to-SQL Generation via Editable Step-by-Step\n  Explanations,",
        "abstract": "Relational databases play an important role in business, science, and more.\nHowever, many users cannot fully unleash the analytical power of relational\ndatabases, because they are not familiar with database languages such as SQL.\nMany techniques have been proposed to automatically generate SQL from natural\nlanguage, but they suffer from two issues: (1) they still make many mistakes,\nparticularly for complex queries, and (2) they do not provide a flexible way\nfor non-expert users to validate and refine incorrect queries. To address these\nissues, we introduce a new interaction mechanism that allows users to directly\nedit a step-by-step explanation of a query to fix errors. Our experiments on\nmultiple datasets, as well as a user study with 24 participants, demonstrate\nthat our approach can achieve better performance than multiple SOTA approaches.\nOur code and datasets are available at https://github.com/magic-YuanTian/STEPS.",
        "score": -7.683428764343262
      },
      {
        "title": "Do I have the Knowledge to Answer? Investigating Answerability of\n  Knowledge Base Questions,",
        "abstract": "When answering natural language questions over knowledge bases, missing\nfacts, incomplete schema and limited scope naturally lead to many questions\nbeing unanswerable. While answerability has been explored in other QA settings,\nit has not been studied for QA over knowledge bases (KBQA). We create\nGrailQAbility, a new benchmark KBQA dataset with unanswerability, by first\nidentifying various forms of KB incompleteness that make questions\nunanswerable, and then systematically adapting GrailQA (a popular KBQA dataset\nwith only answerable questions). Experimenting with three state-of-the-art KBQA\nmodels, we find that all three models suffer a drop in performance even after\nsuitable adaptation for unanswerable questions. In addition, these often detect\nunanswerability for wrong reasons and find specific forms of unanswerability\nparticularly difficult to handle. This underscores the need for further\nresearch in making KBQA systems robust to unanswerability",
        "score": -8.197501182556152
      },
      {
        "title": "Towards Understanding the Generalization of Medical Text-to-SQL Models\n  and Datasets,",
        "abstract": "Electronic medical records (EMRs) are stored in relational databases. It can\nbe challenging to access the required information if the user is unfamiliar\nwith the database schema or general database fundamentals. Hence, researchers\nhave explored text-to-SQL generation methods that provide healthcare\nprofessionals direct access to EMR data without needing a database expert.\nHowever, currently available datasets have been essentially \"solved\" with\nstate-of-the-art models achieving accuracy greater than or near 90%. In this\npaper, we show that there is still a long way to go before solving text-to-SQL\ngeneration in the medical domain. To show this, we create new splits of the\nexisting medical text-to-SQL dataset MIMICSQL that better measure the\ngeneralizability of the resulting models. We evaluate state-of-the-art language\nmodels on our new split showing substantial drops in performance with accuracy\ndropping from up to 92% to 28%, thus showing substantial room for improvement.\nMoreover, we introduce a novel data augmentation approach to improve the\ngeneralizability of the language models. Overall, this paper is the first step\ntowards developing more robust text-to-SQL models in the medical\ndomain.\\footnote{The dataset and code will be released upon acceptance.",
        "score": -8.371332168579102
      },
      {
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL,",
        "abstract": "Existing Text-to-SQL generators require the entire schema to be encoded with\nthe user text. This is expensive or impractical for large databases with tens\nof thousands of columns. Standard dense retrieval techniques are inadequate for\nschema subsetting of a large structured database, where the correct semantics\nof retrieval demands that we rank sets of schema elements rather than\nindividual elements. In response, we propose a two-stage process for effective\ncoverage during retrieval. First, we instruct an LLM to hallucinate a minimal\nDB schema deemed adequate to answer the query. We use the hallucinated schema\nto retrieve a subset of the actual schema, by composing the results from\nmultiple dense retrievals. Remarkably, hallucination $\\unicode{x2013}$\ngenerally considered a nuisance $\\unicode{x2013}$ turns out to be actually\nuseful as a bridging mechanism. Since no existing benchmarks exist for schema\nsubsetting on large databases, we introduce three benchmarks. Two\nsemi-synthetic datasets are derived from the union of schemas in two well-known\ndatasets, SPIDER and BIRD, resulting in 4502 and 798 schema elements\nrespectively. A real-life benchmark called SocialDB is sourced from an actual\nlarge data warehouse comprising 17844 schema elements. We show that our method1\nleads to significantly higher recall than SOTA retrieval-based augmentation\nmethods.",
        "score": -8.37391185760498
      },
      {
        "title": "SPINACH: SPARQL-Based Information Navigation for Challenging Real-World\n  Questions,",
        "abstract": "Recent work integrating Large Language Models (LLMs) has led to significant\nimprovements in the Knowledge Base Question Answering (KBQA) task. However, we\nposit that existing KBQA datasets that either have simple questions, use\nsynthetically generated logical forms, or are based on small knowledge base\n(KB) schemas, do not capture the true complexity of KBQA tasks.\n  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA\ndataset collected from forum discussions on Wikidata's \"Request a Query\" forum\nwith 320 decontextualized question-SPARQL pairs. Much more complex than\nexisting datasets, SPINACH calls for strong KBQA systems that do not rely on\ntraining data to learn the KB schema, but can dynamically explore large and\noften incomplete schemas and reason about them.\n  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach\nthat mimics how a human expert would write SPARQLs for such challenging\nquestions. Experiments on existing datasets show SPINACH's capability in KBQA,\nachieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10\ndatasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%\nof the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH\ndataset, SPINACH agent outperforms all baselines, including the best\nGPT-4-based KBQA agent, by 38.1% in F1.",
        "score": -8.630390167236328
      },
      {
        "title": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables,",
        "abstract": "In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class\nof database systems that can seamlessly query text and tables using SQL. To\nenable seamless querying of textual data using SQL in an MMDB, we propose to\nextend relational databases with so-called multi-modal operators (MMOps) which\nare based on the advances of recent large language models such as GPT-3. The\nmain idea of MMOps is that they allow text collections to be treated as tables\nwithout the need to manually transform the data. As we show in our evaluation,\nour MMDB prototype can not only outperform state-of-the-art approaches such as\ntext-to-table in terms of accuracy and performance but it also requires\nsignificantly less training data to fine-tune the model for an unseen text\ncollection.",
        "score": -8.681234359741211
      },
      {
        "title": "A Declarative System for Optimizing AI Workloads,",
        "abstract": "A long-standing goal of data management systems has been to build systems\nwhich can compute quantitative insights over large corpora of unstructured data\nin a cost-effective manner. Until recently, it was difficult and expensive to\nextract facts from company documents, data from scientific papers, or metrics\nfrom image and video corpora. Today's models can accomplish these tasks with\nhigh accuracy. However, a programmer who wants to answer a substantive\nAI-powered query must orchestrate large numbers of models, prompts, and data\noperations. For even a single query, the programmer has to make a vast number\nof decisions such as the choice of model, the right inference method, the most\ncost-effective inference hardware, the ideal prompt design, and so on. The\noptimal set of decisions can change as the query changes and as the\nrapidly-evolving technical landscape shifts. In this paper we present\nPalimpzest, a system that enables anyone to process AI-powered analytical\nqueries simply by defining them in a declarative language. The system uses its\ncost optimization framework to implement the query plan with the best\ntrade-offs between runtime, financial cost, and output data quality. We\ndescribe the workload of AI-powered analytics tasks, the optimization methods\nthat Palimpzest uses, and the prototype system itself. We evaluate Palimpzest\non tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching.\nWe show that even our simple prototype offers a range of appealing plans,\nincluding one that is 3.3x faster and 2.9x cheaper than the baseline method,\nwhile also offering better data quality. With parallelism enabled, Palimpzest\ncan produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a\nsingle-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the\nbaseline. These require no additional work by the user.",
        "score": -8.753677368164062
      },
      {
        "title": "Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for\n  Text-to-SQL,",
        "abstract": "The current state-of-the-art (SOTA) for automated text-to-SQL still falls\nwell short of expert human performance as measured by execution accuracy (EX)\non the BIRD-SQL benchmark. The most accurate methods are also slow and\nexpensive. To advance the SOTA for text-to-SQL while reducing cost and\nimproving speed, we explore the combination of low-cost fine tuning, novel\nmethods for diverse retrieval-augmented generation (RAG) and new input and\noutput formats that help large language models (LLMs) achieve higher EX. We\nintroduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record\nfor EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher\nperformance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI,\nbut uses the low-cost GPT-3.5 Turbo while exceeding the performance of the\nnext-best model using OpenAI, which instead uses the more expensive GPT-4.\nDubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by\nover 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push\nEX higher.",
        "score": -9.038333892822266
      },
      {
        "title": "RH-SQL: Refined Schema and Hardness Prompt for Text-to-SQL,",
        "abstract": "Text-to-SQL is a technology that converts natural language queries into the\nstructured query language SQL. A novel research approach that has recently\ngained attention focuses on methods based on the complexity of SQL queries,\nachieving notable performance improvements. However, existing methods entail\nsignificant storage and training costs, which hampers their practical\napplication. To address this issue, this paper introduces a method for\nText-to-SQL based on Refined Schema and Hardness Prompt. By filtering out\nlow-relevance schema information with a refined schema and identifying query\nhardness through a Language Model (LM) to form prompts, this method reduces\nstorage and training costs while maintaining performance. It's worth mentioning\nthat this method is applicable to any sequence-to-sequence (seq2seq) LM. Our\nexperiments on the Spider dataset, specifically with large-scale LMs, achieved\nan exceptional Execution accuracy (EX) of 82.6%, demonstrating the\neffectiveness and greater suitability of our method for real-world\napplications.",
        "score": -9.043423652648926
      },
      {
        "title": "Blar-SQL: Faster, Stronger, Smaller NL2SQL,",
        "abstract": "Large Language Models (LLMs) have gained considerable notoriety in the field\nof natural language to SQL tasks (NL2SQL). In this study, we show how task\ndecomposition can greatly benefit LLMs in database understanding and query\ngeneration in order to answer human questions with an SQL query.\n  We fined-tuned open source models, specifically Llama-2 and Code Llama, by\ncombining 2 different models each designated to focus on one of two tasks in\norder to leverage each model's core competency to further increase the accuracy\nof the final SQL query.\n  We propose a new framework to divide the schema into chunks in order to fit\nmore information into a limited context. Our results are comparable with those\nobtained by GPT-4 at the same time being 135 times smaller, 90 times faster and\nmore than 100 times cheaper than GPT-4.",
        "score": -9.065886497497559
      },
      {
        "title": "Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs\n  versus Traditional Relational Databases,",
        "abstract": "Large Language Models (LLMs) can automate or substitute different types of\ntasks in the software engineering process. This study evaluates the resource\nutilization and accuracy of LLM in interpreting and executing natural language\nqueries against traditional SQL within relational database management systems.\nWe empirically examine the resource utilization and accuracy of nine LLMs\nvarying from 7 to 34 Billion parameters, including Llama2 7B, Llama2 13B,\nMistral, Mixtral, Optimus-7B, SUS-chat-34B, platypus-yi-34b,\nNeuralHermes-2.5-Mistral-7B and Starling-LM-7B-alpha, using a small transaction\ndataset. Our findings indicate that using LLMs for database queries incurs\nsignificant energy overhead (even small and quantized models), making it an\nenvironmentally unfriendly approach. Therefore, we advise against replacing\nrelational databases with LLMs due to their substantial resource utilization.",
        "score": -9.356084823608398
      },
      {
        "title": "Translating between SQL Dialects for Cloud Migration,",
        "abstract": "Migrations of systems from on-site premises to the cloud has been a\nfundamental endeavor by many industrial institutions. A crucial component of\nsuch cloud migrations is the transition of databases to be hosted online. In\nthis work, we consider the difficulties of this migration for SQL databases.\nWhile SQL is one of the prominent methods for storing database procedures,\nthere are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.)\nwhich can complicate migrations when the on-premise SQL dialect differs to the\ndialect hosted on the cloud. Tools exist by common cloud provides such as AWS\nand Azure to aid in translating between dialects in order to mitigate the\nmajority of the difficulties. However, these tools do not successfully\ntranslate $100\\%$ of the code. Consequently, software engineers must manually\nconvert the remainder of the untranslated database. For large organizations,\nthis task quickly becomes intractable and so more innovative solutions are\nrequired. We consider this challenge a novel yet vital industrial research\nproblem for any large corporation that is considering cloud migrations.\nFurthermore, we introduce potential avenues of research to tackle this\nchallenge that have yielded promising preliminary results.",
        "score": -9.516653060913086
      },
      {
        "title": "Domain Adaptation of a State of the Art Text-to-SQL Model: Lessons\n  Learned and Challenges Found,",
        "abstract": "There are many recent advanced developments for the Text-to-SQL task, where\nthe Picard model is one of the the top performing models as measured by the\nSpider dataset competition. However, bringing Text-to-SQL systems to realistic\nuse-cases through domain adaptation remains a tough challenge. We analyze how\nwell the base T5 Language Model and Picard perform on query structures\ndifferent from the Spider dataset, we fine-tuned the base model on the Spider\ndata and on independent databases (DB). To avoid accessing the DB content\nonline during inference, we also present an alternative way to disambiguate the\nvalues in an input question using a rule-based approach that relies on an\nintermediate representation of the semantic concepts of an input question. In\nour results we show in what cases T5 and Picard can deliver good performance,\nwe share the lessons learned, and discuss current domain adaptation challenges.",
        "score": -9.899486541748047
      },
      {
        "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas,",
        "abstract": "We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.",
        "score": -9.901570320129395
      },
      {
        "title": "MarkQA: A large scale KBQA dataset with numerical reasoning,",
        "abstract": "While question answering over knowledge bases (KBQA) has shown progress in\naddressing factoid questions, KBQA with numerical reasoning remains relatively\nunexplored. In this paper, we focus on the complex numerical reasoning in KBQA\nand propose a new task, NR-KBQA, which necessitates the ability to perform both\nmulti-hop reasoning and numerical reasoning. We design a logic form in Python\nformat called PyQL to represent the reasoning process of numerical reasoning\nquestions. To facilitate the development of NR-KBQA, we present a large dataset\ncalled MarkQA, which is automatically constructed from a small set of seeds.\nEach question in MarkQA is equipped with its corresponding SPARQL query,\nalongside the step-by-step reasoning process in the QDMR format and PyQL\nprogram. Experimental results of some state-of-the-art QA methods on the MarkQA\nshow that complex numerical reasoning in KBQA faces great challenges.",
        "score": -9.966489791870117
      },
      {
        "title": "BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in\n  Relational Algebra,",
        "abstract": "Many existing end-to-end systems for hybrid question answering tasks can\noften be boiled down to a \"prompt-and-pray\" paradigm, where the user has\nlimited control and insight into the intermediate reasoning steps used to\nachieve the final result. Additionally, due to the context size limitation of\nmany transformer-based LLMs, it is often not reasonable to expect that the full\nstructured and unstructured context will fit into a given prompt in a zero-shot\nsetting, let alone a few-shot setting. We introduce BlendSQL, a superset of\nSQLite to act as a unified dialect for orchestrating reasoning across both\nunstructured and structured data. For hybrid question answering tasks involving\nmulti-hop reasoning, we encode the full decomposed reasoning roadmap into a\nsingle interpretable BlendSQL query. Notably, we show that BlendSQL can scale\nto massive datasets and improve the performance of end-to-end systems while\nusing 35% fewer tokens. Our code is available and installable as a package at\nhttps://github.com/parkervg/blendsql.",
        "score": -10.34261703491211
      }
    ]
  },
  {
    "title": "Dual-Level Cross-Modal Contrastive Clustering",
    "abstract": "  Image clustering, which involves grouping images into different clusters\nwithout labels, is a key task in unsupervised learning. Although previous deep\nclustering methods have achieved remarkable results, they only explore the\nintrinsic information of the image itself but overlook external supervision\nknowledge to improve the semantic understanding of images. Recently,\nvisual-language pre-trained model on large-scale datasets have been used in\nvarious downstream tasks and have achieved great results. However, there is a\ngap between visual representation learning and textual semantic learning, and\nhow to properly utilize the representation of two different modalities for\nclustering is still a big challenge. To tackle the challenges, we propose a\nnovel image clustering framwork, named Dual-level Cross-Modal Contrastive\nClustering (DXMC). Firstly, external textual information is introduced for\nconstructing a semantic space which is adopted to generate image-text pairs.\nSecondly, the image-text pairs are respectively sent to pre-trained image and\ntext encoder to obtain image and text embeddings which subsquently are fed into\nfour well-designed networks. Thirdly, dual-level cross-modal contrastive\nlearning is conducted between discriminative representations of different\nmodalities and distinct level. Extensive experimental results on five benchmark\ndatasets demonstrate the superiority of our proposed method.\n",
    "related_paper_titles": [
      "Image Clustering with External Guidance",
      "Image Clustering via the Principle of Rate Reduction in the Age of\n  Pretrained Models",
      "Clustering-friendly Representation Learning via Instance Discrimination\n  and Feature Decorrelation"
    ],
    "related_paper_abstract": [
      "  The core of clustering is incorporating prior knowledge to construct\nsupervision signals. From classic k-means based on data compactness to recent\ncontrastive clustering guided by self-supervision, the evolution of clustering\nmethods intrinsically corresponds to the progression of supervision signals. At\npresent, substantial efforts have been devoted to mining internal supervision\nsignals from data. Nevertheless, the abundant external knowledge such as\nsemantic descriptions, which naturally conduces to clustering, is regrettably\noverlooked. In this work, we propose leveraging external knowledge as a new\nsupervision signal to guide clustering, even though it seems irrelevant to the\ngiven data. To implement and validate our idea, we design an externally guided\nclustering method (Text-Aided Clustering, TAC), which leverages the textual\nsemantics of WordNet to facilitate image clustering. Specifically, TAC first\nselects and retrieves WordNet nouns that best distinguish images to enhance the\nfeature discriminability. Then, to improve image clustering performance, TAC\ncollaborates text and image modalities by mutually distilling cross-modal\nneighborhood information. Experiments demonstrate that TAC achieves\nstate-of-the-art performance on five widely used and three more challenging\nimage clustering benchmarks, including the full ImageNet-1K dataset.\n",
      "  The advent of large pre-trained models has brought about a paradigm shift in\nboth visual representation learning and natural language processing. However,\nclustering unlabeled images, as a fundamental and classic machine learning\nproblem, still lacks an effective solution, particularly for large-scale\ndatasets. In this paper, we propose a novel image clustering pipeline that\nleverages the powerful feature representation of large pre-trained models such\nas CLIP and cluster images effectively and efficiently at scale. We first\ndeveloped a novel algorithm to estimate the number of clusters in a given\ndataset. We then show that the pre-trained features are significantly more\nstructured by further optimizing the rate reduction objective. The resulting\nfeatures may significantly improve the clustering accuracy, e.g., from 57\\% to\n66\\% on ImageNet-1k. Furthermore, by leveraging CLIP's multimodality bridge\nbetween image and text, we develop a simple yet effective self-labeling\nalgorithm that produces meaningful captions for the clusters. Through extensive\nexperiments, we show that our pipeline works well on standard datasets such as\nCIFAR-10, CIFAR-100, and ImageNet-1k. It also extends to datasets that are not\ncurated for clustering, such as LAION-Aesthetics and WikiArts. We released the\ncode in https://github.com/LeslieTrue/CPP.\n",
      "  Clustering is one of the most fundamental tasks in machine learning.\nRecently, deep clustering has become a major trend in clustering techniques.\nRepresentation learning often plays an important role in the effectiveness of\ndeep clustering, and thus can be a principal cause of performance degradation.\nIn this paper, we propose a clustering-friendly representation learning method\nusing instance discrimination and feature decorrelation. Our\ndeep-learning-based representation learning method is motivated by the\nproperties of classical spectral clustering. Instance discrimination learns\nsimilarities among data and feature decorrelation removes redundant correlation\namong features. We utilize an instance discrimination method in which learning\nindividual instance classes leads to learning similarity among instances.\nThrough detailed experiments and examination, we show that the approach can be\nadapted to learning a latent space for clustering. We design novel\nsoftmax-formulated decorrelation constraints for learning. In evaluations of\nimage clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy\nof 81.5% and 95.4%, respectively. We also show that the softmax-formulated\nconstraints are compatible with various neural networks.\n"
    ],
    "entities": [],
    "retrieved_papers": [
      {
        "title": "Deep Image Clustering with Contrastive Learning and Multi-scale Graph\n  Convolutional Networks,",
        "abstract": "Deep clustering has shown its promising capability in joint representation\nlearning and clustering via deep neural networks. Despite the significant\nprogress, the existing deep clustering works mostly utilize some\ndistribution-based clustering loss, lacking the ability to unify representation\nlearning and multi-scale structure learning. To address this, this paper\npresents a new deep clustering approach termed image clustering with\ncontrastive learning and multi-scale graph convolutional networks (IcicleGCN),\nwhich bridges the gap between convolutional neural network (CNN) and graph\nconvolutional network (GCN) as well as the gap between contrastive learning and\nmulti-scale structure learning for the deep clustering task. Our framework\nconsists of four main modules, namely, the CNN-based backbone, the Instance\nSimilarity Module (ISM), the Joint Cluster Structure Learning and Instance\nreconstruction Module (JC-SLIM), and the Multi-scale GCN module (M-GCN).\nSpecifically, the backbone network with two weight-sharing views is utilized to\nlearn the representations for the two augmented samples (from each image). The\nlearned representations are then fed to ISM and JC-SLIM for joint\ninstance-level and cluster-level contrastive learning, respectively, during\nwhich an auto-encoder in JC-SLIM is also pretrained to serve as a bridge to the\nM-GCN module. Further, to enforce multi-scale neighborhood structure learning,\ntwo streams of GCNs and the auto-encoder are simultaneously trained via (i) the\nlayer-wise interaction with representation fusion and (ii) the joint\nself-adaptive learning. Experiments on multiple image datasets demonstrate the\nsuperior clustering performance of IcicleGCN over the state-of-the-art. The\ncode is available at https://github.com/xuyuankun631/IcicleGCN.",
        "score": 2.4712772369384766
      },
      {
        "title": "Multi-level Cross-modal Alignment for Image Clustering,",
        "abstract": "Recently, the cross-modal pretraining model has been employed to produce\nmeaningful pseudo-labels to supervise the training of an image clustering\nmodel. However, numerous erroneous alignments in a cross-modal pre-training\nmodel could produce poor-quality pseudo-labels and degrade clustering\nperformance. To solve the aforementioned issue, we propose a novel\n\\textbf{Multi-level Cross-modal Alignment} method to improve the alignments in\na cross-modal pretraining model for downstream tasks, by building a smaller but\nbetter semantic space and aligning the images and texts in three levels, i.e.,\ninstance-level, prototype-level, and semantic-level. Theoretical results show\nthat our proposed method converges, and suggests effective means to reduce the\nexpected clustering risk of our method. Experimental results on five benchmark\ndatasets clearly show the superiority of our new method.",
        "score": 2.466658115386963
      },
      {
        "title": "Semantic-Enhanced Image Clustering,",
        "abstract": "Image clustering is an important and open-challenging task in computer\nvision. Although many methods have been proposed to solve the image clustering\ntask, they only explore images and uncover clusters according to the image\nfeatures, thus being unable to distinguish visually similar but semantically\ndifferent images. In this paper, we propose to investigate the task of image\nclustering with the help of a visual-language pre-training model. Different\nfrom the zero-shot setting, in which the class names are known, we only know\nthe number of clusters in this setting. Therefore, how to map images to a\nproper semantic space and how to cluster images from both image and semantic\nspaces are two key problems. To solve the above problems, we propose a novel\nimage clustering method guided by the visual-language pre-training model CLIP,\nnamed \\textbf{Semantic-Enhanced Image Clustering (SIC)}. In this new method, we\npropose a method to map the given images to a proper semantic space first and\nefficient methods to generate pseudo-labels according to the relationships\nbetween images and semantics. Finally, we propose performing clustering with\nconsistency learning in both image space and semantic space, in a\nself-supervised learning fashion. The theoretical result of convergence\nanalysis shows that our proposed method can converge at a sublinear speed.\nTheoretical analysis of expectation risk also shows that we can reduce the\nexpected risk by improving neighborhood consistency, increasing prediction\nconfidence, or reducing neighborhood imbalance. Experimental results on five\nbenchmark datasets clearly show the superiority of our new method.",
        "score": 2.305598735809326
      },
      {
        "title": "Multi-label Cluster Discrimination for Visual Representation Learning,",
        "abstract": "Contrastive Language Image Pre-training (CLIP) has recently demonstrated\nsuccess across various tasks due to superior feature representation empowered\nby image-text contrastive learning. However, the instance discrimination method\nused by CLIP can hardly encode the semantic structure of training data. To\nhandle this limitation, cluster discrimination has been proposed through\niterative cluster assignment and classification. Nevertheless, most cluster\ndiscrimination approaches only define a single pseudo-label for each image,\nneglecting multi-label signals in the image. In this paper, we propose a novel\nMulti-Label Cluster Discrimination method named MLCD to enhance representation\nlearning. In the clustering step, we first cluster the large-scale LAION-400M\ndataset into one million centers based on off-the-shelf embedding features.\nConsidering that natural images frequently contain multiple visual objects or\nattributes, we select the multiple closest centers as auxiliary class labels.\nIn the discrimination step, we design a novel multi-label classification loss,\nwhich elegantly separates losses from positive classes and negative classes,\nand alleviates ambiguity on decision boundary. We validate the proposed\nmulti-label cluster discrimination method with experiments on different scales\nof models and pre-training datasets. Experimental results show that our method\nachieves state-of-the-art performance on multiple downstream tasks including\nlinear probe, zero-shot classification, and image-text retrieval.",
        "score": 2.2810723781585693
      },
      {
        "title": "You Never Cluster Alone,",
        "abstract": "Recent advances in self-supervised learning with instance-level contrastive\nobjectives facilitate unsupervised clustering. However, a standalone datum is\nnot perceiving the context of the holistic cluster, and may undergo sub-optimal\nassignment. In this paper, we extend the mainstream contrastive learning\nparadigm to a cluster-level scheme, where all the data subjected to the same\ncluster contribute to a unified representation that encodes the context of each\ndata group. Contrastive learning with this representation then rewards the\nassignment of each datum. To implement this vision, we propose twin-contrast\nclustering (TCC). We define a set of categorical variables as clustering\nassignment confidence, which links the instance-level learning track with the\ncluster-level one. On one hand, with the corresponding assignment variables\nbeing the weight, a weighted aggregation along the data points implements the\nset representation of a cluster. We further propose heuristic cluster\naugmentation equivalents to enable cluster-level contrastive learning. On the\nother hand, we derive the evidence lower-bound of the instance-level\ncontrastive objective with the assignments. By reparametrizing the assignment\nvariables, TCC is trained end-to-end, requiring no alternating steps. Extensive\nexperiments show that TCC outperforms the state-of-the-art on challenging\nbenchmarks.",
        "score": 2.2786166667938232
      },
      {
        "title": "Vision Transformer for Contrastive Clustering,",
        "abstract": "Vision Transformer (ViT) has shown its advantages over the convolutional\nneural network (CNN) with its ability to capture global long-range dependencies\nfor visual representation learning. Besides ViT, contrastive learning is\nanother popular research topic recently. While previous contrastive learning\nworks are mostly based on CNNs, some recent studies have attempted to combine\nViT and contrastive learning for enhanced self-supervised learning. Despite the\nconsiderable progress, these combinations of ViT and contrastive learning\nmostly focus on the instance-level contrastiveness, which often overlook the\nglobal contrastiveness and also lack the ability to directly learn the\nclustering result (e.g., for images). In view of this, this paper presents a\nnovel deep clustering approach termed Vision Transformer for Contrastive\nClustering (VTCC), which for the first time, to our knowledge, unifies the\nTransformer and the contrastive learning for the image clustering task.\nSpecifically, with two random augmentations performed on each image, we utilize\na ViT encoder with two weight-sharing views as the backbone. To remedy the\npotential instability of the ViT, we incorporate a convolutional stem to split\neach augmented sample into a sequence of patches, which uses multiple stacked\nsmall convolutions instead of a big convolution in the patch projection layer.\nBy learning the feature representations for the sequences of patches via the\nbackbone, an instance projector and a cluster projector are further utilized to\nperform the instance-level contrastive learning and the global clustering\nstructure learning, respectively. Experiments on eight image datasets\ndemonstrate the stability (during the training-from-scratch) and the\nsuperiority (in clustering performance) of our VTCC approach over the\nstate-of-the-art.",
        "score": 2.2715706825256348
      },
      {
        "title": "Cluster-aware Contrastive Learning for Unsupervised Out-of-distribution\n  Detection,",
        "abstract": "Unsupervised out-of-distribution (OOD) Detection aims to separate the samples\nfalling outside the distribution of training data without label information.\nAmong numerous branches, contrastive learning has shown its excellent\ncapability of learning discriminative representation in OOD detection. However,\nfor its limited vision, merely focusing on instance-level relationship between\naugmented samples, it lacks attention to the relationship between samples with\nsame semantics. Based on the classic contrastive learning, we propose\nCluster-aware Contrastive Learning (CCL) framework for unsupervised OOD\ndetection, which considers both instance-level and semantic-level information.\nSpecifically, we study a cooperation strategy of clustering and contrastive\nlearning to effectively extract the latent semantics and design a cluster-aware\ncontrastive loss function to enhance OOD discriminative ability. The loss\nfunction can simultaneously pay attention to the global and local relationships\nby treating both the cluster centers and the samples belonging to the same\ncluster as positive samples. We conducted sufficient experiments to verify the\neffectiveness of our framework and the model achieves significant improvement\non various image benchmarks.",
        "score": 2.1198811531066895
      },
      {
        "title": "Contrastive Representation Disentanglement for Clustering,",
        "abstract": "Clustering continues to be a significant and challenging task. Recent studies\nhave demonstrated impressive results by applying clustering to feature\nrepresentations acquired through self-supervised learning, particularly on\nsmall datasets. However, when dealing with datasets containing a large number\nof clusters, such as ImageNet, current methods struggle to achieve satisfactory\nclustering performance. In this paper, we introduce a novel method called\nContrastive representation Disentanglement for Clustering (CDC) that leverages\ncontrastive learning to directly disentangle the feature representation for\nclustering. In CDC, we decompose the representation into two distinct\ncomponents: one component encodes categorical information under an\nequipartition constraint, and the other component captures instance-specific\nfactors. To train our model, we propose a contrastive loss that effectively\nutilizes both components of the representation. We conduct a theoretical\nanalysis of the proposed loss and highlight how it assigns different weights to\nnegative samples during the process of disentangling the feature\nrepresentation. Further analysis of the gradients reveals that larger weights\nemphasize a stronger focus on hard negative samples. As a result, the proposed\nloss exhibits strong expressiveness, enabling efficient disentanglement of\ncategorical information. Through experimental evaluation on various benchmark\ndatasets, our method demonstrates either state-of-the-art or highly competitive\nclustering performance. Notably, on the complete ImageNet dataset, we achieve\nan accuracy of 53.4%, surpassing existing methods by a substantial margin of\n+10.2%.",
        "score": 2.098836898803711
      },
      {
        "title": "DeepCluE: Enhanced Image Clustering via Multi-layer Ensembles in Deep\n  Neural Networks,",
        "abstract": "Deep clustering has recently emerged as a promising technique for complex\ndata clustering. Despite the considerable progress, previous deep clustering\nworks mostly build or learn the final clustering by only utilizing a single\nlayer of representation, e.g., by performing the K-means clustering on the last\nfully-connected layer or by associating some clustering loss to a specific\nlayer, which neglect the possibilities of jointly leveraging multi-layer\nrepresentations for enhancing the deep clustering performance. In view of this,\nthis paper presents a Deep Clustering via Ensembles (DeepCluE) approach, which\nbridges the gap between deep clustering and ensemble clustering by harnessing\nthe power of multiple layers in deep neural networks. In particular, we utilize\na weight-sharing convolutional neural network as the backbone, which is trained\nwith both the instance-level contrastive learning (via an instance projector)\nand the cluster-level contrastive learning (via a cluster projector) in an\nunsupervised manner. Thereafter, multiple layers of feature representations are\nextracted from the trained network, upon which the ensemble clustering process\nis further conducted. Specifically, a set of diversified base clusterings are\ngenerated from the multi-layer representations via a highly efficient\nclusterer. Then the reliability of clusters in multiple base clusterings is\nautomatically estimated by exploiting an entropy-based criterion, based on\nwhich the set of base clusterings are re-formulated into a weighted-cluster\nbipartite graph. By partitioning this bipartite graph via transfer cut, the\nfinal consensus clustering can be obtained. Experimental results on six image\ndatasets confirm the advantages of DeepCluE over the state-of-the-art deep\nclustering approaches.",
        "score": 2.075887680053711
      },
      {
        "title": "Strongly Augmented Contrastive Clustering,",
        "abstract": "Deep clustering has attracted increasing attention in recent years due to its\ncapability of joint representation learning and clustering via deep neural\nnetworks. In its latest developments, the contrastive learning has emerged as\nan effective technique to substantially enhance the deep clustering\nperformance. However, the existing contrastive learning based deep clustering\nalgorithms mostly focus on some carefully-designed augmentations (often with\nlimited transformations to preserve the structure), referred to as weak\naugmentations, but cannot go beyond the weak augmentations to explore the more\nopportunities in stronger augmentations (with more aggressive transformations\nor even severe distortions). In this paper, we present an end-to-end deep\nclustering approach termed Strongly Augmented Contrastive Clustering (SACC),\nwhich extends the conventional two-augmentation-view paradigm to multiple views\nand jointly leverages strong and weak augmentations for strengthened deep\nclustering. Particularly, we utilize a backbone network with triply-shared\nweights, where a strongly augmented view and two weakly augmented views are\nincorporated. Based on the representations produced by the backbone, the\nweak-weak view pair and the strong-weak view pairs are simultaneously exploited\nfor the instance-level contrastive learning (via an instance projector) and the\ncluster-level contrastive learning (via a cluster projector), which, together\nwith the backbone, can be jointly optimized in a purely unsupervised manner.\nExperimental results on five challenging image datasets have shown the\nsuperiority of our SACC approach over the state-of-the-art. The code is\navailable at https://github.com/dengxiaozhi/SACC.",
        "score": 2.0704994201660156
      },
      {
        "title": "Joint Debiased Representation and Image Clustering Learning with\n  Self-Supervision,",
        "abstract": "Contrastive learning is among the most successful methods for visual\nrepresentation learning, and its performance can be further improved by jointly\nperforming clustering on the learned representations. However, existing methods\nfor joint clustering and contrastive learning do not perform well on\nlong-tailed data distributions, as majority classes overwhelm and distort the\nloss of minority classes, thus preventing meaningful representations to be\nlearned. Motivated by this, we develop a novel joint clustering and contrastive\nlearning framework by adapting the debiased contrastive loss to avoid\nunder-clustering minority classes of imbalanced datasets. We show that our\nproposed modified debiased contrastive loss and divergence clustering loss\nimproves the performance across multiple datasets and learning tasks. The\nsource code is available at\nhttps://anonymous.4open.science/r/SSL-debiased-clustering",
        "score": 2.0622735023498535
      },
      {
        "title": "Deep Multiview Clustering by Contrasting Cluster Assignments,",
        "abstract": "Multiview clustering (MVC) aims to reveal the underlying structure of\nmultiview data by categorizing data samples into clusters. Deep learning-based\nmethods exhibit strong feature learning capabilities on large-scale datasets.\nFor most existing deep MVC methods, exploring the invariant representations of\nmultiple views is still an intractable problem. In this paper, we propose a\ncross-view contrastive learning (CVCL) method that learns view-invariant\nrepresentations and produces clustering results by contrasting the cluster\nassignments among multiple views. Specifically, we first employ deep\nautoencoders to extract view-dependent features in the pretraining stage. Then,\na cluster-level CVCL strategy is presented to explore consistent semantic label\ninformation among the multiple views in the fine-tuning stage. Thus, the\nproposed CVCL method is able to produce more discriminative cluster assignments\nby virtue of this learning strategy. Moreover, we provide a theoretical\nanalysis of soft cluster assignment alignment. Extensive experimental results\nobtained on several datasets demonstrate that the proposed CVCL method\noutperforms several state-of-the-art approaches.",
        "score": 2.041688919067383
      },
      {
        "title": "Learning Representations for Clustering via Partial Information\n  Discrimination and Cross-Level Interaction,",
        "abstract": "In this paper, we present a novel deep image clustering approach termed PICI,\nwhich enforces the partial information discrimination and the cross-level\ninteraction in a joint learning framework. In particular, we leverage a\nTransformer encoder as the backbone, through which the masked image modeling\nwith two paralleled augmented views is formulated. After deriving the class\ntokens from the masked images by the Transformer encoder, three partial\ninformation learning modules are further incorporated, including the PISD\nmodule for training the auto-encoder via masked image reconstruction, the PICD\nmodule for employing two levels of contrastive learning, and the CLI module for\nmutual interaction between the instance-level and cluster-level subspaces.\nExtensive experiments have been conducted on six real-world image datasets,\nwhich demononstrate the superior clustering performance of the proposed PICI\napproach over the state-of-the-art deep clustering approaches. The source code\nis available at https://github.com/Regan-Zhang/PICI.",
        "score": 2.0056729316711426
      },
      {
        "title": "Self-supervised Image Clustering from Multiple Incomplete Views via\n  Constrastive Complementary Generation,",
        "abstract": "Incomplete Multi-View Clustering aims to enhance clustering performance by\nusing data from multiple modalities. Despite the fact that several approaches\nfor studying this issue have been proposed, the following drawbacks still\npersist: 1) It's difficult to learn latent representations that account for\ncomplementarity yet consistency without using label information; 2) and thus\nfails to take full advantage of the hidden information in incomplete data\nresults in suboptimal clustering performance when complete data is scarce. In\nthis paper, we propose Contrastive Incomplete Multi-View Image Clustering with\nGenerative Adversarial Networks (CIMIC-GAN), which uses GAN to fill in\nincomplete data and uses double contrastive learning to learn consistency on\ncomplete and incomplete data. More specifically, considering diversity and\ncomplementary information among multiple modalities, we incorporate\nautoencoding representation of complete and incomplete data into double\ncontrastive learning to achieve learning consistency. Integrating GANs into the\nautoencoding process can not only take full advantage of new features of\nincomplete data, but also better generalize the model in the presence of high\ndata missing rates. Experiments conducted on \\textcolor{black}{four}\nextensively-used datasets show that CIMIC-GAN outperforms state-of-the-art\nincomplete multi-View clustering methods.",
        "score": 1.9122776985168457
      },
      {
        "title": "Text-Guided Image Clustering,",
        "abstract": "Image clustering divides a collection of images into meaningful groups,\ntypically interpreted post-hoc via human-given annotations. Those are usually\nin the form of text, begging the question of using text as an abstraction for\nimage clustering. Current image clustering methods, however, neglect the use of\ngenerated textual descriptions. We, therefore, propose Text-Guided Image\nClustering, i.e., generating text using image captioning and visual\nquestion-answering (VQA) models and subsequently clustering the generated text.\nFurther, we introduce a novel approach to inject task- or domain knowledge for\nclustering by prompting VQA models. Across eight diverse image clustering\ndatasets, our results show that the obtained text representations often\noutperform image features. Additionally, we propose a counting-based cluster\nexplainability method. Our evaluations show that the derived keyword-based\nexplanations describe clusters better than the respective cluster accuracy\nsuggests. Overall, this research challenges traditional approaches and paves\nthe way for a paradigm shift in image clustering, using generated text.",
        "score": 1.7820048332214355
      },
      {
        "title": "CODER: Coupled Diversity-Sensitive Momentum Contrastive Learning for\n  Image-Text Retrieval,",
        "abstract": "Image-Text Retrieval (ITR) is challenging in bridging visual and lingual\nmodalities. Contrastive learning has been adopted by most prior arts. Except\nfor limited amount of negative image-text pairs, the capability of constrastive\nlearning is restricted by manually weighting negative pairs as well as\nunawareness of external knowledge. In this paper, we propose our novel Coupled\nDiversity-Sensitive Momentum Constrastive Learning (CODER) for improving\ncross-modal representation. Firstly, a novel diversity-sensitive contrastive\nlearning (DCL) architecture is invented. We introduce dynamic dictionaries for\nboth modalities to enlarge the scale of image-text pairs, and\ndiversity-sensitiveness is achieved by adaptive negative pair weighting.\nFurthermore, two branches are designed in CODER. One learns instance-level\nembeddings from image/text, and it also generates pseudo online clustering\nlabels for its input image/text based on their embeddings. Meanwhile, the other\nbranch learns to query from commonsense knowledge graph to form concept-level\ndescriptors for both modalities. Afterwards, both branches leverage DCL to\nalign the cross-modal embedding spaces while an extra pseudo clustering label\nprediction loss is utilized to promote concept-level representation learning\nfor the second branch. Extensive experiments conducted on two popular\nbenchmarks, i.e. MSCOCO and Flicker30K, validate CODER remarkably outperforms\nthe state-of-the-art approaches.",
        "score": 1.6714941263198853
      },
      {
        "title": "Asymmetric double-winged multi-view clustering network for exploring\n  Diverse and Consistent Information,",
        "abstract": "In unsupervised scenarios, deep contrastive multi-view clustering (DCMVC) is\nbecoming a hot research spot, which aims to mine the potential relationships\nbetween different views. Most existing DCMVC algorithms focus on exploring the\nconsistency information for the deep semantic features, while ignoring the\ndiverse information on shallow features. To fill this gap, we propose a novel\nmulti-view clustering network termed CodingNet to explore the diverse and\nconsistent information simultaneously in this paper. Specifically, instead of\nutilizing the conventional auto-encoder, we design an asymmetric structure\nnetwork to extract shallow and deep features separately. Then, by aligning the\nsimilarity matrix on the shallow feature to the zero matrix, we ensure the\ndiversity for the shallow features, thus offering a better description of\nmulti-view data. Moreover, we propose a dual contrastive mechanism that\nmaintains consistency for deep features at both view-feature and pseudo-label\nlevels. Our framework's efficacy is validated through extensive experiments on\nsix widely used benchmark datasets, outperforming most state-of-the-art\nmulti-view clustering algorithms.",
        "score": 1.630255937576294
      },
      {
        "title": "Deep Clustering by Semantic Contrastive Learning,",
        "abstract": "Whilst contrastive learning has recently brought notable benefits to deep\nclustering of unlabelled images by learning sample-specific discriminative\nvisual features, its potential for explicitly inferring class decision\nboundaries is less well understood. This is because its instance discrimination\nstrategy is not class sensitive, therefore, the clusters derived on the\nresulting sample-specific feature space are not optimised for corresponding to\nmeaningful class decision boundaries. In this work, we solve this problem by\nintroducing Semantic Contrastive Learning (SCL). SCL imposes explicitly\ndistance-based cluster structures on unlabelled training data by formulating a\nsemantic (cluster-aware) contrastive learning objective. Moreover, we introduce\na clustering consistency condition to be satisfied jointly by both instance\nvisual similarities and cluster decision boundaries, and concurrently\noptimising both to reason about the hypotheses of semantic ground-truth classes\n(unknown/unlabelled) on-the-fly by their consensus. This semantic contrastive\nlearning approach to discovering unknown class decision boundaries has\nconsiderable advantages to unsupervised learning of object recognition tasks.\nExtensive experiments show that SCL outperforms state-of-the-art contrastive\nlearning and deep clustering methods on six object recognition benchmarks,\nespecially on the more challenging finer-grained and larger datasets.",
        "score": 1.6285462379455566
      },
      {
        "title": "Deep Contrastive Multi-view Clustering under Semantic Feature Guidance,",
        "abstract": "Contrastive learning has achieved promising performance in the field of\nmulti-view clustering recently. However, the positive and negative sample\nconstruction mechanisms ignoring semantic consistency lead to false negative\npairs, limiting the performance of existing algorithms from further\nimprovement. To solve this problem, we propose a multi-view clustering\nframework named Deep Contrastive Multi-view Clustering under Semantic feature\nguidance (DCMCS) to alleviate the influence of false negative pairs.\nSpecifically, view-specific features are firstly extracted from raw features\nand fused to obtain fusion view features according to view importance. To\nmitigate the interference of view-private information, specific view and fusion\nview semantic features are learned by cluster-level contrastive learning and\nconcatenated to measure the semantic similarity of instances. By minimizing\ninstance-level contrastive loss weighted by semantic similarity, DCMCS\nadaptively weakens contrastive leaning between false negative pairs.\nExperimental results on several public datasets demonstrate the proposed\nframework outperforms the state-of-the-art methods.",
        "score": 1.1951011419296265
      },
      {
        "title": "Representation Learning via Consistent Assignment of Views to Clusters,",
        "abstract": "We introduce Consistent Assignment for Representation Learning (CARL), an\nunsupervised learning method to learn visual representations by combining ideas\nfrom self-supervised contrastive learning and deep clustering. By viewing\ncontrastive learning from a clustering perspective, CARL learns unsupervised\nrepresentations by learning a set of general prototypes that serve as energy\nanchors to enforce different views of a given image to be assigned to the same\nprototype. Unlike contemporary work on contrastive learning with deep\nclustering, CARL proposes to learn the set of general prototypes in an online\nfashion, using gradient descent without the necessity of using\nnon-differentiable algorithms or K-Means to solve the cluster assignment\nproblem. CARL surpasses its competitors in many representations learning\nbenchmarks, including linear evaluation, semi-supervised learning, and transfer\nlearning.",
        "score": 1.1696009635925293
      },
      {
        "title": "UNIMO: Towards Unified-Modal Understanding and Generation via\n  Cross-Modal Contrastive Learning,",
        "abstract": "Existed pre-training methods either focus on single-modal tasks or\nmulti-modal tasks, and cannot effectively adapt to each other. They can only\nutilize single-modal data (i.e. text or image) or limited multi-modal data\n(i.e. image-text pairs). In this work, we propose a unified-modal pre-training\narchitecture, namely UNIMO, which can effectively adapt to both single-modal\nand multi-modal understanding and generation tasks. Large scale of free text\ncorpus and image collections can be utilized to improve the capability of\nvisual and textual understanding, and cross-modal contrastive learning (CMCL)\nis leveraged to align the textual and visual information into a unified\nsemantic space over a corpus of image-text pairs. As the non-paired\nsingle-modal data is very rich, our model can utilize much larger scale of data\nto learn more generalizable representations. Moreover, the textual knowledge\nand visual knowledge can enhance each other in the unified semantic space. The\nexperimental results show that UNIMO significantly improves the performance of\nseveral single-modal and multi-modal downstream tasks. Our code and pre-trained\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/",
        "score": 1.0792160034179688
      },
      {
        "title": "Text-Guided Alternative Image Clustering,",
        "abstract": "Traditional image clustering techniques only find a single grouping within\nvisual data. In particular, they do not provide a possibility to explicitly\ndefine multiple types of clustering. This work explores the potential of large\nvision-language models to facilitate alternative image clustering. We propose\nText-Guided Alternative Image Consensus Clustering (TGAICC), a novel approach\nthat leverages user-specified interests via prompts to guide the discovery of\ndiverse clusterings. To achieve this, it generates a clustering for each\nprompt, groups them using hierarchical clustering, and then aggregates them\nusing consensus clustering. TGAICC outperforms image- and text-based baselines\non four alternative image clustering benchmark datasets. Furthermore, using\ncount-based word statistics, we are able to obtain text-based explanations of\nthe alternative clusterings. In conclusion, our research illustrates how\ncontemporary large vision-language models can transform explanatory data\nanalysis, enabling the generation of insightful, customizable, and diverse\nimage clusterings.",
        "score": 1.0479419231414795
      },
      {
        "title": "Multi-Modal Proxy Learning Towards Personalized Visual Multiple\n  Clustering,",
        "abstract": "Multiple clustering has gained significant attention in recent years due to\nits potential to reveal multiple hidden structures of data from different\nperspectives. The advent of deep multiple clustering techniques has notably\nadvanced the performance by uncovering complex patterns and relationships\nwithin large datasets. However, a major challenge arises as users often do not\nneed all the clusterings that algorithms generate, and figuring out the one\nneeded requires a substantial understanding of each clustering result.\nTraditionally, aligning a user's brief keyword of interest with the\ncorresponding vision components was challenging, but the emergence of\nmulti-modal and large language models (LLMs) has begun to bridge this gap. In\nresponse, given unlabeled target visual data, we propose Multi-MaP, a novel\nmethod employing a multi-modal proxy learning process. It leverages CLIP\nencoders to extract coherent text and image embeddings, with GPT-4 integrating\nusers' interests to formulate effective textual contexts. Moreover, reference\nword constraint and concept-level constraint are designed to learn the optimal\ntext proxy according to the user's interest. Multi-MaP not only adeptly\ncaptures a user's interest via a keyword but also facilitates identifying\nrelevant clusterings. Our extensive experiments show that Multi-MaP\nconsistently outperforms state-of-the-art methods in all benchmark\nmulti-clustering vision tasks. Our code is available at\nhttps://github.com/Alexander-Yao/Multi-MaP.",
        "score": 0.9930740594863892
      },
      {
        "title": "Image Clustering via the Principle of Rate Reduction in the Age of\n  Pretrained Models,",
        "abstract": "The advent of large pre-trained models has brought about a paradigm shift in\nboth visual representation learning and natural language processing. However,\nclustering unlabeled images, as a fundamental and classic machine learning\nproblem, still lacks an effective solution, particularly for large-scale\ndatasets. In this paper, we propose a novel image clustering pipeline that\nleverages the powerful feature representation of large pre-trained models such\nas CLIP and cluster images effectively and efficiently at scale. We first\ndeveloped a novel algorithm to estimate the number of clusters in a given\ndataset. We then show that the pre-trained features are significantly more\nstructured by further optimizing the rate reduction objective. The resulting\nfeatures may significantly improve the clustering accuracy, e.g., from 57\\% to\n66\\% on ImageNet-1k. Furthermore, by leveraging CLIP's multimodality bridge\nbetween image and text, we develop a simple yet effective self-labeling\nalgorithm that produces meaningful captions for the clusters. Through extensive\nexperiments, we show that our pipeline works well on standard datasets such as\nCIFAR-10, CIFAR-100, and ImageNet-1k. It also extends to datasets that are not\ncurated for clustering, such as LAION-Aesthetics and WikiArts. We released the\ncode in https://github.com/LeslieTrue/CPP.",
        "score": 0.9624717831611633
      },
      {
        "title": "HCSC: Hierarchical Contrastive Selective Coding,",
        "abstract": "Hierarchical semantic structures naturally exist in an image dataset, in\nwhich several semantically relevant image clusters can be further integrated\ninto a larger cluster with coarser-grained semantics. Capturing such structures\nwith image representations can greatly benefit the semantic understanding on\nvarious downstream tasks. Existing contrastive representation learning methods\nlack such an important model capability. In addition, the negative pairs used\nin these methods are not guaranteed to be semantically distinct, which could\nfurther hamper the structural correctness of learned image representations. To\ntackle these limitations, we propose a novel contrastive learning framework\ncalled Hierarchical Contrastive Selective Coding (HCSC). In this framework, a\nset of hierarchical prototypes are constructed and also dynamically updated to\nrepresent the hierarchical semantic structures underlying the data in the\nlatent space. To make image representations better fit such semantic\nstructures, we employ and further improve conventional instance-wise and\nprototypical contrastive learning via an elaborate pair selection scheme. This\nscheme seeks to select more diverse positive pairs with similar semantics and\nmore precise negative pairs with truly distinct semantics. On extensive\ndownstream tasks, we verify the superior performance of HCSC over\nstate-of-the-art contrastive methods, and the effectiveness of major model\ncomponents is proved by plentiful analytical studies. We build a comprehensive\nmodel zoo in Sec. D. Our source code and model weights are available at\nhttps://github.com/gyfastas/HCSC",
        "score": 0.9370460510253906
      },
      {
        "title": "Joint Debiased Representation Learning and Imbalanced Data Clustering,",
        "abstract": "One of the most promising approaches for unsupervised learning is combining\ndeep representation learning and deep clustering. Some recent works propose to\nsimultaneously learn representation using deep neural networks and perform\nclustering by defining a clustering loss on top of embedded features. However,\nthese approaches are sensitive to imbalanced data and out-of-distribution\nsamples. As a consequence, these methods optimize clustering by pushing data\nclose to randomly initialized cluster centers. This is problematic when the\nnumber of instances varies largely in different classes or a cluster with few\nsamples has less chance to be assigned a good centroid. To overcome these\nlimitations, we introduce a new unsupervised framework for joint debiased\nrepresentation learning and image clustering. We simultaneously train two deep\nlearning models, a deep representation network that captures the data\ndistribution, and a deep clustering network that learns embedded features and\nperforms clustering. Specifically, the clustering network and learning\nrepresentation network both take advantage of our proposed statistics pooling\nblock that represents mean, variance, and cardinality to handle the\nout-of-distribution samples and class imbalance. Our experiments show that\nusing these representations, one can considerably improve results on imbalanced\nimage clustering across a variety of image datasets. Moreover, the learned\nrepresentations generalize well when transferred to the out-of-distribution\ndataset.",
        "score": 0.7913407683372498
      },
      {
        "title": "A Survey on Deep Clustering: From the Prior Perspective,",
        "abstract": "Facilitated by the powerful feature extraction ability of neural networks,\ndeep clustering has achieved great success in analyzing high-dimensional and\ncomplex real-world data. The performance of deep clustering methods is affected\nby various factors such as network structures and learning objectives. However,\nas pointed out in this survey, the essence of deep clustering lies in the\nincorporation and utilization of prior knowledge, which is largely ignored by\nexisting works. From pioneering deep clustering methods based on data structure\nassumptions to recent contrastive clustering methods based on data augmentation\ninvariances, the development of deep clustering intrinsically corresponds to\nthe evolution of prior knowledge. In this survey, we provide a comprehensive\nreview of deep clustering methods by categorizing them into six types of prior\nknowledge. We find that in general the prior innovation follows two trends,\nnamely, i) from mining to constructing, and ii) from internal to external.\nBesides, we provide a benchmark on five widely-used datasets and analyze the\nperformance of methods with diverse priors. By providing a novel prior\nknowledge perspective, we hope this survey could provide some novel insights\nand inspire future research in the deep clustering community.",
        "score": 0.6249865293502808
      },
      {
        "title": "Federated Momentum Contrastive Clustering,",
        "abstract": "We present federated momentum contrastive clustering (FedMCC), a learning\nframework that can not only extract discriminative representations over\ndistributed local data but also perform data clustering. In FedMCC, a\ntransformed data pair passes through both the online and target networks,\nresulting in four representations over which the losses are determined. The\nresulting high-quality representations generated by FedMCC can outperform\nseveral existing self-supervised learning methods for linear evaluation and\nsemi-supervised learning tasks. FedMCC can easily be adapted to ordinary\ncentralized clustering through what we call momentum contrastive clustering\n(MCC). We show that MCC achieves state-of-the-art clustering accuracy results\nin certain datasets such as STL-10 and ImageNet-10. We also present a method to\nreduce the memory footprint of our clustering schemes.",
        "score": 0.6135175824165344
      },
      {
        "title": "Non-Linguistic Supervision for Contrastive Learning of Sentence\n  Embeddings,",
        "abstract": "Semantic representation learning for sentences is an important and\nwell-studied problem in NLP. The current trend for this task involves training\na Transformer-based sentence encoder through a contrastive objective with text,\ni.e., clustering sentences with semantically similar meanings and scattering\nothers. In this work, we find the performance of Transformer models as sentence\nencoders can be improved by training with multi-modal multi-task losses, using\nunpaired examples from another modality (e.g., sentences and unrelated\nimage/audio data). In particular, besides learning by the contrastive loss on\ntext, our model clusters examples from a non-linguistic domain (e.g.,\nvisual/audio) with a similar contrastive loss at the same time. The reliance of\nour framework on unpaired non-linguistic data makes it language-agnostic,\nenabling it to be widely applicable beyond English NLP. Experiments on 7\nsemantic textual similarity benchmarks reveal that models trained with the\nadditional non-linguistic (images/audio) contrastive objective lead to higher\nquality sentence embeddings. This indicates that Transformer models are able to\ngeneralize better by doing a similar task (i.e., clustering) with unpaired\nexamples from different modalities in a multi-task fashion.",
        "score": 0.4400148093700409
      },
      {
        "title": "Learning Visual Representations via Language-Guided Sampling,",
        "abstract": "Although an object may appear in numerous contexts, we often describe it in a\nlimited number of ways. Language allows us to abstract away visual variation to\nrepresent and communicate concepts. Building on this intuition, we propose an\nalternative approach to visual representation learning: using language\nsimilarity to sample semantically similar image pairs for contrastive learning.\nOur approach diverges from image-based contrastive learning by sampling view\npairs using language similarity instead of hand-crafted augmentations or\nlearned clusters. Our approach also differs from image-text contrastive\nlearning by relying on pre-trained language models to guide the learning rather\nthan directly minimizing a cross-modal loss. Through a series of experiments,\nwe show that language-guided learning yields better features than image-based\nand image-text representation learning approaches.",
        "score": 0.26607486605644226
      }
    ]
  },
  {
    "title": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications",
    "abstract": "  The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications.\n",
    "related_paper_titles": [
      "CLUE: A Clinical Language Understanding Evaluation for LLMs",
      "A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models :\n  Safety, Consensus, Objectivity, Reproducibility and Explainability",
      "Clinical Camel: An Open Expert-Level Medical Language Model with\n  Dialogue-Based Knowledge Encoding"
    ],
    "related_paper_abstract": [
      "  Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, evaluation has\nprimarily been limited to non-clinical tasks, which do not reflect the\ncomplexity of practical clinical applications. To fill this gap, we present the\nClinical Language Understanding Evaluation (CLUE), a benchmark tailored to\nevaluate LLMs on clinical tasks. CLUE includes six tasks to test the practical\napplicability of LLMs in complex healthcare settings. Our evaluation includes a\ntotal of $25$ LLMs. In contrast to previous evaluations, CLUE shows a decrease\nin performance for nine out of twelve biomedical models. Our benchmark\nrepresents a step towards a standardized approach to evaluating and developing\nLLMs in healthcare to align future model development with the real-world needs\nof clinical application. We open-source all evaluation scripts and datasets for\nfuture research at https://github.com/TIO-IKIM/CLUE.\n",
      "  A comprehensive qualitative evaluation framework for large language models\n(LLM) in healthcare that expands beyond traditional accuracy and quantitative\nmetrics needed. We propose 5 key aspects for evaluation of LLMs: Safety,\nConsensus, Objectivity, Reproducibility and Explainability (S.C.O.R.E.). We\nsuggest that S.C.O.R.E. may form the basis for an evaluation framework for\nfuture LLM-based models that are safe, reliable, trustworthy, and ethical for\nhealthcare and clinical applications.\n",
      "  We present Clinical Camel, an open large language model (LLM) explicitly\ntailored for clinical research. Fine-tuned from LLaMA-2 using QLoRA, Clinical\nCamel achieves state-of-the-art performance across medical benchmarks among\nopenly available medical LLMs. Leveraging efficient single-GPU training,\nClinical Camel surpasses GPT-3.5 in five-shot evaluations on all assessed\nbenchmarks, including 64.3% on the USMLE Sample Exam (compared to 58.5% for\nGPT-3.5), 77.9% on PubMedQA (compared to 60.2%), 60.7% on MedQA (compared to\n53.6%), and 54.2% on MedMCQA (compared to 51.0%). In addition to these\nbenchmarks, Clinical Camel demonstrates its broader capabilities, such as\nsynthesizing plausible clinical notes. This work introduces dialogue-based\nknowledge encoding, a novel method to synthesize conversational data from dense\nmedical texts. While benchmark results are encouraging, extensive and rigorous\nhuman evaluation across diverse clinical scenarios is imperative to ascertain\nsafety before implementation. By openly sharing Clinical Camel, we hope to\nfoster transparent and collaborative research, working towards the safe\nintegration of LLMs within the healthcare domain. Significant challenges\nconcerning reliability, bias, and the potential for outdated knowledge persist.\nNonetheless, the transparency provided by an open approach reinforces the\nscientific rigor essential for future clinical applications.\n"
    ],
    "entities": [
      "Large Language Models Large Language Models",
      "Persian",
      "Mistral",
      "LLaMA",
      "SFT",
      "DPO",
      "Large Language Model",
      "ICL",
      "CoT",
      "Llama",
      "Language Models",
      "CAD",
      "GPT",
      "Retrieval Augmented",
      "Adam",
      "OpenAI",
      "RLHF",
      "NLG",
      "Human Feedback",
      "ToM",
      "Gemini",
      "APIs",
      "DTs",
      "NLP",
      "Chinese",
      "Direct Preference",
      "SVM",
      "SLMs",
      "KG",
      "MATH"
    ],
    "retrieved_papers": [
      {
        "title": "Large language models in healthcare and medical domain: A review,",
        "abstract": "The deployment of large language models (LLMs) within the healthcare sector\nhas sparked both enthusiasm and apprehension. These models exhibit the\nremarkable capability to provide proficient responses to free-text queries,\ndemonstrating a nuanced understanding of professional medical knowledge. This\ncomprehensive survey delves into the functionalities of existing LLMs designed\nfor healthcare applications, elucidating the trajectory of their development,\nstarting from traditional Pretrained Language Models (PLMs) to the present\nstate of LLMs in healthcare sector. First, we explore the potential of LLMs to\namplify the efficiency and effectiveness of diverse healthcare applications,\nparticularly focusing on clinical language understanding tasks. These tasks\nencompass a wide spectrum, ranging from named entity recognition and relation\nextraction to natural language inference, multi-modal medical applications,\ndocument classification, and question-answering. Additionally, we conduct an\nextensive comparison of the most recent state-of-the-art LLMs in the healthcare\ndomain, while also assessing the utilization of various open-source LLMs and\nhighlighting their significance in healthcare applications. Furthermore, we\npresent the essential performance metrics employed to evaluate LLMs in the\nbiomedical domain, shedding light on their effectiveness and limitations.\nFinally, we summarize the prominent challenges and constraints faced by large\nlanguage models in the healthcare sector, offering a holistic perspective on\ntheir potential benefits and shortcomings. This review provides a comprehensive\nexploration of the current landscape of LLMs in healthcare, addressing their\nrole in transforming medical applications and the areas that warrant further\nresearch and development.",
        "score": 4.135923862457275
      },
      {
        "title": "A Comprehensive Survey on Evaluating Large Language Model Applications\n  in the Medical Industry,",
        "abstract": "Since the inception of the Transformer architecture in 2017, Large Language\nModels (LLMs) such as GPT and BERT have evolved significantly, impacting\nvarious industries with their advanced capabilities in language understanding\nand generation. These models have shown potential to transform the medical\nfield, highlighting the necessity for specialized evaluation frameworks to\nensure their effective and ethical deployment. This comprehensive survey\ndelineates the extensive application and requisite evaluation of LLMs within\nhealthcare, emphasizing the critical need for empirical validation to fully\nexploit their capabilities in enhancing healthcare outcomes. Our survey is\nstructured to provide an in-depth analysis of LLM applications across clinical\nsettings, medical text data processing, research, education, and public health\nawareness. We begin by exploring the roles of LLMs in various medical\napplications, detailing their evaluation based on performance in tasks such as\nclinical diagnosis, medical text data processing, information retrieval, data\nanalysis, and educational content generation. The subsequent sections offer a\ncomprehensive discussion on the evaluation methods and metrics employed,\nincluding models, evaluators, and comparative experiments. We further examine\nthe benchmarks and datasets utilized in these evaluations, providing a\ncategorized description of benchmarks for tasks like question answering,\nsummarization, information extraction, bioinformatics, information retrieval\nand general comprehensive benchmarks. This structure ensures a thorough\nunderstanding of how LLMs are assessed for their effectiveness, accuracy,\nusability, and ethical alignment in the medical domain. ...",
        "score": 4.082762718200684
      },
      {
        "title": "A Survey on Medical Large Language Models: Technology, Application,\n  Trustworthiness, and Future Directions,",
        "abstract": "Large language models (LLMs), such as GPT series models, have received\nsubstantial attention due to their impressive capabilities for generating and\nunderstanding human-level language. More recently, LLMs have emerged as an\ninnovative and powerful adjunct in the medical field, transforming traditional\npractices and heralding a new era of enhanced healthcare services. This survey\nprovides a comprehensive overview of Medical Large Language Models (Med-LLMs),\noutlining their evolution from general to the medical-specific domain (i.e,\nTechnology and Application), as well as their transformative impact on\nhealthcare (e.g., Trustworthiness and Safety). Concretely, starting from the\nfundamental history and technology of LLMs, we first delve into the progressive\nadaptation and refinements of general LLM models in the medical domain,\nespecially emphasizing the advanced algorithms that boost the LLMs' performance\nin handling complicated medical environments, including clinical reasoning,\nknowledge graph, retrieval-augmented generation, human alignment, and\nmulti-modal learning. Secondly, we explore the extensive applications of\nMed-LLMs across domains such as clinical decision support, report generation,\nand medical education, illustrating their potential to streamline healthcare\nservices and augment patient outcomes. Finally, recognizing the imperative and\nresponsible innovation, we discuss the challenges of ensuring fairness,\naccountability, privacy, and robustness in Med-LLMs applications. Finally, we\nconduct a concise discussion for anticipating possible future trajectories of\nMed-LLMs, identifying avenues for the prudent expansion of Med-LLMs. By\nconsolidating above-mentioned insights, this review seeks to provide a\ncomprehensive investigation of the potential strengths and limitations of\nMed-LLMs for professionals and researchers, ensuring a responsible landscape in\nthe healthcare setting.",
        "score": 3.807290554046631
      },
      {
        "title": "Evaluating large language models in medical applications: a survey,",
        "abstract": "Large language models (LLMs) have emerged as powerful tools with\ntransformative potential across numerous domains, including healthcare and\nmedicine. In the medical domain, LLMs hold promise for tasks ranging from\nclinical decision support to patient education. However, evaluating the\nperformance of LLMs in medical contexts presents unique challenges due to the\ncomplex and critical nature of medical information. This paper provides a\ncomprehensive overview of the landscape of medical LLM evaluation, synthesizing\ninsights from existing studies and highlighting evaluation data sources, task\nscenarios, and evaluation methods. Additionally, it identifies key challenges\nand opportunities in medical LLM evaluation, emphasizing the need for continued\nresearch and innovation to ensure the responsible integration of LLMs into\nclinical practice.",
        "score": 3.2945265769958496
      },
      {
        "title": "Guiding Clinical Reasoning with Large Language Models via Knowledge\n  Seeds,",
        "abstract": "Clinical reasoning refers to the cognitive process that physicians employ in\nevaluating and managing patients. This process typically involves suggesting\nnecessary examinations, diagnosing patients' diseases, and deciding on\nappropriate therapies, etc. Accurate clinical reasoning requires extensive\nmedical knowledge and rich clinical experience, setting a high bar for\nphysicians. This is particularly challenging in developing countries due to the\noverwhelming number of patients and limited physician resources, contributing\nsignificantly to global health inequity and necessitating automated clinical\nreasoning approaches. Recently, the emergence of large language models (LLMs)\nsuch as ChatGPT and GPT-4 have demonstrated their potential in clinical\nreasoning. However, these LLMs are prone to hallucination problems, and the\nreasoning process of LLMs may not align with the clinical decision path of\nphysicians. In this study, we introduce a novel framework, In-Context Padding\n(ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer\ncritical clinical reasoning elements (referred to as knowledge seeds) and use\nthese as anchors to guide the generation process of LLMs. Experiments on two\nclinical question datasets demonstrate that ICP significantly improves the\nclinical reasoning ability of LLMs.",
        "score": 3.1727755069732666
      },
      {
        "title": "Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,\n  Data, and Algorithm,",
        "abstract": "Large language models (LLMs) are gaining increasing interests to improve\nclinical efficiency for medical diagnosis, owing to their unprecedented\nperformance in modelling natural language. Ensuring the safe and reliable\nclinical applications, the evaluation of LLMs indeed becomes critical for\nbetter mitigating the potential risks, e.g., hallucinations. However, current\nevaluation methods heavily rely on labor-intensive human participation to\nachieve human-preferred judgements. To overcome this challenge, we propose an\nautomatic evaluation paradigm tailored to assess the LLMs' capabilities in\ndelivering clinical services, e.g., disease diagnosis and treatment. The\nevaluation paradigm contains three basic elements: metric, data, and algorithm.\nSpecifically, inspired by professional clinical practice pathways, we formulate\na LLM-specific clinical pathway (LCP) to define the clinical capabilities that\na doctor agent should possess. Then, Standardized Patients (SPs) from the\nmedical education are introduced as the guideline for collecting medical data\nfor evaluation, which can well ensure the completeness of the evaluation\nprocedure. Leveraging these steps, we develop a multi-agent framework to\nsimulate the interactive environment between SPs and a doctor agent, which is\nequipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the\nbehaviors of a doctor agent are in accordance with LCP. The above paradigm can\nbe extended to any similar clinical scenarios to automatically evaluate the\nLLMs' medical capabilities. Applying such paradigm, we construct an evaluation\nbenchmark in the field of urology, including a LCP, a SPs dataset, and an\nautomated RAE. Extensive experiments are conducted to demonstrate the\neffectiveness of the proposed approach, providing more insights for LLMs' safe\nand reliable deployments in clinical practice.",
        "score": 3.1576645374298096
      },
      {
        "title": "Integrating UMLS Knowledge into Large Language Models for Medical\n  Question Answering,",
        "abstract": "Large language models (LLMs) have demonstrated powerful text generation\ncapabilities, bringing unprecedented innovation to the healthcare field. While\nLLMs hold immense promise for applications in healthcare, applying them to real\nclinical scenarios presents significant challenges, as these models may\ngenerate content that deviates from established medical facts and even exhibit\npotential biases. In our research, we develop an augmented LLM framework based\non the Unified Medical Language System (UMLS), aiming to better serve the\nhealthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our\nbenchmark models, and conduct automatic evaluations using the ROUGE Score and\nBERTScore on 104 questions from the LiveQA test set. Additionally, we establish\ncriteria for physician-evaluation based on four dimensions: Factuality,\nCompleteness, Readability and Relevancy. ChatGPT-3.5 is used for physician\nevaluation with 20 questions on the LiveQA test set. Multiple resident\nphysicians conducted blind reviews to evaluate the generated content, and the\nresults indicate that this framework effectively enhances the factuality,\ncompleteness, and relevance of generated content. Our research demonstrates the\neffectiveness of using UMLS-augmented LLMs and highlights the potential\napplication value of LLMs in in medical question-answering.",
        "score": 3.121476411819458
      },
      {
        "title": "Large Language Models in the Clinic: A Comprehensive Benchmark,",
        "abstract": "The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and complex clinical tasks that are close to\nreal-world practice, i.e., referral QA, treatment recommendation,\nhospitalization (long document) summarization, patient education, pharmacology\nQA and drug interaction for emerging drugs. We conduct an extensive evaluation\nof twenty-two LLMs under both zero-shot and few-shot settings. Finally, we\ninvite medical experts to evaluate the clinical usefulness of LLMs.",
        "score": 3.020409107208252
      },
      {
        "title": "CLUE: A Clinical Language Understanding Evaluation for LLMs,",
        "abstract": "Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, evaluation has\nprimarily been limited to non-clinical tasks, which do not reflect the\ncomplexity of practical clinical applications. To fill this gap, we present the\nClinical Language Understanding Evaluation (CLUE), a benchmark tailored to\nevaluate LLMs on clinical tasks. CLUE includes six tasks to test the practical\napplicability of LLMs in complex healthcare settings. Our evaluation includes a\ntotal of $25$ LLMs. In contrast to previous evaluations, CLUE shows a decrease\nin performance for nine out of twelve biomedical models. Our benchmark\nrepresents a step towards a standardized approach to evaluating and developing\nLLMs in healthcare to align future model development with the real-world needs\nof clinical application. We open-source all evaluation scripts and datasets for\nfuture research at https://github.com/TIO-IKIM/CLUE.",
        "score": 2.834336996078491
      },
      {
        "title": "ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data\n  and Comprehensive Evaluation,",
        "abstract": "Large language models have exhibited exceptional performance on various\nNatural Language Processing (NLP) tasks, leveraging techniques such as the\npre-training, and instruction fine-tuning. Despite these advances, their\neffectiveness in medical applications is limited, due to challenges such as\nfactual inaccuracies, reasoning abilities, and lack grounding in real-world\nexperience. In this study, we present ClinicalGPT, a language model explicitly\ndesigned and optimized for clinical scenarios. By incorporating extensive and\ndiverse real-world data, such as medical records, domain-specific knowledge,\nand multi-round dialogue consultations in the training process, ClinicalGPT is\nbetter prepared to handle multiple clinical task. Furthermore, we introduce a\ncomprehensive evaluation framework that includes medical knowledge\nquestion-answering, medical exams, patient consultations, and diagnostic\nanalysis of medical records. Our results demonstrate that ClinicalGPT\nsignificantly outperforms other models in these tasks, highlighting the\neffectiveness of our approach in adapting large language models to the critical\ndomain of healthcare.",
        "score": 2.8241724967956543
      },
      {
        "title": "Evaluation of General Large Language Models in Contextually Assessing\n  Semantic Concepts Extracted from Adult Critical Care Electronic Health Record\n  Notes,",
        "abstract": "The field of healthcare has increasingly turned its focus towards Large\nLanguage Models (LLMs) due to their remarkable performance. However, their\nperformance in actual clinical applications has been underexplored. Traditional\nevaluations based on question-answering tasks don't fully capture the nuanced\ncontexts. This gap highlights the need for more in-depth and practical\nassessments of LLMs in real-world healthcare settings. Objective: We sought to\nevaluate the performance of LLMs in the complex clinical context of adult\ncritical care medicine using systematic and comprehensible analytic methods,\nincluding clinician annotation and adjudication. Methods: We investigated the\nperformance of three general LLMs in understanding and processing real-world\nclinical notes. Concepts from 150 clinical notes were identified by MetaMap and\nthen labeled by 9 clinicians. Each LLM's proficiency was evaluated by\nidentifying the temporality and negation of these concepts using different\nprompts for an in-depth analysis. Results: GPT-4 showed overall superior\nperformance compared to other LLMs. In contrast, both GPT-3.5 and\ntext-davinci-003 exhibit enhanced performance when the appropriate prompting\nstrategies are employed. The GPT family models have demonstrated considerable\nefficiency, evidenced by their cost-effectiveness and time-saving capabilities.\nConclusion: A comprehensive qualitative performance evaluation framework for\nLLMs is developed and operationalized. This framework goes beyond singular\nperformance aspects. With expert annotations, this methodology not only\nvalidates LLMs' capabilities in processing complex medical data but also\nestablishes a benchmark for future LLM evaluations across specialized domains.",
        "score": 2.754760980606079
      },
      {
        "title": "An Automatic Evaluation Framework for Multi-turn Medical Consultations\n  Capabilities of Large Language Models,",
        "abstract": "Large language models (LLMs) have achieved significant success in interacting\nwith human. However, recent studies have revealed that these models often\nsuffer from hallucinations, leading to overly confident but incorrect\njudgments. This limits their application in the medical domain, where tasks\nrequire the utmost accuracy. This paper introduces an automated evaluation\nframework that assesses the practical capabilities of LLMs as virtual doctors\nduring multi-turn consultations. Consultation tasks are designed to require\nLLMs to be aware of what they do not know, to inquire about missing medical\ninformation from patients, and to ultimately make diagnoses. To evaluate the\nperformance of LLMs for these tasks, a benchmark is proposed by reformulating\nmedical multiple-choice questions from the United States Medical Licensing\nExaminations (USMLE), and comprehensive evaluation metrics are developed and\nevaluated on three constructed test sets. A medical consultation training set\nis further constructed to improve the consultation ability of LLMs. The results\nof the experiments show that fine-tuning with the training set can alleviate\nhallucinations and improve LLMs' performance on the proposed benchmark.\nExtensive experiments and ablation studies are conducted to validate the\neffectiveness and robustness of the proposed framework.",
        "score": 2.7277798652648926
      },
      {
        "title": "MedExpQA: Multilingual Benchmarking of Large Language Models for Medical\n  Question Answering,",
        "abstract": "Large Language Models (LLMs) have the potential of facilitating the\ndevelopment of Artificial Intelligence technology to assist medical experts for\ninteractive decision support, which has been demonstrated by their competitive\nperformances in Medical QA. However, while impressive, the required quality bar\nfor medical applications remains far from being achieved. Currently, LLMs\nremain challenged by outdated knowledge and by their tendency to generate\nhallucinated content. Furthermore, most benchmarks to assess medical knowledge\nlack reference gold explanations which means that it is not possible to\nevaluate the reasoning of LLMs predictions. Finally, the situation is\nparticularly grim if we consider benchmarking LLMs for languages other than\nEnglish which remains, as far as we know, a totally neglected topic. In order\nto address these shortcomings, in this paper we present MedExpQA, the first\nmultilingual benchmark based on medical exams to evaluate LLMs in Medical\nQuestion Answering. To the best of our knowledge, MedExpQA includes for the\nfirst time reference gold explanations written by medical doctors which can be\nleveraged to establish various gold-based upper-bounds for comparison with LLMs\nperformance. Comprehensive multilingual experimentation using both the gold\nreference explanations and Retrieval Augmented Generation (RAG) approaches show\nthat performance of LLMs still has large room for improvement, especially for\nlanguages other than English. Furthermore, and despite using state-of-the-art\nRAG methods, our results also demonstrate the difficulty of obtaining and\nintegrating readily available medical knowledge that may positively impact\nresults on downstream evaluations for Medical Question Answering. So far the\nbenchmark is available in four languages, but we hope that this work may\nencourage further development to other languages.",
        "score": 2.6924972534179688
      },
      {
        "title": "Are Large Language Models Ready for Healthcare? A Comparative Study on\n  Clinical Language Understanding,",
        "abstract": "Large language models (LLMs) have made significant progress in various\ndomains, including healthcare. However, the specialized nature of clinical\nlanguage understanding tasks presents unique challenges and limitations that\nwarrant further investigation. In this study, we conduct a comprehensive\nevaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within\nthe realm of clinical language understanding tasks. These tasks span a diverse\nrange, including named entity recognition, relation extraction, natural\nlanguage inference, semantic textual similarity, document classification, and\nquestion-answering. We also introduce a novel prompting strategy,\nself-questioning prompting (SQP), tailored to enhance LLMs' performance by\neliciting informative questions and answers pertinent to the clinical scenarios\nat hand. Our evaluation underscores the significance of task-specific learning\nstrategies and prompting techniques for improving LLMs' effectiveness in\nhealthcare-related tasks. Additionally, our in-depth error analysis on the\nchallenging relation extraction task offers valuable insights into error\ndistribution and potential avenues for improvement using SQP. Our study sheds\nlight on the practical implications of employing LLMs in the specialized domain\nof healthcare, serving as a foundation for future research and the development\nof potential applications in healthcare settings.",
        "score": 2.521496295928955
      },
      {
        "title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using\n  Iterative In-Context-Learning,",
        "abstract": "From grading papers to summarizing medical documents, large language models\n(LLMs) are evermore used for evaluation of text generated by humans and AI\nalike. However, despite their extensive utility, LLMs exhibit distinct failure\nmodes, necessitating a thorough audit and improvement of their text evaluation\ncapabilities. Here we introduce ALLURE, a systematic approach to Auditing Large\nLanguage Models Understanding and Reasoning Errors. ALLURE involves comparing\nLLM-generated evaluations with annotated data, and iteratively incorporating\ninstances of significant deviation into the evaluator, which leverages\nin-context learning (ICL) to enhance and improve robust evaluation of text by\nLLMs. Through this iterative process, we refine the performance of the\nevaluator LLM, ultimately reducing reliance on human annotators in the\nevaluation process. We anticipate ALLURE to serve diverse applications of LLMs\nin various domains related to evaluation of textual data, such as medical\nsummarization, education, and and productivity.",
        "score": 2.4624555110931396
      },
      {
        "title": "MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark\n  for Language Model Evaluation,",
        "abstract": "Curated datasets for healthcare are often limited due to the need of human\nannotations from experts. In this paper, we present MedEval, a multi-level,\nmulti-task, and multi-domain medical benchmark to facilitate the development of\nlanguage models for healthcare. MedEval is comprehensive and consists of data\nfrom several healthcare systems and spans 35 human body regions from 8\nexamination modalities. With 22,779 collected sentences and 21,228 reports, we\nprovide expert annotations at multiple levels, offering a granular potential\nusage of the data and supporting a wide range of tasks. Moreover, we\nsystematically evaluated 10 generic and domain-specific language models under\nzero-shot and finetuning settings, from domain-adapted baselines in healthcare\nto general-purposed state-of-the-art large language models (e.g., ChatGPT). Our\nevaluations reveal varying effectiveness of the two categories of language\nmodels across different tasks, from which we notice the importance of\ninstruction tuning for few-shot usage of large language models. Our\ninvestigation paves the way toward benchmarking language models for healthcare\nand provides valuable insights into the strengths and limitations of adopting\nlarge language models in medical domains, informing their practical\napplications and future advancements.",
        "score": 2.3938183784484863
      },
      {
        "title": "Med42 -- Evaluating Fine-Tuning Strategies for Medical LLMs:\n  Full-Parameter vs. Parameter-Efficient Approaches,",
        "abstract": "This study presents a comprehensive analysis and comparison of two\npredominant fine-tuning methodologies - full-parameter fine-tuning and\nparameter-efficient tuning - within the context of medical Large Language\nModels (LLMs). We developed and refined a series of LLMs, based on the Llama-2\narchitecture, specifically designed to enhance medical knowledge retrieval,\nreasoning, and question-answering capabilities. Our experiments systematically\nevaluate the effectiveness of these tuning strategies across various well-known\nmedical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of\n72% on the US Medical Licensing Examination (USMLE) datasets, setting a new\nstandard in performance for openly available medical LLMs. Through this\ncomparative analysis, we aim to identify the most effective and efficient\nmethod for fine-tuning LLMs in the medical domain, thereby contributing\nsignificantly to the advancement of AI-driven healthcare applications.",
        "score": 2.321981430053711
      },
      {
        "title": "Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese\n  Medical Exam Dataset,",
        "abstract": "Recent advancements in large language models (LLMs) have transformed the\nfield of question answering (QA). However, evaluating LLMs in the medical field\nis challenging due to the lack of standardized and comprehensive datasets. To\naddress this gap, we introduce CMExam, sourced from the Chinese National\nMedical Licensing Examination. CMExam consists of 60K+ multiple-choice\nquestions for standardized and objective evaluations, as well as solution\nexplanations for model reasoning evaluation in an open-ended manner. For\nin-depth analyses of LLMs, we invited medical professionals to label five\nadditional question-wise annotations, including disease groups, clinical\ndepartments, medical disciplines, areas of competency, and question difficulty\nlevels. Alongside the dataset, we further conducted thorough experiments with\nrepresentative LLMs and QA algorithms on CMExam. The results show that GPT-4\nhad the best accuracy of 61.6% and a weighted F1 score of 0.617. These results\nhighlight a great disparity when compared to human accuracy, which stood at\n71.6%. For explanation tasks, while LLMs could generate relevant reasoning and\ndemonstrate improved performance after finetuning, they fall short of a desired\nstandard, indicating ample room for improvement. To the best of our knowledge,\nCMExam is the first Chinese medical exam dataset to provide comprehensive\nmedical annotations. The experiments and findings of LLM evaluation also\nprovide valuable insights into the challenges and potential solutions in\ndeveloping Chinese medical QA systems and LLM evaluation pipelines. The dataset\nand relevant code are available at https://github.com/williamliujl/CMExam.",
        "score": 2.261469602584839
      },
      {
        "title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering,",
        "abstract": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate the automatic evaluations of\nfactuality. We also propose OLAPH, a simple and novel framework that enables\nthe improvement of factuality through automatic evaluations. The OLAPH\nframework iteratively trains LLMs to mitigate hallucinations using sampling\npredictions and preference optimization. In other words, we iteratively set the\nhighest-scoring response as a preferred response derived from sampling\npredictions and train LLMs to align with the preferred response that improves\nfactuality. We highlight that, even on evaluation metrics not used during\ntraining, LLMs trained with our OLAPH framework demonstrate significant\nperformance improvement in factuality. Our findings reveal that a 7B LLM\ntrained with our OLAPH framework can provide long answers comparable to the\nmedical experts' answers in terms of factuality. We believe that our work could\nshed light on gauging the long-text generation ability of LLMs in the medical\ndomain. Our code and datasets are available at\nhttps://github.com/dmis-lab/OLAPH}{https://github.com/dmis-lab/OLAPH.",
        "score": 2.251183032989502
      },
      {
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models,",
        "abstract": "This research paper focuses on the challenges posed by hallucinations in\nlarge language models (LLMs), particularly in the context of the medical\ndomain. Hallucination, wherein these models generate plausible yet unverified\nor incorrect information, can have serious consequences in healthcare\napplications. We propose a new benchmark and dataset, Med-HALT (Medical Domain\nHallucination Test), designed specifically to evaluate and reduce\nhallucinations. Med-HALT provides a diverse multinational dataset derived from\nmedical examinations across various countries and includes multiple innovative\ntesting modalities. Med-HALT includes two categories of tests reasoning and\nmemory-based hallucination tests, designed to assess LLMs's problem-solving and\ninformation retrieval abilities.\n  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,\nMPT, and Falcon, revealing significant differences in their performance. The\npaper provides detailed insights into the dataset, promoting transparency and\nreproducibility. Through this work, we aim to contribute to the development of\nsafer and more reliable language models in healthcare. Our benchmark can be\nfound at medhalt.github.io",
        "score": 2.1757266521453857
      },
      {
        "title": "Large Language Models Encode Clinical Knowledge,",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but the quality bar for medical\nand clinical applications is high. Today, attempts to assess models' clinical\nknowledge typically rely on automated evaluations on limited benchmarks. There\nis no standard to evaluate model predictions and reasoning across a breadth of\ntasks. To address this, we present MultiMedQA, a benchmark combining six\nexisting open question answering datasets spanning professional medical exams,\nresearch, and consumer queries; and HealthSearchQA, a new free-response dataset\nof medical questions searched online. We propose a framework for human\nevaluation of model answers along multiple axes including factuality,\nprecision, possible harm, and bias. In addition, we evaluate PaLM (a\n540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on\nMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves\nstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,\nMedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US\nMedical License Exam questions), surpassing prior state-of-the-art by over 17%.\nHowever, human evaluation reveals key gaps in Flan-PaLM responses. To resolve\nthis we introduce instruction prompt tuning, a parameter-efficient approach for\naligning LLMs to new domains using a few exemplars. The resulting model,\nMed-PaLM, performs encouragingly, but remains inferior to clinicians. We show\nthat comprehension, recall of knowledge, and medical reasoning improve with\nmodel scale and instruction prompt tuning, suggesting the potential utility of\nLLMs in medicine. Our human evaluations reveal important limitations of today's\nmodels, reinforcing the importance of both evaluation frameworks and method\ndevelopment in creating safe, helpful LLM models for clinical applications.",
        "score": 2.085110664367676
      },
      {
        "title": "D-NLP at SemEval-2024 Task 2: Evaluating Clinical Inference Capabilities\n  of Large Language Models,",
        "abstract": "Large language models (LLMs) have garnered significant attention and\nwidespread usage due to their impressive performance in various tasks. However,\nthey are not without their own set of challenges, including issues such as\nhallucinations, factual inconsistencies, and limitations in\nnumerical-quantitative reasoning. Evaluating LLMs in miscellaneous reasoning\ntasks remains an active area of research. Prior to the breakthrough of LLMs,\nTransformers had already proven successful in the medical domain, effectively\nemployed for various natural language understanding (NLU) tasks. Following this\ntrend, LLMs have also been trained and utilized in the medical domain, raising\nconcerns regarding factual accuracy, adherence to safety protocols, and\ninherent limitations. In this paper, we focus on evaluating the natural\nlanguage inference capabilities of popular open-source and closed-source LLMs\nusing clinical trial reports as the dataset. We present the performance results\nof each LLM and further analyze their performance on a development set,\nparticularly focusing on challenging instances that involve medical\nabbreviations and require numerical-quantitative reasoning. Gemini, our leading\nLLM, achieved a test set F1-score of 0.748, securing the ninth position on the\ntask scoreboard. Our work is the first of its kind, offering a thorough\nexamination of the inference capabilities of LLMs within the medical domain.",
        "score": 1.6188145875930786
      },
      {
        "title": "K-QA: A Real-World Medical Q&A Benchmark,",
        "abstract": "Ensuring the accuracy of responses provided by large language models (LLMs)\nis crucial, particularly in clinical settings where incorrect information may\ndirectly impact patient health. To address this challenge, we construct K-QA, a\ndataset containing 1,212 patient questions originating from real-world\nconversations held on K Health (an AI-driven clinical platform). We employ a\npanel of in-house physicians to answer and manually decompose a subset of K-QA\ninto self-contained statements. Additionally, we formulate two NLI-based\nevaluation metrics approximating recall and precision: (1) comprehensiveness,\nmeasuring the percentage of essential clinical information in the generated\nanswer and (2) hallucination rate, measuring the number of statements from the\nphysician-curated response contradicted by the LLM answer. Finally, we use K-QA\nalong with these metrics to evaluate several state-of-the-art models, as well\nas the effect of in-context learning and medically-oriented augmented retrieval\nschemes developed by the authors. Our findings indicate that in-context\nlearning improves the comprehensiveness of the models, and augmented retrieval\nis effective in reducing hallucinations. We make K-QA available to to the\ncommunity to spur research into medically accurate NLP applications.",
        "score": 1.495788812637329
      },
      {
        "title": "SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for\n  Clinical Trials,",
        "abstract": "Large Language Models (LLMs) are at the forefront of NLP achievements but\nfall short in dealing with shortcut learning, factual inconsistency, and\nvulnerability to adversarial inputs.These shortcomings are especially critical\nin medical contexts, where they can misrepresent actual model capabilities.\nAddressing this, we present SemEval-2024 Task 2: Safe Biomedical Natural\nLanguage Inference for ClinicalTrials. Our contributions include the refined\nNLI4CT-P dataset (i.e., Natural Language Inference for Clinical Trials -\nPerturbed), designed to challenge LLMs with interventional and causal reasoning\ntasks, along with a comprehensive evaluation of methods and results for\nparticipant submissions. A total of 106 participants registered for the task\ncontributing to over 1200 individual submissions and 25 system overview papers.\nThis initiative aims to advance the robustness and applicability of NLI models\nin healthcare, ensuring safer and more dependable AI assistance in clinical\ndecision-making. We anticipate that the dataset, models, and outcomes of this\ntask can support future research in the field of biomedical NLI. The dataset,\ncompetition leaderboard, and website are publicly available.",
        "score": 1.2985416650772095
      },
      {
        "title": "Towards Expert-Level Medical Question Answering with Large Language\n  Models,",
        "abstract": "Recent artificial intelligence (AI) systems have reached milestones in \"grand\nchallenges\" ranging from Go to protein-folding. The capability to retrieve\nmedical knowledge, reason over it, and answer medical questions comparably to\nphysicians has long been viewed as one such grand challenge.\n  Large language models (LLMs) have catalyzed significant progress in medical\nquestion answering; Med-PaLM was the first model to exceed a \"passing\" score in\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\non the MedQA dataset. However, this and other prior work suggested significant\nroom for improvement, especially when models' answers were compared to\nclinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\nfinetuning, and prompting strategies including a novel ensemble refinement\napproach.\n  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art. We also observed performance\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\nclinical topics datasets.\n  We performed detailed human evaluations on long-form questions along multiple\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\n< 0.001). We also observed significant improvements compared to Med-PaLM on\nevery evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form\n\"adversarial\" questions to probe LLM limitations.\n  While further studies are necessary to validate the efficacy of these models\nin real-world settings, these results highlight rapid progress towards\nphysician-level performance in medical question answering.",
        "score": 1.2311245203018188
      },
      {
        "title": "Capabilities of GPT-4 on Medical Challenge Problems,",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.",
        "score": 1.2287684679031372
      },
      {
        "title": "Do We Still Need Clinical Language Models?,",
        "abstract": "Although recent advances in scaling large language models (LLMs) have\nresulted in improvements on many NLP tasks, it remains unclear whether these\nmodels trained primarily with general web text are the right tool in highly\nspecialized, safety critical domains such as clinical text. Recent results have\nsuggested that LLMs encode a surprising amount of medical knowledge. This\nraises an important question regarding the utility of smaller domain-specific\nlanguage models. With the success of general-domain LLMs, is there still a need\nfor specialized clinical models? To investigate this question, we conduct an\nextensive empirical analysis of 12 language models, ranging from 220M to 175B\nparameters, measuring their performance on 3 different clinical tasks that test\ntheir ability to parse and reason over electronic health records. As part of\nour experiments, we train T5-Base and T5-Large models from scratch on clinical\nnotes from MIMIC III and IV to directly investigate the efficiency of clinical\ntokens. We show that relatively small specialized clinical models substantially\noutperform all in-context learning approaches, even when finetuned on limited\nannotated data. Further, we find that pretraining on clinical tokens allows for\nsmaller, more parameter-efficient models that either match or outperform much\nlarger language models trained on general text. We release the code and the\nmodels used under the PhysioNet Credentialed Health Data license and data use\nagreement.",
        "score": 1.1869158744812012
      },
      {
        "title": "Evaluation of Language Models in the Medical Context Under\n  Resource-Constrained Settings,",
        "abstract": "Since the emergence of the Transformer architecture, language model\ndevelopment has increased, driven by their promising potential. However,\nreleasing these models into production requires properly understanding their\nbehavior, particularly in sensitive domains such as medicine. Despite this\nneed, the medical literature still lacks technical assessments of pre-trained\nlanguage models, which are especially valuable in resource-constrained settings\nin terms of computational power or limited budget. To address this gap, we\nprovide a comprehensive survey of language models in the medical domain. In\naddition, we selected a subset of these models for thorough evaluation,\nfocusing on classification and text generation tasks. Our subset encompasses 53\nmodels, ranging from 110 million to 13 billion parameters, spanning the three\nfamilies of Transformer-based models and from diverse knowledge domains. This\nstudy employs a series of approaches for text classification together with\nzero-shot prompting instead of model training or fine-tuning, which closely\nresembles the limited resource setting in which many users of language models\nfind themselves. Encouragingly, our findings reveal remarkable performance\nacross various tasks and datasets, underscoring the latent potential of certain\nmodels to contain medical knowledge, even without domain specialization.\nConsequently, our study advocates for further exploration of model applications\nin medical contexts, particularly in resource-constrained settings. The code is\navailable on https://github.com/anpoc/Language-models-in-medicine.",
        "score": 0.7226390242576599
      },
      {
        "title": "Open (Clinical) LLMs are Sensitive to Instruction Phrasings,",
        "abstract": "Instruction-tuned Large Language Models (LLMs) can perform a wide range of\ntasks given natural language instructions to do so, but they are sensitive to\nhow such instructions are phrased. This issue is especially concerning in\nhealthcare, as clinicians are unlikely to be experienced prompt engineers and\nthe potential consequences of inaccurate outputs are heightened in this domain.\n  This raises a practical question: How robust are instruction-tuned LLMs to\nnatural variations in the instructions provided for clinical NLP tasks? We\ncollect prompts from medical doctors across a range of tasks and quantify the\nsensitivity of seven LLMs -- some general, others specialized -- to natural\n(i.e., non-adversarial) instruction phrasings. We find that performance varies\nsubstantially across all models, and that -- perhaps surprisingly --\ndomain-specific models explicitly trained on clinical data are especially\nbrittle, compared to their general domain counterparts. Further, arbitrary\nphrasing differences can affect fairness, e.g., valid but distinct instructions\nfor mortality prediction yield a range both in overall performance, and in\nterms of differences between demographic groups.",
        "score": 0.02909766137599945
      },
      {
        "title": "Can large language models reason about medical questions?,",
        "abstract": "Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nclose- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer\nand reason about difficult real-world-based questions. We focus on three\npopular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple\nprompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and\nretrieval augmentation. Based on an expert annotation of the generated CoTs, we\nfound that InstructGPT can often read, reason and recall expert knowledge.\nLast, by leveraging advances in prompt engineering (few-shot and ensemble\nmethods), we demonstrated that GPT-3.5 not only yields calibrated predictive\ndistributions, but also reaches the passing score on three datasets:\nMedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are\nclosing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.",
        "score": -1.084059238433838
      }
    ]
  }
]